{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "city_name = 'hayward'\n",
    "url = 'https://hayward.legistar.com/Calendar.aspx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib import parse\n",
    "from urllib import request\n",
    "import bs4 as bs\n",
    "\n",
    "import selenium as se\n",
    "from selenium.webdriver import Firefox\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from tqdm import tqdm\n",
    "\n",
    "# helper functions\n",
    "def get_page_links(driver):\n",
    "    pagelinks_xpath = \"//td[@class='rgPagerCell NumericPages']/div[1]/a\"\n",
    "    pagelinks = driver.find_elements_by_xpath(pagelinks_xpath)\n",
    "    pagelinks = pagelinks[:int(len(pagelinks)/2)]\n",
    "    return [l.text for l in pagelinks], pagelinks\n",
    "\n",
    "def extract_table(page_source, table_id):\n",
    "    # find table in page\n",
    "    soup = bs.BeautifulSoup(page_source)\n",
    "    table = soup.select(table_id)[0]\n",
    "    num_cols = int(table.td.get('colspan'))\n",
    "\n",
    "    # extract column headers\n",
    "    header_data = [''.join(cell.stripped_strings) for cell in table.find_all('th')]\n",
    "    header_data = [h for h in header_data if h!='Data pager']\n",
    "    assert(len(header_data)==num_cols)\n",
    "\n",
    "    # extract text and URL data from table\n",
    "    text_data, url_data = [], []\n",
    "    for row in table.find_all('tr'):\n",
    "        row_text, row_url = [], []\n",
    "        for td in row.find_all('td'):\n",
    "            row_text.append(''.join(td.stripped_strings))\n",
    "            if td.find('a'):\n",
    "                row_url.append(td.a.get('href'))\n",
    "            else:\n",
    "                row_url.append(np.nan)\n",
    "            if len(row_text)==num_cols and len(row_url)==num_cols:\n",
    "                text_data.append(row_text)\n",
    "                url_data.append(row_url)\n",
    "                \n",
    "    # turn into dataframe\n",
    "    num_cols = table.td.get('colspan')\n",
    "    text_df = pd.DataFrame(text_data, columns=header_data)\n",
    "    url_df = pd.DataFrame(url_data, columns=header_data)\n",
    "    df = pd.merge(text_df, url_df, left_index=True, right_index=True, suffixes=(' Text', ' URL'))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch driver\n",
    "driver = Firefox()\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL: select date range dropdown  to 'All Years'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrape page 1?\n",
      "Scrape page 2?\n",
      "Scrape page 3?\n",
      "Scrape page 4?\n",
      "Scrape page 5?\n",
      "Scrape page 6?\n",
      "Scrape page 7?\n",
      "Scrape page 8?\n",
      "Scrape page 9?\n",
      "Scrape page 10?\n",
      "Scrape page 11?\n",
      "Scrape page 12?\n",
      "Scrape page 13?\n"
     ]
    }
   ],
   "source": [
    "# click through pages and save html\n",
    "c = 1\n",
    "page_data = []\n",
    "while True:\n",
    "    pages, pagelinks = get_page_links(driver)\n",
    "    try:\n",
    "        # click on the integer we want\n",
    "        i = pages.index(str(c))\n",
    "        link = pagelinks[i]\n",
    "    except:\n",
    "        # if it's not there and the list ends with '...', click on '...'\n",
    "        if pages[-1]=='...':\n",
    "            link = pagelinks[-1]\n",
    "        # if it's not there and the list starts with '...', we are done.\n",
    "        else:\n",
    "            break\n",
    "    link.click()\n",
    "    input('Scrape page {}?'.format(c))\n",
    "    page_data.append(driver.page_source)\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  4.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract table data\n",
    "table_id = '#ctl00_ContentPlaceHolder1_gridCalendar_ctl00'\n",
    "page_dfs = [extract_table(page, table_id) for page in tqdm(page_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1289"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat(page_dfs)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name Text', 'Meeting Date Text', ' Text', 'Meeting Time Text',\n",
       "       'Meeting Location Text', 'Meeting Details Text',\n",
       "       'Staff/ProjectApplicant Presentations Text', 'Agenda Text',\n",
       "       'Action Minutes Text', 'Documents ReceivedAfter Published Agenda Text',\n",
       "       'Official Minutes Text', 'Video Text', 'Name URL', 'Meeting Date URL',\n",
       "       ' URL', 'Meeting Time URL', 'Meeting Location URL',\n",
       "       'Meeting Details URL', 'Staff/ProjectApplicant Presentations URL',\n",
       "       'Agenda URL', 'Action Minutes URL',\n",
       "       'Documents ReceivedAfter Published Agenda URL', 'Official Minutes URL',\n",
       "       'Video URL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../data/scraping/scraped_tables/{}.csv'.format(city_name)\n",
    "data.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate results\n",
    "# submit_button_id = 'ctl00_ContentPlaceHolder1_btnSearch'\n",
    "# button = driver.find_element_by_id(submit_button_id)\n",
    "# button.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
