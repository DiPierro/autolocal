{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autolocal.aws import aws_config\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata():\n",
    "    # start_date = [int(d) for d in args.start_date.split(\"-\")]\n",
    "    # end_date = [int(d) for d in args.end_date.split(\"-\")]\n",
    "    # for year in range(start_date[0], end_date[0]):\n",
    "    #     for month in range(start_date[1], end_date[1]):\n",
    "    #         for day in range(start_date[2], end_date[2]):\n",
    "    table = boto3.resource(\n",
    "        'dynamodb',\n",
    "        region_name=aws_config.region_name,\n",
    "        ).Table(aws_config.db_document_table_name)\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        region_name=aws_config.region_name\n",
    "        )\n",
    "\n",
    "    response = table.scan()\n",
    "    data = response['Items']\n",
    "\n",
    "    while 'LastEvaluatedKey' in response:\n",
    "        response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "        data.extend(response['Items'])\n",
    "\n",
    "    metadata = pd.DataFrame(data)\n",
    "    metadata[\"date\"] = [datetime.strptime(d, '%Y-%m-%d') for d in metadata[\"date\"]]\n",
    "    metadata['local_path_pkl'] = metadata['local_path_txt'].apply(lambda x: \"vectors\"+x[4:-3]+\"pkl\")\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = read_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_filenames(queries, metadata, start_date=None, end_date=None, agenda_only=False):\n",
    "\n",
    "    cities = set()\n",
    "    \n",
    "    # filter metadata to only those files that match the query municipality and time_window\n",
    "    for query in queries:\n",
    "        cities.update(query[\"municipalities\"])\n",
    "\n",
    "    potential_documents = metadata\n",
    "\n",
    "#     print(\"start\", start_date)\n",
    "#     print(\"end\", end_date)\n",
    "    potential_documents = potential_documents[potential_documents[\"date\"] >= start_date]\n",
    "    if end_date:\n",
    "        potential_documents = potential_documents[potential_documents[\"date\"] <= end_date]\n",
    "\n",
    "    potential_documents = potential_documents[[(c in cities) for c in potential_documents[\"city\"]]]\n",
    "\n",
    "    if agenda_only:\n",
    "        potential_documents = potential_documents[potential_documents[\"doc_type\"]==\"Agenda\"]\n",
    "\n",
    "    return list(potential_documents['local_path_txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_split(s):\n",
    "    sentences = re.split('[.\\n!?\"\\f]', s)\n",
    "    return [s for s in sentences if len(s.strip())>0]\n",
    "\n",
    "def tokenize(s):\n",
    "    tokens = re.findall(r'\\w+', s)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_pkl_path(s3_path):\n",
    "    return os.path.join(\"../data/pkls/\", os.path.basename(s3_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vectors(pkl_filename):\n",
    "    local_pkl_path = get_local_pkl_path(pkl_filename)\n",
    "    try:\n",
    "        return pickle.load(open(local_pkl_path, 'rb'))\n",
    "    except Exception as e:\n",
    "        print(\"failed to load local vectors\")\n",
    "        print(e)\n",
    "        s3 = boto3.resource('s3')\n",
    "        s3.meta.client.download_file(\n",
    "            aws_config.s3_document_bucket_name,\n",
    "            pkl_filename,\n",
    "            local_pkl_path)\n",
    "        return pickle.load(open(local_pkl_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(s3_path):\n",
    "    s3 = boto3.resource(\n",
    "        's3',\n",
    "        region_name=aws_config.region_name\n",
    "        )\n",
    "    autolocal_docs_bucket = s3.Bucket(\n",
    "        aws_config.s3_document_bucket_name\n",
    "        )\n",
    "    try:\n",
    "        return autolocal_docs_bucket.Object(s3_path).get()['Body'].read().decode(\"ascii\", \"ignore\")\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(s3_paths):\n",
    "    log_every = 100\n",
    "\n",
    "    documents = {}\n",
    "    n_docs_total = len(s3_paths)\n",
    "\n",
    "    i = 0\n",
    "    n_docs_read = 0\n",
    "    for s3_path in s3_paths:\n",
    "        pkl_path = \"vectors\" + s3_path[4:-3] + \"pkl\"\n",
    "        try:\n",
    "            doc_string = read_doc(s3_path)\n",
    "            if doc_string:\n",
    "                doc_sentences = sentence_split(doc_string)\n",
    "                doc_tokens = []\n",
    "                for sentence in doc_sentences:\n",
    "                    sentence_tokens = tokenize(sentence)\n",
    "                    doc_tokens.append(sentence_tokens)\n",
    "                try:\n",
    "                    vectors = read_vectors(pkl_path)\n",
    "                    documents[s3_path] = {\n",
    "                        \"original_text\": doc_string,\n",
    "                        \"sentences\": doc_sentences,\n",
    "                        \"vectors\": vectors\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print('missing vectors for: {}'.format(pkl_path))\n",
    "                    print(e)\n",
    "        except Exception as e:\n",
    "            if i < 10:\n",
    "                print(\"Key not found in S3: {}\".format(s3_path))\n",
    "                print(e)\n",
    "            elif i == 10:\n",
    "                print(\"More than 10 keys not found\")\n",
    "                print(e)\n",
    "                break\n",
    "            i+=1\n",
    "#         if n_docs_read % log_every == 0:\n",
    "#             print(\"{} of {} documents read\".format(n_docs_read, n_docs_total))\n",
    "        n_docs_read+=1\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_docs(municipalities, all_docs, metadata, start_date=None, end_date=None):\n",
    "    # filter metadata to only those files that match the query municipality and time_window\n",
    "    potential_documents = metadata\n",
    "    if start_date:\n",
    "        potential_documents = potential_documents[potential_documents[\"date\"] >= start_date]\n",
    "    if end_date:\n",
    "        potential_documents = potential_documents[potential_documents[\"date\"] >= end_date]\n",
    "    potential_documents = potential_documents[[(c in municipalities) for c in potential_documents[\"city\"]]]\n",
    "    # filter all docs to only filenames in subset of metadata\n",
    "    filenames = list(potential_documents['local_path_txt'])\n",
    "    urls = list(potential_documents['url'])\n",
    "    docs_to_return = []\n",
    "    for i in range(len(filenames)):\n",
    "        f = filenames[i]\n",
    "        u = urls[i]\n",
    "        if f in all_docs:\n",
    "            docs_to_return.append({**all_docs[f], 'filename':f, 'url':u})\n",
    "    # return [{**all_docs[f], 'filename':f, 'url':\"example.com\"} for f in potential_documents['local_path_txt'] if f in all_docs]\n",
    "    return docs_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_docs(relevant_docs):\n",
    "    min_section_length = 50 # tokens\n",
    "    # TODO: this cuts of end of doc\n",
    "    \n",
    "    sections = []\n",
    "    for doc in relevant_docs:\n",
    "        original_text = doc[\"original_text\"]\n",
    "        pages = original_text.split('\\f')\n",
    "        page_numbers = []\n",
    "        for p, page in enumerate(pages):\n",
    "            page_sentences = sentence_split(page)\n",
    "            # for each sentence, what page was it on?\n",
    "            for sentence in page_sentences:\n",
    "                sentence_tokens = tokenize(sentence)\n",
    "                page_numbers.append(p+1)\n",
    "        doc_sentences = doc[\"sentences\"]\n",
    "        doc_sentences_with_extra = doc[\"vectors\"][\"sentences\"]\n",
    "        doc_vectors_with_extra = doc[\"vectors\"][\"vectors\"]\n",
    "        nonempty_sentence_indices = [i for i in range(len(doc_sentences_with_extra)) if len(doc_sentences_with_extra[i].strip())>0]\n",
    "        doc_vectors = [doc_vectors_with_extra[i] for i in nonempty_sentence_indices]\n",
    "        section = []\n",
    "        section_tokens = 0\n",
    "        if (len(doc_sentences) == len(doc_vectors)):\n",
    "            for i in range(len(doc_sentences)):\n",
    "                sentence = doc_sentences[i]\n",
    "                page = page_numbers[i]\n",
    "                sentence_vectors = doc_vectors[i]\n",
    "                sentence_tokens = tokenize(sentence)\n",
    "                section.append({\n",
    "                    \"sentence\": sentence,\n",
    "                    \"page\": page,\n",
    "                    \"sentence_vectors\": sentence_vectors,\n",
    "                    \"sentence_tokens\": sentence_tokens\n",
    "                })\n",
    "                section_tokens += len(sentence_tokens)\n",
    "                if section_tokens >= min_section_length:\n",
    "                    section_text = \". \".join([s[\"sentence\"].strip() for s in section])\n",
    "                    sections.append({\n",
    "                        \"sentences\": section,\n",
    "                        \"section_text\": section_text,\n",
    "                        \"filename\": doc[\"filename\"],\n",
    "                        \"url\": doc[\"url\"]\n",
    "                    })\n",
    "                    section = []\n",
    "                    section_tokens = 0\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_is_too_similar(a, b):\n",
    "    # if there are only 2 edits to get from one text to the other, it's not good\n",
    "    return editdistance.eval(a, b) < 50\n",
    "\n",
    "# make sure we're not giving similar text among the top k (e.g. shows up on both minutes and agenda)\n",
    "def check_repeated_text(top_k, section_text):\n",
    "    for old in top_k:\n",
    "        if text_is_too_similar(old[1][\"section_text\"], section_text):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def select_top_k(doc_sections, doc_sections_scores, k):\n",
    "    sorted_sections = sorted(zip(doc_sections_scores, doc_sections), key=lambda pair: pair[0], reverse=True)\n",
    "    top_k = []\n",
    "    text_returned = []\n",
    "    for x in sorted_sections:\n",
    "        score = x[0]\n",
    "        if score > 0:\n",
    "            filename = x[1][\"filename\"]\n",
    "            starting_page = x[1][\"sentences\"][0][\"page\"]\n",
    "            ending_page = x[1][\"sentences\"][-1][\"page\"]\n",
    "            section_text = x[1][\"section_text\"]\n",
    "            if check_repeated_text(top_k, section_text):\n",
    "                pass\n",
    "#                 print(\"this excpert has already been returned\")\n",
    "            else:\n",
    "                top_k.append(x)\n",
    "            if len(top_k) >= k:\n",
    "                break\n",
    "    return top_k\n",
    "\n",
    "def update_with_top_k(results, top_k_sections, query):\n",
    "    for section in top_k_sections:\n",
    "        x = section[1]\n",
    "        x.update(query)\n",
    "        x[\"start_page\"] = x[\"sentences\"][0][\"page\"]\n",
    "        x[\"sentences\"] = []\n",
    "        results.append(x)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_vector_per_doc(vectors):\n",
    "    # vectors is a list of np arrays where:\n",
    "    # dims: (LAYERS(3), TOKENS(varies), DIMENSIONS(1024))\n",
    "    \"\"\"\n",
    "    https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f\n",
    "    In the ELMo paper, there are 3 layers of word embedding,\n",
    "    layer zero is the character-based context independent layer,\n",
    "    followed by two Bi-LSTM layers. The authors have empirically\n",
    "    shown that the word vectors generated from the first Bi-LSTM\n",
    "    layer can better capture the syntax, and the second layer can\n",
    "    capture the semantics better.\n",
    "    \"\"\"\n",
    "    vectors = np.concatenate([v[2] for v in vectors], 0)\n",
    "    return vectors\n",
    "\n",
    "def score_doc_sections(doc_sections, orig_keywords, elmo):\n",
    "    orig_keywords = [k.strip() for k in orig_keywords]\n",
    "    keywords = []\n",
    "    for k in orig_keywords:\n",
    "        words = k.split(\" \")\n",
    "        for word in words:\n",
    "            keywords.append(word)\n",
    "    keyword_vectors = single_vector_per_doc([elmo.embed_sentence(keywords)])\n",
    "    # keyword_weights = []\n",
    "    # fix_case = casing_function()\n",
    "    # idf_smoothing_count = 10\n",
    "    # for k in keywords:\n",
    "    #     words = k.split(\" \")\n",
    "    #     if len(words) > 1:\n",
    "    #         keyword_weights.append(1./idf_smoothing_count)\n",
    "    #     else:\n",
    "    #         k = fix_case(k)\n",
    "    #         if k in idf:\n",
    "    #             keyword_weights.append(1./(1./idf[k]+idf_smoothing_count))\n",
    "    #         else:\n",
    "    #             keyword_weights.append(1./idf_smoothing_count)\n",
    "    doc_sections_scores = []\n",
    "    for s, section in enumerate(doc_sections):\n",
    "        section_vectors = single_vector_per_doc([s[\"sentence_vectors\"] for s in section[\"sentences\"]])\n",
    "        section_text = section['section_text']\n",
    "        no_keywords_found = True\n",
    "        for k in orig_keywords:\n",
    "            if bool(re.search(\"([^\\w]|^)\" + k + \"([^\\w]|$)\", section_text)):\n",
    "            # if k in section_text:\n",
    "            #     TODO: consider casing\n",
    "                no_keywords_found = False\n",
    "        for k in orig_keywords:\n",
    "            if k.islower():\n",
    "                if bool(re.search(\"([^\\w]|^)\" + k + \"([^\\w]|$)\", section_text.lower())):\n",
    "                    no_keywords_found = False\n",
    "        if no_keywords_found:\n",
    "            score = 0\n",
    "        elif section_vectors.shape[0]>0:\n",
    "            similarities = cosine_similarity(section_vectors, keyword_vectors)\n",
    "            # if threshold_similarity > -1:\n",
    "            #     similarities = similarities*(similarities>threshold_similarity)\n",
    "            keyword_similarities = np.mean(similarities, axis=0)\n",
    "            # score = np.sum(keyword_similarities*keyword_weights)\n",
    "            score = np.mean(keyword_similarities)\n",
    "        else:\n",
    "            score = 0\n",
    "        doc_sections_scores.append(score)\n",
    "\n",
    "    return doc_sections_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "queries = [\n",
    "    {\n",
    "        \"keywords\": [\"housing\", \"affordable housing\", \"homelessness\", \"auxiliary dwelling unit\", \"ADU\"],\n",
    "        \"municipalities\": [\"San Jose\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "def recommender_stats(start_date, end_date):\n",
    "    n_unique_docs = None\n",
    "    n_recalled_docs = None\n",
    "    \n",
    "    relevant_filenames = find_relevant_filenames(\n",
    "    queries,\n",
    "    metadata,\n",
    "    start_date = start_date,\n",
    "    end_date = end_date,\n",
    "    agenda_only=True)\n",
    "#     print(\"{} relevant documents found.\".format(len(relevant_filenames)))\n",
    "#     print(\"reading relevant documents\")\n",
    "    all_docs = read_docs(relevant_filenames)\n",
    "#     print(\"{} documents read\".format(len(all_docs)))\n",
    "    \n",
    "    results = []\n",
    "    for q, query in enumerate(queries):\n",
    "#         print(\"running query {} of {}\".format(q, len(queries)))\n",
    "    #     user_id = query[\"id\"]\n",
    "    #     email_address = query[\"email_address\"]\n",
    "    #     print(\"email address: {}\".format(email_address))\n",
    "        keywords = query[\"keywords\"]\n",
    "#         print(\"keywords: {}\".format(keywords))\n",
    "        municipalities = query[\"municipalities\"]\n",
    "#         print(\"municipalities: {}\".format(municipalities))\n",
    "        relevant_docs = select_relevant_docs(municipalities, all_docs, metadata)\n",
    "#         print(\"{} relevant documents identified for this query\".format(len(relevant_docs)))\n",
    "#         print(\"segmenting documents\")\n",
    "        doc_sections = segment_docs(relevant_docs)\n",
    "#         print(\"{} document sections to choose from\".format(len(doc_sections)))\n",
    "#         print(\"scoring documents\")\n",
    "        doc_sections_scores = score_doc_sections(\n",
    "            doc_sections,\n",
    "            keywords,\n",
    "            elmo\n",
    "        )\n",
    "        top_k_sections = select_top_k(doc_sections, doc_sections_scores, k)\n",
    "        results = update_with_top_k(results, top_k_sections, query)\n",
    "#         for r in results:\n",
    "#             print(\"~~~\")\n",
    "#             print(r[\"section_text\"])\n",
    "#             print(\"~~~\")\n",
    "#         print(\"~~~~~~~~~~~~~~~\")\n",
    "        \n",
    "        relevant_filenames_for_emily = [\n",
    "            \"docs/san-jose/San-Jose_2019-11-05_City-Council_Agenda.txt\",\n",
    "            \"docs/san-jose/San-Jose_2019-09-24_City-Council_Agenda.txt\",\n",
    "            \"docs/san-jose/San-Jose_2019-10-01_City-Council_Agenda.txt\"\n",
    "        ]\n",
    "        documents = set([x[\"filename\"] for x in results])\n",
    "#         print(documents)\n",
    "        n_unique_docs =len(documents)\n",
    "        docs_found = 0\n",
    "        for filename in relevant_filenames_for_emily:\n",
    "            if filename in documents:\n",
    "                print(filename)\n",
    "                docs_found += 1\n",
    "        n_recalled_docs = docs_found\n",
    "    \n",
    "    return n_unique_docs, n_recalled_docs, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 relevant documents found.\n",
      "reading relevant documents\n",
      "18 documents read\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "start_date = datetime.strptime(\"2019-09-02\", '%Y-%m-%d')\n",
    "end_date = start_date + timedelta(weeks=2)\n",
    "queries = [\n",
    "    {\n",
    "#         \"keywords\": [\"housing\", \"affordable housing\", \"homelessness\", \"accessory dwelling unit\", \"ADU\"],\n",
    "        \"keywords\": [\"affordable housing\", \"homelessness\", \"ADU\"],\n",
    "        \"municipalities\": [\"San Jose\"]\n",
    "    }\n",
    "]\n",
    "relevant_filenames = find_relevant_filenames(\n",
    "    queries,\n",
    "    metadata,\n",
    "    start_date = start_date,\n",
    "    end_date = end_date,\n",
    "    agenda_only=True)\n",
    "print(\"{} relevant documents found.\".format(len(relevant_filenames)))\n",
    "print(\"reading relevant documents\")\n",
    "all_docs = read_docs(relevant_filenames)\n",
    "print(\"{} documents read\".format(len(all_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running query 0 of 1\n",
      "keywords: ['housing', 'affordable housing', 'homelessness', 'accessory dwelling unit', 'ADU']\n",
      "municipalities: ['San Jose']\n",
      "18 relevant documents identified for this query\n",
      "segmenting documents\n",
      "405 document sections to choose from\n",
      "scoring documents\n",
      "[{'sentences': [], 'section_text': 'housing opportunities. Neighborhood Services - Serve, foster, and strengthen community by providing access to lifelong. learning and opportunities to enjoy life. Transportation & Aviation Services - A safe and efficient transportation system that contributes to. the livability and economic health of the City; and provide for the air transportation needs of the. community and the region at levels that is acceptable to the community', 'filename': 'docs/san-jose/San-Jose_2019-09-10_City-Council_Agenda.txt', 'url': 'https://sanjose.legistar.com/View.ashx?M=A&ID=717966&GUID=D9B9B02A-F925-487C-9CD4-C874ED77625A', 'keywords': ['housing', 'affordable housing', 'homelessness', 'accessory dwelling unit', 'ADU'], 'municipalities': ['San Jose'], 'start_page': 2}, {'sentences': [], 'section_text': '(1) Authorizing the Director of Housing to negotiate and execute an. agreement with the Housing Trust Silicon Valley to provide funding for. forgivable loans to homeowners in the City of San Jos who build a. legal accessory dwelling unit and agree to house an income eligible. household for a period of five years; and', 'filename': 'docs/san-jose/San-Jose_2019-09-10_City-Council_Agenda.txt', 'url': 'https://sanjose.legistar.com/View.ashx?M=A&ID=717966&GUID=D9B9B02A-F925-487C-9CD4-C874ED77625A', 'keywords': ['housing', 'affordable housing', 'homelessness', 'accessory dwelling unit', 'ADU'], 'municipalities': ['San Jose'], 'start_page': 16}, {'sentences': [], 'section_text': 'appropriation of funds. (b) Adopt the following 2019-2020 Appropriation Ordinance. Amendments in the General Fund:. (1) Decrease the Homeless Rapid Rehousing City-Wide. appropriation to the Housing Department by $1,250,000; and. (2) Establish an ADU Forgivable Loan Program City-Wide. appropriation to the Housing Department in the amount of $1,250,000', 'filename': 'docs/san-jose/San-Jose_2019-09-10_City-Council_Agenda.txt', 'url': 'https://sanjose.legistar.com/View.ashx?M=A&ID=717966&GUID=D9B9B02A-F925-487C-9CD4-C874ED77625A', 'keywords': ['housing', 'affordable housing', 'homelessness', 'accessory dwelling unit', 'ADU'], 'municipalities': ['San Jose'], 'start_page': 16}, {'sentences': [], 'section_text': \"Workshop pilot  program, device lending pilot program, and public. computer and Wi-Fi data usage. Memorandum. FY 2018-2019 Consolidated Annual Performance Evaluation Report. (CAPER). (Housing). 1. Accept the report on the progress towards achieving the housing and. community development goals identified in the City's five-year. Consolidated Plan (2015-2020) and the FY 2018-2019 Annual Action\", 'filename': 'docs/san-jose/San-Jose_2019-09-12_Neighborhood-Services-And-Education-Committee-(Nse)_Agenda.txt', 'url': 'https://sanjose.legistar.com/View.ashx?M=A&ID=711316&GUID=A9C762B2-F43E-4C99-BDFA-D1A62ED6CECD', 'keywords': ['housing', 'affordable housing', 'homelessness', 'accessory dwelling unit', 'ADU'], 'municipalities': ['San Jose'], 'start_page': 3}, {'sentences': [], 'section_text': 'Housing Department by $580,000. (d) Direct the Administration to work with the City Attorneys Office to. return to Council for consideration of an ordinance or resolution. suspending or waiving the collection of the annual business tax and park. impact in-lieu fees for certain qualifying accessory dwelling units and', 'filename': 'docs/san-jose/San-Jose_2019-09-10_City-Council_Agenda.txt', 'url': 'https://sanjose.legistar.com/View.ashx?M=A&ID=717966&GUID=D9B9B02A-F925-487C-9CD4-C874ED77625A', 'keywords': ['housing', 'affordable housing', 'homelessness', 'accessory dwelling unit', 'ADU'], 'municipalities': ['San Jose'], 'start_page': 16}]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for q, query in enumerate(queries):\n",
    "    print(\"running query {} of {}\".format(q, len(queries)))\n",
    "#     user_id = query[\"id\"]\n",
    "#     email_address = query[\"email_address\"]\n",
    "#     print(\"email address: {}\".format(email_address))\n",
    "    keywords = query[\"keywords\"]\n",
    "    print(\"keywords: {}\".format(keywords))\n",
    "    municipalities = query[\"municipalities\"]\n",
    "    print(\"municipalities: {}\".format(municipalities))\n",
    "    relevant_docs = select_relevant_docs(municipalities, all_docs, metadata)\n",
    "    print(\"{} relevant documents identified for this query\".format(len(relevant_docs)))\n",
    "    print(\"segmenting documents\")\n",
    "    doc_sections = segment_docs(relevant_docs)\n",
    "    print(\"{} document sections to choose from\".format(len(doc_sections)))\n",
    "    print(\"scoring documents\")\n",
    "    doc_sections_scores = score_doc_sections(\n",
    "        doc_sections,\n",
    "        keywords,\n",
    "        elmo\n",
    "    )\n",
    "    top_k_sections = select_top_k(doc_sections, doc_sections_scores, k)\n",
    "    results = update_with_top_k(results, top_k_sections, query)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_filenames_for_emily = [\n",
    "    \"docs/san-jose/San-Jose_2019-11-05_City-Council_Agenda.txt\",\n",
    "    \"docs/san-jose/San-Jose_2019-09-24_City-Council_Agenda.txt\",\n",
    "    \"docs/san-jose/San-Jose_2019-10-01_City-Council_Agenda.txt\"\n",
    "]\n",
    "documents = [x[\"filename\"] for x in results]\n",
    "len(set(documents))\n",
    "docs_found = 0\n",
    "for filename in relevant_filenames_for_emily:\n",
    "    if filename in documents:\n",
    "        print(filename)\n",
    "        docs_found += 1\n",
    "docs_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/02 & 09/16 & 18 & 15 & 2 \\\\\n",
      "09/05 & 09/19 & 22 & 16 & 2 \\\\\n",
      "09/09 & 09/23 & 20 & 15 & 2 \\\\\n",
      "docs/san-jose/San-Jose_2019-09-24_City-Council_Agenda.txt\n",
      "09/12 & 09/26 & 18 & 14 & 2 \\\\\n",
      "docs/san-jose/San-Jose_2019-09-24_City-Council_Agenda.txt\n",
      "09/16 & 09/30 & 17 & 13 & 2 \\\\\n",
      "docs/san-jose/San-Jose_2019-09-24_City-Council_Agenda.txt\n",
      "09/19 & 10/03 & 22 & 17 & 2 \\\\\n",
      "docs/san-jose/San-Jose_2019-09-24_City-Council_Agenda.txt\n",
      "09/23 & 10/07 & 24 & 19 & 2 \\\\\n",
      "09/26 & 10/10 & 25 & 18 & 3 \\\\\n",
      "09/30 & 10/14 & 25 & 17 & 3 \\\\\n",
      "10/03 & 10/17 & 26 & 17 & 5 \\\\\n",
      "10/07 & 10/21 & 23 & 14 & 5 \\\\\n",
      "10/10 & 10/24 & 20 & 13 & 3 \\\\\n",
      "10/14 & 10/28 & 19 & 13 & 2 \\\\\n",
      "10/17 & 10/31 & 18 & 11 & 3 \\\\\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.strptime(\"2019-09-02\", '%Y-%m-%d')\n",
    "dow = \"Monday\"\n",
    "last_start_date = datetime.strptime(\"2019-10-17\", '%Y-%m-%d')\n",
    "\n",
    "emily_story_dates = [datetime.strptime(x, '%Y-%m-%d') for x in [\n",
    "    \"2019-11-05\",\n",
    "    \"2019-09-24\",\n",
    "    \"2019-10-01\"\n",
    "]]\n",
    "\n",
    "relevant_filenames_for_emily = [\n",
    "    \"docs/san-jose/San-Jose_2019-11-05_City-Council_Agenda.txt\",\n",
    "    \"docs/san-jose/San-Jose_2019-09-24_City-Council_Agenda.txt\",\n",
    "    \"docs/san-jose/San-Jose_2019-10-01_City-Council_Agenda.txt\"\n",
    "]\n",
    "\n",
    "queries = [\n",
    "    {\n",
    "#         \"keywords\": [\"housing\", \"affordable housing\", \"homelessness\", \"accessory dwelling unit\", \"ADU\"],\n",
    "#         \"keywords\": [\"housing\", \"affordable housing\", \"homelessness\", \"ADU\"],\n",
    "#         \"keywords\": [\"housing\", \"affordable housing\", \"homeless\", \"homelessness\" \"accessory dwelling unit\", \"ADU\"],\n",
    "        \"keywords\": [\"housing\", \"affordable housing\", \"homelessness\", \"ADU\"],\n",
    "        \"municipalities\": [\"San Jose\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "while start_date <= last_start_date:\n",
    "    end_date = start_date + timedelta(weeks=2)\n",
    "    \n",
    "    story_dates_in_this_time_window = [x for x in emily_story_dates if x >= start_date and x <= end_date]\n",
    "    num_articles = len(story_dates_in_this_time_window)\n",
    "    \n",
    "    subset_metadata = metadata\n",
    "    subset_metadata = subset_metadata[subset_metadata[\"date\"] >= start_date]\n",
    "    subset_metadata = subset_metadata[subset_metadata[\"date\"] <= end_date]\n",
    "    subset_metadata = subset_metadata[subset_metadata[\"doc_type\"] == \"Agenda\"]\n",
    "    subset_metadata = subset_metadata[subset_metadata[\"city\"] == \"San Jose\"]\n",
    "    committees = set(subset_metadata[\"committee\"])\n",
    "    \n",
    "    \n",
    "    total, found, results = recommender_stats(start_date, end_date)\n",
    "    all_results.append({\n",
    "        \"results\": results,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date\n",
    "    })\n",
    "#     print(committees)\n",
    "    print(\n",
    "        \"{} & {} & {} & {} & {} \\\\\\\\\".format(\n",
    "#         \"{} & {} & {} & {} & {} & {} & {} \\\\\\\\\".format(\n",
    "#         \"start: {}, end: {}, num_agendas: {}, num_articles: {}\".format(\n",
    "            start_date.strftime(\"%m/%d\"),\n",
    "            end_date.strftime(\"%m/%d\"),\n",
    "            subset_metadata.shape[0],\n",
    "            len(committees),\n",
    "#             num_articles,\n",
    "#             found,\n",
    "            total\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if dow == \"Monday\":\n",
    "        start_date = start_date + timedelta(days=3)\n",
    "        dow = \"Thursday\"\n",
    "    else:\n",
    "        start_date = start_date - timedelta(days=3) + timedelta(weeks=1)\n",
    "        dow = \"Monday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'results': [{'sentences': [],\n",
       "    'section_text': 'housing opportunities. Neighborhood Services - Serve, foster, and strengthen community by providing access to lifelong. learning and opportunities to enjoy life. Transportation & Aviation Services - A safe and efficient transportation system that contributes to. the livability and economic health of the City; and provide for the air transportation needs of the. community and the region at levels that is acceptable to the community',\n",
       "    'filename': 'docs/san-jose/San-Jose_2019-10-01_City-Council_Agenda.txt',\n",
       "    'url': 'https://sanjose.legistar.com/View.ashx?M=A&ID=709097&GUID=45DA4900-8361-4625-9640-0A7C36E69022',\n",
       "    'keywords': ['housing', 'affordable housing', 'homelessness', 'ADU'],\n",
       "    'municipalities': ['San Jose'],\n",
       "    'start_page': 2},\n",
       "   {'sentences': [],\n",
       "    'section_text': 'and Changes to Existing Loan and Grant Terms for the Markham Plaza I. Project. (a) Adopt a resolution:. (1) Authorizing the issuance of (a) tax-exempt multifamily housing. revenue bonds designated as City of San Jos Multifamily Housing. Revenue Bonds (Markham Plaza I), Series 2019B-1 (the Series. 2019B-1 Bonds) and City of San Jos Multifamily Housing Revenue',\n",
       "    'filename': 'docs/san-jose/San-Jose_2019-10-01_City-Council_Agenda.txt',\n",
       "    'url': 'https://sanjose.legistar.com/View.ashx?M=A&ID=709097&GUID=45DA4900-8361-4625-9640-0A7C36E69022',\n",
       "    'keywords': ['housing', 'affordable housing', 'homelessness', 'ADU'],\n",
       "    'municipalities': ['San Jose'],\n",
       "    'start_page': 13},\n",
       "   {'sentences': [],\n",
       "    'section_text': 'Project. (a) Adopt a resolution:. (1) Authorizing the issuance of (a) tax-exempt multifamily housing. revenue bonds designated as City of San Jos Multifamily Housing. Revenue Bonds (Markham Plaza I), Series 2019B-1 (the Series. 2019B-1 Bonds) and City of San Jos Multifamily Housing Revenue. Bonds (Markham Plaza I), Subordinate Series 2019B-2 (the Series',\n",
       "    'filename': 'docs/san-jose/San-Jose_2019-09-24_City-Council_Agenda.txt',\n",
       "    'url': 'https://sanjose.legistar.com/View.ashx?M=A&ID=709096&GUID=CF2165D1-28CB-4670-AB54-F35B649DE71D',\n",
       "    'keywords': ['housing', 'affordable housing', 'homelessness', 'ADU'],\n",
       "    'municipalities': ['San Jose'],\n",
       "    'start_page': 15},\n",
       "   {'sentences': [],\n",
       "    'section_text': 'Housing Impact Fee exemption to December 31, 2023. (b) Amending Title 4. 46 and align the construction tax reduction with. the certificate of occupancy deadline for the Affordable Housing Impact. Fee exemption, and removing the planning and build permit. requirements. CEQA: Not a Project, File No. PP17-009, Staff Reports, Assessments,',\n",
       "    'filename': 'docs/san-jose/San-Jose_2019-09-24_City-Council_Agenda.txt',\n",
       "    'url': 'https://sanjose.legistar.com/View.ashx?M=A&ID=709096&GUID=CF2165D1-28CB-4670-AB54-F35B649DE71D',\n",
       "    'keywords': ['housing', 'affordable housing', 'homelessness', 'ADU'],\n",
       "    'municipalities': ['San Jose'],\n",
       "    'start_page': 11},\n",
       "   {'sentences': [],\n",
       "    'section_text': 'development and allowed by other funds and authorizing the Director of. Housing to negotiate and execute loan documents, amendments, and all. other documents related to these changes; and. (b) Authorizing the Director to negotiate and execute amendments to the. HOME restriction and HOME Agreement, as needed, to ensure. compliance with Housing and Urban Development (HUD) HOME',\n",
       "    'filename': 'docs/san-jose/San-Jose_2019-09-24_City-Council_Agenda.txt',\n",
       "    'url': 'https://sanjose.legistar.com/View.ashx?M=A&ID=709096&GUID=CF2165D1-28CB-4670-AB54-F35B649DE71D',\n",
       "    'keywords': ['housing', 'affordable housing', 'homelessness', 'ADU'],\n",
       "    'municipalities': ['San Jose'],\n",
       "    'start_page': 10}],\n",
       "  'start_date': datetime.datetime(2019, 9, 19, 0, 0),\n",
       "  'end_date': datetime.datetime(2019, 10, 3, 0, 0)},\n",
       " {'results': [{'sentences': [],\n",
       "    'section_text': 'housing opportunities. Neighborhood Services - Serve, foster, and strengthen community by providing access to lifelong. learning and opportunities to enjoy life. Transportation & Aviation Services - A safe and efficient transportation system that contributes to. the livability and economic health of the City; and provide for the air transportation needs of the. community and the region at levels that is acceptable to the community',\n",
       "    'filename': 'docs/san-jose/San-Jose_2019-09-24_City-Council_Agenda.txt',\n",
       "    'url': 'https://sanjose.legistar.com/View.ashx?M=A&ID=709096&GUID=CF2165D1-28CB-4670-AB54-F35B649DE71D',\n",
       "    'keywords': ['housing', 'affordable housing', 'homelessness', 'ADU'],\n",
       "    'municipalities': ['San Jose'],\n",
       "    'start_page': 2},\n",
       "   {'sentences': [],\n",
       "    'section_text': 'and Changes to Existing Loan and Grant Terms for the Markham Plaza I. Project. (a) Adopt a resolution:. (1) Authorizing the issuance of (a) tax-exempt multifamily housing. revenue bonds designated as City of San Jos Multifamily Housing. Revenue Bonds (Markham Plaza I), Series 2019B-1 (the Series. 2019B-1 Bonds) and City of San Jos Multifamily Housing Revenue',\n",
       "    'filename': 'docs/san-jose/San-Jose_2019-10-01_City-Council_Agenda.txt',\n",
       "    'url': 'https://sanjose.legistar.com/View.ashx?M=A&ID=709097&GUID=45DA4900-8361-4625-9640-0A7C36E69022',\n",
       "    'keywords': ['housing', 'affordable housing', 'homelessness', 'ADU'],\n",
       "    'municipalities': ['San Jose'],\n",
       "    'start_page': 13},\n",
       "   {'sentences': [],\n",
       "    'section_text': 'Project. (a) Adopt a resolution:. (1) Authorizing the issuance of (a) tax-exempt multifamily housing. revenue bonds designated as City of San Jos Multifamily Housing. Revenue Bonds (Markham Plaza I), Series 2019B-1 (the Series. 2019B-1 Bonds) and City of San Jos Multifamily Housing Revenue. Bonds (Markham Plaza I), Subordinate Series 2019B-2 (the Series',\n",
       "    'filename': 'docs/san-jose/San-Jose_2019-09-24_City-Council_Agenda.txt',\n",
       "    'url': 'https://sanjose.legistar.com/View.ashx?M=A&ID=709096&GUID=CF2165D1-28CB-4670-AB54-F35B649DE71D',\n",
       "    'keywords': ['housing', 'affordable housing', 'homelessness', 'ADU'],\n",
       "    'municipalities': ['San Jose'],\n",
       "    'start_page': 15},\n",
       "   {'sentences': [],\n",
       "    'section_text': 'Housing Impact Fee exemption to December 31, 2023. (b) Amending Title 4. 46 and align the construction tax reduction with. the certificate of occupancy deadline for the Affordable Housing Impact. Fee exemption, and removing the planning and build permit. requirements. CEQA: Not a Project, File No. PP17-009, Staff Reports, Assessments,',\n",
       "    'filename': 'docs/san-jose/San-Jose_2019-09-24_City-Council_Agenda.txt',\n",
       "    'url': 'https://sanjose.legistar.com/View.ashx?M=A&ID=709096&GUID=CF2165D1-28CB-4670-AB54-F35B649DE71D',\n",
       "    'keywords': ['housing', 'affordable housing', 'homelessness', 'ADU'],\n",
       "    'municipalities': ['San Jose'],\n",
       "    'start_page': 11},\n",
       "   {'sentences': [],\n",
       "    'section_text': 'development and allowed by other funds and authorizing the Director of. Housing to negotiate and execute loan documents, amendments, and all. other documents related to these changes; and. (b) Authorizing the Director to negotiate and execute amendments to the. HOME restriction and HOME Agreement, as needed, to ensure. compliance with Housing and Urban Development (HUD) HOME',\n",
       "    'filename': 'docs/san-jose/San-Jose_2019-09-24_City-Council_Agenda.txt',\n",
       "    'url': 'https://sanjose.legistar.com/View.ashx?M=A&ID=709096&GUID=CF2165D1-28CB-4670-AB54-F35B649DE71D',\n",
       "    'keywords': ['housing', 'affordable housing', 'homelessness', 'ADU'],\n",
       "    'municipalities': ['San Jose'],\n",
       "    'start_page': 10}],\n",
       "  'start_date': datetime.datetime(2019, 9, 23, 0, 0),\n",
       "  'end_date': datetime.datetime(2019, 10, 7, 0, 0)}]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['housing',\n",
       " 'affordable housing',\n",
       " 'homelessness',\n",
       " 'accessory dwelling unit',\n",
       " 'ADU']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/28 & 10/12 & 25 & 17 & 3 \\\\\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.strptime(\"2019-09-28\", '%Y-%m-%d')\n",
    "end_date = datetime.strptime(\"2019-10-12\", '%Y-%m-%d')\n",
    "\n",
    "queries = [\n",
    "    {\n",
    "#         \"keywords\": [\"housing\", \"affordable housing\", \"homelessness\", \"accessory dwelling unit\", \"ADU\"],\n",
    "        \"keywords\": [\"housing\", \"affordable housing\", \"homelessness\", \"affordable dwelling unit\", \"ADU\"],\n",
    "#         \"keywords\": [\"affordable housing\", \"homelessness\", \"ADU\"],\n",
    "        \"municipalities\": [\"San Jose\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "emily_story_dates = [datetime.strptime(x, '%Y-%m-%d') for x in [\n",
    "    \"2019-11-05\",\n",
    "    \"2019-09-24\",\n",
    "    \"2019-10-01\"\n",
    "]]\n",
    "\n",
    "relevant_filenames_for_emily = [\n",
    "    \"docs/san-jose/San-Jose_2019-11-05_City-Council_Agenda.txt\",\n",
    "    \"docs/san-jose/San-Jose_2019-09-24_City-Council_Agenda.txt\",\n",
    "    \"docs/san-jose/San-Jose_2019-10-01_City-Council_Agenda.txt\"\n",
    "]\n",
    "\n",
    "queries = [\n",
    "    {\n",
    "#         \"keywords\": [\"housing\", \"affordable housing\", \"homelessness\", \"accessory dwelling unit\", \"ADU\"],\n",
    "        \"keywords\": [\"housing\", \"affordable housing\", \"homelessness\", \"ADU\"],\n",
    "#         \"keywords\": [\"affordable housing\", \"homelessness\", \"ADU\"],\n",
    "        \"municipalities\": [\"San Jose\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "end_date = start_date + timedelta(weeks=2)\n",
    "\n",
    "story_dates_in_this_time_window = [x for x in emily_story_dates if x >= start_date and x <= end_date]\n",
    "num_articles = len(story_dates_in_this_time_window)\n",
    "\n",
    "subset_metadata = metadata\n",
    "subset_metadata = subset_metadata[subset_metadata[\"date\"] >= start_date]\n",
    "subset_metadata = subset_metadata[subset_metadata[\"date\"] <= end_date]\n",
    "subset_metadata = subset_metadata[subset_metadata[\"doc_type\"] == \"Agenda\"]\n",
    "subset_metadata = subset_metadata[subset_metadata[\"city\"] == \"San Jose\"]\n",
    "committees = set(subset_metadata[\"committee\"])\n",
    "\n",
    "\n",
    "total, found, results = recommender_stats(start_date, end_date)\n",
    "print(\n",
    "    \"{} & {} & {} & {} & {} \\\\\\\\\".format(\n",
    "#         \"{} & {} & {} & {} & {} & {} & {} \\\\\\\\\".format(\n",
    "#         \"start: {}, end: {}, num_agendas: {}, num_articles: {}\".format(\n",
    "        start_date.strftime(\"%m/%d\"),\n",
    "        end_date.strftime(\"%m/%d\"),\n",
    "        subset_metadata.shape[0],\n",
    "        len(committees),\n",
    "#             num_articles,\n",
    "#             found,\n",
    "        total\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
