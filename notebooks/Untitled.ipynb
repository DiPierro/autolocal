{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from io import BytesIO\n",
    "from autolocal.parsers.nlp import Tokenizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "from  tqdm import tqdm\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "from autolocal.emailer import send_emails\n",
    "import editdistance\n",
    "import re\n",
    "\n",
    "def single_vector_per_doc(vectors):\n",
    "    # vectors is a list of np arrays where:\n",
    "    # dims: (LAYERS(3), TOKENS(varies), DIMENSIONS(1024))\n",
    "    \"\"\"\n",
    "    https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f\n",
    "    In the ELMo paper, there are 3 layers of word embedding,\n",
    "    layer zero is the character-based context independent layer,\n",
    "    followed by two Bi-LSTM layers. The authors have empirically\n",
    "    shown that the word vectors generated from the first Bi-LSTM\n",
    "    layer can better capture the syntax, and the second layer can\n",
    "    capture the semantics better.\n",
    "    \"\"\"\n",
    "    vectors = np.concatenate([v[2] for v in vectors], 0)\n",
    "    return vectors\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "autolocal_docs_bucket = s3.Bucket('autolocal-documents')\n",
    "def read_doc(s3_path):\n",
    "    try:\n",
    "        return autolocal_docs_bucket.Object(s3_path).get()['Body'].read().decode(\"ascii\", \"ignore\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def read_metadata():\n",
    "    table = boto3.resource('dynamodb', region_name='us-west-1').Table('autolocal-documents')\n",
    "    s3_client = boto3.client('s3')\n",
    "    metadata = pd.DataFrame(table.scan()[\"Items\"])\n",
    "    metadata[\"date\"] = [datetime.strptime(d, '%Y-%m-%d') for d in metadata[\"date\"]]\n",
    "    metadata['local_path_pkl'] = metadata['local_path_txt'].apply(lambda x: x[:-3]+\"pkl\")\n",
    "    return metadata\n",
    "metadata = read_metadata()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def read(s3_path):\n",
    "    # print(os.path.join(\"../data/pkls/\", os.path.basename(s3_path)))\n",
    "    return pickle.load(open(os.path.join(\"../data/pkls/\", os.path.basename(s3_path)), 'rb'))\n",
    "\n",
    "def write(array, s3_path):\n",
    "    pickle.dump(array, open(os.path.join(\"../data/pkls/\", os.path.basename(s3_path)), 'wb'))\n",
    "\n",
    "def sentence_split(s):\n",
    "    sentences = re.split('[.\\n!?\"\\f]', s)\n",
    "    return [s for s in sentences if len(s.strip())>0]\n",
    "\n",
    "def tokenize(s):\n",
    "    tokens = re.findall(r'\\w+', s)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "starting_dates_for_filtering = {\n",
    "    'upcoming_only': datetime.now() + timedelta(days=0.5),\n",
    "    'upcoming': datetime.now() + timedelta(days=0.5),\n",
    "    'this_week': datetime.now() - timedelta(weeks=1),\n",
    "    'this_year': datetime.now() - timedelta(days=365),\n",
    "    'this_month': datetime.now() - timedelta(weeks=5),\n",
    "    'past_six_months':datetime.now() - timedelta(days=183),\n",
    "    'last_six_months':datetime.now() - timedelta(days=183),\n",
    "    'past': None,\n",
    "    'all': None\n",
    "}\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def read_queries():\n",
    "    table = boto3.resource('dynamodb', region_name='us-west-2').Table('autoLocalNews')\n",
    "    queries = table.scan()[\"Items\"]\n",
    "    return queries\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def find_relevant_filenames(queries, metadata): \n",
    "    \n",
    "    # filter metadata to only those files that match the query municipality and time_window\n",
    "    municipalities_by_time_window = {}\n",
    "    for query in queries:\n",
    "        time_window = query['Time Window']\n",
    "        if time_window in municipalities_by_time_window:\n",
    "            municipalities_by_time_window[time_window].update(query['Municipalities'])\n",
    "        else:\n",
    "            municipalities_by_time_window[time_window] = set(query['Municipalities'])\n",
    "            \n",
    "    relevant_filenames = set()\n",
    "    for time_window in municipalities_by_time_window:\n",
    "        starting_date = starting_dates_for_filtering[time_window]\n",
    "        potential_documents = metadata\n",
    "        if starting_date:\n",
    "            potential_documents = potential_documents[potential_documents[\"date\"] >= starting_date]\n",
    "        cities = municipalities_by_time_window[time_window]\n",
    "        potential_documents = potential_documents[[(c in cities) for c in potential_documents[\"city\"]]]\n",
    "        relevant_filenames.update(potential_documents['local_path_txt'])\n",
    "    return relevant_filenames\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def read_docs(s3_paths):\n",
    "    log_every = 100\n",
    "\n",
    "    documents = {}\n",
    "    n_docs_total = len(s3_paths)\n",
    "\n",
    "    i = 0\n",
    "    n_docs_read = 0\n",
    "    for s3_path in s3_paths:\n",
    "        try:\n",
    "            doc_string = read_doc(s3_path)\n",
    "            doc_sentences = sentence_split(doc_string)\n",
    "            doc_tokens = []\n",
    "            for sentence in doc_sentences:\n",
    "                sentence_tokens = tokenize(sentence)\n",
    "                doc_tokens.append(sentence_tokens)\n",
    "            filename_pkl = s3_path[:-3] + \"pkl\"\n",
    "            try:\n",
    "                vectors = read(filename_pkl)\n",
    "                documents[s3_path] = {\n",
    "                    \"original_text\": doc_string,\n",
    "                    \"sentences\": doc_sentences,\n",
    "#                     \"tokens\": doc_tokens,\n",
    "                    \"vectors\": vectors\n",
    "                }\n",
    "            except:\n",
    "                print('missing vectors for: {}'.format(s3_path))\n",
    "        except Exception as e:\n",
    "            if i < 10:\n",
    "                print(\"Key not found: {}\".format(s3_path))\n",
    "            elif i == 10:\n",
    "                print(\"More than 10 keys not found\")\n",
    "                print(e)\n",
    "                break\n",
    "            i+=1\n",
    "        if n_docs_read % log_every == 0:\n",
    "            print(\"{} of {} documents read\".format(n_docs_read, n_docs_total))\n",
    "        n_docs_read+=1\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def select_relevant_docs(municipalities, time_window, all_docs, metadata):\n",
    "    # filter metadata to only those files that match the query municipality and time_window\n",
    "    starting_date = starting_dates_for_filtering[time_window]\n",
    "    potential_documents = metadata\n",
    "    if starting_date:\n",
    "        potential_documents = potential_documents[potential_documents[\"date\"] >= starting_date]\n",
    "    potential_documents = potential_documents[[(c in municipalities) for c in potential_documents[\"city\"]]]\n",
    "    # filter all docs to only filenames in subset of metadata\n",
    "    filenames = list(potential_documents['local_path_txt'])\n",
    "    urls = list(potential_documents['url'])\n",
    "    docs_to_return = []\n",
    "    for i in range(len(filenames)):\n",
    "        f = filenames[i]\n",
    "        u = urls[i]\n",
    "        if f in all_docs:\n",
    "            docs_to_return.append({**all_docs[f], 'filename':f, 'url':u})\n",
    "    # return [{**all_docs[f], 'filename':f, 'url':\"example.com\"} for f in potential_documents['local_path_txt'] if f in all_docs]\n",
    "    return docs_to_return\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# TODO: [PRIORITY] include section numbers, extract overlapping sections, enforce no overlaps in returned content\n",
    "# TODO: Play with section length\n",
    "# TODO: smart sectioning that's sensitive to multiple line breaks and other section break signals\n",
    "def segment_docs(relevant_docs):\n",
    "    min_section_length = 50 # tokens\n",
    "    # TODO: this cuts of end of doc\n",
    "    \n",
    "    sections = []\n",
    "    for doc in relevant_docs:\n",
    "        original_text = doc[\"original_text\"]\n",
    "        pages = original_text.split('\\f')\n",
    "        page_numbers = []\n",
    "        for p, page in enumerate(pages):\n",
    "            page_sentences = sentence_split(page)\n",
    "            # for each sentence, what page was it on?\n",
    "            for sentence in page_sentences:\n",
    "                sentence_tokens = tokenize(sentence)\n",
    "                page_numbers.append(p+1)\n",
    "        doc_sentences = doc[\"sentences\"]\n",
    "        doc_sentences_with_extra = doc[\"vectors\"][\"sentences\"]\n",
    "        doc_vectors_with_extra = doc[\"vectors\"][\"vectors\"]\n",
    "        nonempty_sentence_indices = [i for i in range(len(doc_sentences_with_extra)) if len(doc_sentences_with_extra[i].strip())>0]\n",
    "        doc_vectors = [doc_vectors_with_extra[i] for i in nonempty_sentence_indices]\n",
    "        section = []\n",
    "        section_tokens = 0\n",
    "        if (len(doc_sentences) == len(doc_vectors)):\n",
    "            for i in range(len(doc_sentences)):\n",
    "                sentence = doc_sentences[i]\n",
    "                page = page_numbers[i]\n",
    "                sentence_vectors = doc_vectors[i]\n",
    "                sentence_tokens = tokenize(sentence)\n",
    "                section.append({\n",
    "                    \"sentence\": sentence,\n",
    "                    \"page\": page,\n",
    "                    \"sentence_vectors\": sentence_vectors,\n",
    "                    \"sentence_tokens\": sentence_tokens\n",
    "                })\n",
    "                section_tokens += len(sentence_tokens)\n",
    "                if section_tokens >= min_section_length:\n",
    "                    section_text = \". \".join([s[\"sentence\"].strip() for s in section])\n",
    "                    sections.append({\n",
    "                        \"sentences\": section,\n",
    "                        \"section_text\": section_text,\n",
    "                        \"filename\": doc[\"filename\"],\n",
    "                        \"url\": doc[\"url\"]\n",
    "                    })\n",
    "                    section = []\n",
    "                    section_tokens = 0\n",
    "    return sections\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#             page_tokens = simple_tokenizer.tokenize(page)\n",
    "#             for t in page_tokens:\n",
    "#                 page_numbers.append(p+1)\n",
    "#         doc_tokens = doc[\"tokens\"]\n",
    "#         vectors = select_layer(doc[\"vectors\"])\n",
    "# #         print(vectors.shape)\n",
    "#         filename = doc[\"filename\"]\n",
    "#         n_tokens = len(doc_tokens)\n",
    "# #         print(n_tokens)\n",
    "#         tokens_per_section = 100\n",
    "#         n_sections = n_tokens // tokens_per_section\n",
    "# #         n_sections = int(np.floor(tokens_per_section / n_tokens))\n",
    "# #         print(n_sections)\n",
    "#         for s in range(n_sections):\n",
    "#             start_index = s*tokens_per_section\n",
    "#             end_index = ((s+1)*tokens_per_section)\n",
    "#             section_tokens = doc_tokens[start_index:end_index]\n",
    "#             section_vectors = vectors[start_index:end_index,]\n",
    "#             section_start_page = page_numbers[start_index]\n",
    "#             section_end_page = page_numbers[min(len(page_numbers), end_index)-1]\n",
    "#             doc_sections.append({\n",
    "#                 'filename': doc['filename'],\n",
    "#                 'url': doc['url'],\n",
    "#                 'start_page': section_start_page,\n",
    "#                 'end_page': section_end_page,\n",
    "#                 'text': \" \".join(section_tokens),\n",
    "# #                 'tokens': section_tokens,\n",
    "#                 'vectors': section_vectors\n",
    "#             })\n",
    "            \n",
    "#     return doc_sections\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# doc_sections = segment_docs(relevant_docs)\n",
    "# print(\"sections: {}\".format(len(doc_sections)))\n",
    "# print(doc_sections[0]['section_text'])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "casing = \"lower_non_acronyms\"\n",
    "# casing = \"lower_non_acronyms\"\n",
    "# TODO: is lowercasing necessary?\n",
    "\n",
    "def casing_function():\n",
    "    if casing==\"cased\":\n",
    "        return lambda x: x\n",
    "    elif casing==\"lower\":\n",
    "        return lambda x: x.lower()\n",
    "    elif casing==\"lower_non_acronyms\":\n",
    "        return lambda x: x if x.isupper() else x.lower()\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# TODO: use vectors to find closes words to keywords\n",
    "# TODO: why do shorter documents get higher scores?\n",
    "def score_doc_sections(doc_sections, orig_keywords, elmo):\n",
    "    orig_keywords = [k.strip() for k in orig_keywords]\n",
    "    keywords = []\n",
    "    for k in orig_keywords:\n",
    "        words = k.split(\" \")\n",
    "        for word in words:\n",
    "            keywords.append(word)\n",
    "    keyword_vectors = single_vector_per_doc([elmo.embed_sentence(keywords)])\n",
    "#     keyword_weights = []\n",
    "#     fix_case = casing_function()\n",
    "# #     idf_smoothing_count = 10\n",
    "#     for k in keywords:\n",
    "# #         words = k.split(\" \")\n",
    "# #         if len(words) > 1:\n",
    "# #             keyword_weights.append(1./idf_smoothing_count)\n",
    "# #         else:\n",
    "# #             k = fix_case(k)\n",
    "# #             if k in idf:\n",
    "# #                 keyword_weights.append(1./(1./idf[k]+idf_smoothing_count))\n",
    "# #             else:\n",
    "# #                 keyword_weights.append(1./idf_smoothing_count)\n",
    "    doc_sections_scores = []\n",
    "    for s, section in enumerate(doc_sections):\n",
    "        section_vectors = single_vector_per_doc([s[\"sentence_vectors\"] for s in section[\"sentences\"]])\n",
    "        section_text = section['section_text']\n",
    "        no_keywords_found = True\n",
    "        for k in orig_keywords:\n",
    "            if bool(re.search(\"([^\\w]|^)\" + k + \"([^\\w]|$)\", section_text)):\n",
    "#             if k in section_text:\n",
    "                # TODO: consider casing\n",
    "                no_keywords_found = False\n",
    "        for k in orig_keywords:\n",
    "            if k.islower():\n",
    "                if bool(re.search(\"([^\\w]|^)\" + k + \"([^\\w]|$)\", section_text.lower())):\n",
    "                    no_keywords_found = False\n",
    "        if no_keywords_found:\n",
    "            score = 0\n",
    "        elif section_vectors.shape[0]>0:\n",
    "            similarities = cosine_similarity(section_vectors, keyword_vectors)\n",
    "#             if threshold_similarity > -1:\n",
    "#                 similarities = similarities*(similarities>threshold_similarity)\n",
    "            keyword_similarities = np.mean(similarities, axis=0)\n",
    "#             score = np.sum(keyword_similarities*keyword_weights)\n",
    "            score = np.mean(keyword_similarities)\n",
    "        else:\n",
    "            score = 0\n",
    "        doc_sections_scores.append(score)\n",
    "\n",
    "    return doc_sections_scores\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# doc_sections_scores = score_doc_sections(\n",
    "#     doc_sections,\n",
    "#     keywords\n",
    "# )\n",
    "# # doc_sections_scores\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def text_is_too_similar(a, b):\n",
    "    # if there are only 2 edits to get from one text to the other, it's not good\n",
    "    return editdistance.eval(a, b) < 50\n",
    "\n",
    "# make sure we're not giving similar text among the top k (e.g. shows up on both minutes and agenda)\n",
    "def check_repeated_text(top_k, section_text):\n",
    "    for old in top_k:\n",
    "        if text_is_too_similar(old[1][\"section_text\"], section_text):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def select_top_k(doc_sections, doc_sections_scores, k, user_history):\n",
    "    sorted_sections = sorted(zip(doc_sections_scores, doc_sections), key=lambda pair: pair[0], reverse=True)\n",
    "    top_k = []\n",
    "    text_returned = []\n",
    "    for x in sorted_sections:\n",
    "        score = x[0]\n",
    "        if score > 0:\n",
    "            filename = x[1][\"filename\"]\n",
    "            starting_page = x[1][\"sentences\"][0][\"page\"]\n",
    "            ending_page = x[1][\"sentences\"][-1][\"page\"]\n",
    "            section_text = x[1][\"section_text\"]\n",
    "            # debugging parameter -- DO send the same thing multiple times if we're debugging\n",
    "            if filename in [x[1]['filename'] for x in user_history]:\n",
    "                print(\"this user has already seen their top file ({})\".format(filename))\n",
    "            elif check_repeated_text(top_k, section_text):\n",
    "                print(\"this excpert has already been returned\")\n",
    "            else:\n",
    "                top_k.append(x)\n",
    "            if len(top_k) >= k:\n",
    "                break\n",
    "    return top_k\n",
    "\n",
    "def update_with_top_k(history, top_k_sections, query):\n",
    "    for section in top_k_sections:\n",
    "        x = section[1]\n",
    "        x.update(query)\n",
    "        history.append(x)\n",
    "    return history\n",
    "\n",
    "\n",
    "# 'original_text', 'tokens', 'filename', 'starting_page', 'starting_line', 'ending_page', 'ending_line', 'section_text', 'section_tokens', 'Municipalities', 'id', 'Keywords', 'Time Window'\n",
    "def reformat_results(results):\n",
    "    reformatted_results = {}\n",
    "    # one per query\n",
    "    for result in results:\n",
    "        username = result['id']\n",
    "        keywords = result['Keywords']\n",
    "        query_id = username + \",\".join(keywords) + \",\".join(result['Municipalities']) + \",\".join(result['Time Window'])\n",
    "        if query_id not in reformatted_results:\n",
    "            reformatted_results[query_id] = {\n",
    "                'user_id': username,\n",
    "                'document_sections': []\n",
    "            }\n",
    "        reformatted_results[query_id]['document_sections'].append({\n",
    "            # TODO\n",
    "            \"section_id\": \"000\",\n",
    "            # TODO\n",
    "            \"doc_url\": result['url'],\n",
    "            \"doc_name\": os.path.basename(result['filename']),\n",
    "            \"user_id\": username,\n",
    "            \"page_number\": result[\"sentences\"][0][\"page\"],\n",
    "            \"keywords\": keywords,\n",
    "            \"text\": result['section_text'].encode('ascii', errors='ignore').decode('ascii')\n",
    "        })\n",
    "    return [reformatted_results[r] for r in reformatted_results]\n",
    "\n",
    "# vectors = setup_word_vectors()\n",
    "def run_queries(elmo, k=3): \n",
    "    print(\"reading queries\")\n",
    "    queries = read_queries()\n",
    "    queries = [q for q in queries if ('Status' in q and q['Status'] == 'just_submitted')]\n",
    "    print(queries)\n",
    "    print(\"reading metadata\")\n",
    "    metadata = read_metadata()\n",
    "    # print(\"setting up reader\")\n",
    "    # doc_text_reader = DocTextReader(log_every=100)\n",
    "    print(\"finding relevant filenames\")\n",
    "    relevant_filenames = find_relevant_filenames(queries, metadata)\n",
    "    # (not actually *all*, but all the ones we care about for queries)\n",
    "    print(\"reading relevant documents\")\n",
    "    # all_docs = doc_text_reader.read_docs(relevant_filenames)\n",
    "    all_docs = read_docs(relevant_filenames)\n",
    "    print(\"reading history\")\n",
    "    # history = read_history()\n",
    "    history = []\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for q, query in enumerate(queries):\n",
    "        print(\"running query {} of {}\".format(q, len(queries)))\n",
    "        user_id = query[\"id\"]\n",
    "        print(\"user id: {}\".format(user_id))\n",
    "        user_history = [x for x in history if x['id'] == user_id]\n",
    "        keywords = query[\"Keywords\"]\n",
    "        print(keywords)\n",
    "        time_window = query[\"Time Window\"]\n",
    "        municipalities = query[\"Municipalities\"]\n",
    "        relevant_docs = select_relevant_docs(municipalities, time_window, all_docs, metadata)\n",
    "        print(\"segmenting documents\")\n",
    "        doc_sections = segment_docs(relevant_docs)\n",
    "        print(\"scoring documents\")\n",
    "        doc_sections_scores = score_doc_sections(\n",
    "            doc_sections,\n",
    "            keywords,\n",
    "            elmo\n",
    "        )\n",
    "        top_k_sections = select_top_k(doc_sections, doc_sections_scores, k, user_history)\n",
    "        results = update_with_top_k(results, top_k_sections, query)\n",
    "        history = update_with_top_k(history, top_k_sections, query)\n",
    "        \n",
    "    print(\"sending emails\")\n",
    "    r = reformat_results(results)\n",
    "#     print(r)\n",
    "    send_emails(r)\n",
    "    # write_history(history)\n",
    "    print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading queries\n",
      "reading metadata\n",
      "finding relevant filenames\n",
      "reading relevant documents\n",
      "0 of 37 documents read\n",
      "Key not found: docs/santa-clara/Santa-Clara_2019-12-05_Historical-&-Landmarks-Commission_Agenda.txt\n",
      "Key not found: docs/santa-clara/Santa-Clara_2019-12-02_Economic-Development,-Communications,-And-Marketing-Committee_Agenda.txt\n",
      "reading history\n",
      "running query 0 of 2\n",
      "user id: emily\n",
      "['housing', ' affordable housing', ' homelessness', ' accessory dwelling unit', ' ADU']\n",
      "segmenting documents\n",
      "scoring documents\n",
      "running query 1 of 2\n",
      "user id: hs4man21@stanford.edu\n",
      "['housing', ' affordable', ' accessible dwelling units', ' ADU']\n",
      "segmenting documents\n",
      "scoring documents\n",
      "sending emails\n",
      "[{'user_id': 'emily', 'document_sections': [{'section_id': '000', 'doc_url': 'https://sfgov.legistar.com/View.ashx?M=A&ID=740391&GUID=7EF81218-6123-49A3-8668-501B39BA478C', 'doc_name': 'San-Francisco_2019-11-18_Budget-And-Finance-Committee_Agenda.txt', 'user_id': 'emily', 'page_number': 2, 'keywords': ['housing', ' affordable housing', ' homelessness', ' accessory dwelling unit', ' ADU'], 'text': 'subseries, for the purpose of providing financing for the acquisition and rehabilitation of. a 202-unit multifamily rental housing project known as Eastern Park Apartments;. approving the form of and authorizing the execution of an indenture of trust providing the. terms and conditions of and the authorization for the issuance of such bonds; approving'}, {'section_id': '000', 'doc_url': 'https://cupertino.legistar.com/View.ashx?M=A&ID=656673&GUID=B002B911-8F92-4378-AE28-1CFABB5F0AE5', 'doc_name': 'Cupertino_2019-11-12_Planning-Commission_Agenda.txt', 'user_id': 'emily', 'page_number': 1, 'keywords': ['housing', ' affordable housing', ' homelessness', ' accessory dwelling unit', ' ADU'], 'text': '2. Subject:  Study  Session  regarding  amendments  being  proposed  to  City  standards  for. parkland dedication and fees (Chapter 13. 08 - Park Land Dedication Fee, Chapter 14. 05. -  Park  Maintenance  Fee,  and  Chapter  18. 24 -  Dedications  and  Reservations)  and  the. 2014 Certified  General  Plan  Amendment,  Housing  Element  Update,  and  Associated. Rezoning Draft EIR'}, {'section_id': '000', 'doc_url': 'https://sfgov.legistar.com/View.ashx?M=A&ID=740391&GUID=7EF81218-6123-49A3-8668-501B39BA478C', 'doc_name': 'San-Francisco_2019-11-18_Budget-And-Finance-Committee_Agenda.txt', 'user_id': 'emily', 'page_number': 4, 'keywords': ['housing', ' affordable housing', ' homelessness', ' accessory dwelling unit', ' ADU'], 'text': 'reimburse certain expenditures from proceeds of future bonded indebtedness in an. aggregate principal amount not to exceed $20,655,000; authorizing the Director of the. Mayors Office of Housing and Community Development (Director) to submit an. application and related documents to the California Debt Limit Allocation Committee. (CDLAC) to permit the issuance of residential mortgage revenue bonds in an'}, {'section_id': '000', 'doc_url': 'https://sfgov.legistar.com/View.ashx?M=A&ID=740391&GUID=7EF81218-6123-49A3-8668-501B39BA478C', 'doc_name': 'San-Francisco_2019-11-18_Budget-And-Finance-Committee_Agenda.txt', 'user_id': 'emily', 'page_number': 2, 'keywords': ['housing', ' affordable housing', ' homelessness', ' accessory dwelling unit', ' ADU'], 'text': 'Mayors Office of Housing and Community Development (Director) to submit an. application and related documents to the California Debt Limit Allocation Committee. (CDLAC) to permit the issuance of residential mortgage revenue bonds in an. aggregate principal amount not to exceed $75,000,000 for 4840 Mission Street;. authorizing and directing the Director to direct the Controllers Office to hold in trust an'}, {'section_id': '000', 'doc_url': 'https://hayward.legistar.com/View.ashx?M=A&ID=738519&GUID=51BA4277-8583-4599-9B6F-33D38A17E64E', 'doc_name': 'Hayward_2019-11-19_City-Council_Agenda.txt', 'user_id': 'emily', 'page_number': 2, 'keywords': ['housing', ' affordable housing', ' homelessness', ' accessory dwelling unit', ' ADU'], 'text': 'Alcohol to Minors. Attachment I Staff Report. Attachment II Resolution. Adoption of FY 2020 Statement of Investment Policy and. Delegation of Authority. Attachment I Staff Report. Attachment II City Resolution. Attachment III Housing Authority Resolution. Attachment IV Redevelopment Successor Agency Resolution. Attachment V Public Financing Authority Resolution. Attachment VI FY 2020 Investment Policy'}]}]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-00ebe231548c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreformat_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# print(r)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0msend_emails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;31m# write_history(history)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finished\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/autolocal/autolocal/emailer.py\u001b[0m in \u001b[0;36msend_emails\u001b[0;34m(results)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msend_emails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0memailsToSend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_emails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#contains elements of format: [email_address, [title, url, blurb]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memailsToSend\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0memail_user\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'autolocalnews@gmail.com'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/autolocal/autolocal/emailer.py\u001b[0m in \u001b[0;36mextract_emails\u001b[0;34m(results)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# buf = response.read()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# result = json.loads(buf.decode('utf-8'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0memail_address\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'promo@erindb.com'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'patwei@stanford.edu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chstock@stanford.edu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hs4man21@stanford.edu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0memail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "from autolocal.emailer import send_emails\n",
    "\n",
    "# vectors = setup_word_vectors()\n",
    "k = 5\n",
    "print(\"reading queries\")\n",
    "queries = read_queries()\n",
    "queries = [q for q in queries if ('Status' in q and q['Status'] == 'just_submitted')]\n",
    "# print(queries)\n",
    "print(\"reading metadata\")\n",
    "metadata = read_metadata()\n",
    "# print(\"setting up reader\")\n",
    "# doc_text_reader = DocTextReader(log_every=100)\n",
    "print(\"finding relevant filenames\")\n",
    "relevant_filenames = find_relevant_filenames(queries, metadata)\n",
    "# (not actually *all*, but all the ones we care about for queries)\n",
    "print(\"reading relevant documents\")\n",
    "# all_docs = doc_text_reader.read_docs(relevant_filenames)\n",
    "all_docs = read_docs(relevant_filenames)\n",
    "print(\"reading history\")\n",
    "# history = read_history()\n",
    "history = []\n",
    "\n",
    "results = []\n",
    "\n",
    "for q, query in enumerate(queries):\n",
    "    print(\"running query {} of {}\".format(q, len(queries)))\n",
    "    user_id = query[\"id\"]\n",
    "    print(\"user id: {}\".format(user_id))\n",
    "    user_history = [x for x in history if x['id'] == user_id]\n",
    "    keywords = query[\"Keywords\"]\n",
    "    print(keywords)\n",
    "    time_window = query[\"Time Window\"]\n",
    "    municipalities = query[\"Municipalities\"]\n",
    "    relevant_docs = select_relevant_docs(municipalities, time_window, all_docs, metadata)\n",
    "    print(\"segmenting documents\")\n",
    "    doc_sections = segment_docs(relevant_docs)\n",
    "    print(\"scoring documents\")\n",
    "    doc_sections_scores = score_doc_sections(\n",
    "        doc_sections,\n",
    "        keywords,\n",
    "        elmo\n",
    "    )\n",
    "    top_k_sections = select_top_k(doc_sections, doc_sections_scores, k, user_history)\n",
    "    results = update_with_top_k(results, top_k_sections, query)\n",
    "    history = update_with_top_k(history, top_k_sections, query)\n",
    "\n",
    "print(\"sending emails\")\n",
    "r = reformat_results(results)\n",
    "# print(r)\n",
    "send_emails(r)\n",
    "# write_history(history)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
