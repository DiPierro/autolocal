{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "city_name = 'santa_clara'\n",
    "url = 'https://santaclara.legistar.com/Calendar.aspx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib import parse\n",
    "from urllib import request\n",
    "import bs4 as bs\n",
    "\n",
    "import selenium as se\n",
    "from selenium.webdriver import Firefox\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from tqdm import tqdm\n",
    "\n",
    "# helper functions\n",
    "def get_page_links(driver):\n",
    "    pagelinks_xpath = \"//td[@class='rgPagerCell NumericPages']/div[1]/a\"\n",
    "    pagelinks = driver.find_elements_by_xpath(pagelinks_xpath)\n",
    "    pagelinks = pagelinks[:int(len(pagelinks)/2)]\n",
    "    return [l.text for l in pagelinks], pagelinks\n",
    "\n",
    "def extract_table(page_source, table_id):\n",
    "    # find table in page\n",
    "    soup = bs.BeautifulSoup(page_source)\n",
    "    table = soup.select(table_id)[0]\n",
    "    num_cols = int(table.td.get('colspan'))\n",
    "\n",
    "    # extract column headers\n",
    "    header_data = [''.join(cell.stripped_strings) for cell in table.find_all('th')]\n",
    "    header_data = [h for h in header_data if h!='Data pager']\n",
    "    assert(len(header_data)==num_cols)\n",
    "\n",
    "    # extract text and URL data from table\n",
    "    text_data, url_data = [], []\n",
    "    for row in table.find_all('tr'):\n",
    "        row_text, row_url = [], []\n",
    "        for td in row.find_all('td'):\n",
    "            row_text.append(''.join(td.stripped_strings))\n",
    "            if td.find('a'):\n",
    "                row_url.append(td.a.get('href'))\n",
    "            else:\n",
    "                row_url.append(np.nan)\n",
    "            if len(row_text)==num_cols and len(row_url)==num_cols:\n",
    "                text_data.append(row_text)\n",
    "                url_data.append(row_url)\n",
    "                \n",
    "    # turn into dataframe\n",
    "    num_cols = table.td.get('colspan')\n",
    "    text_df = pd.DataFrame(text_data, columns=header_data)\n",
    "    url_df = pd.DataFrame(url_data, columns=header_data)\n",
    "    df = pd.merge(text_df, url_df, left_index=True, right_index=True, suffixes=(' Text', ' URL'))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch driver\n",
    "driver = Firefox()\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL: select date range dropdown  to 'All Years'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrape page 1?\n",
      "Scrape page 2?\n",
      "Scrape page 3?\n",
      "Scrape page 4?\n",
      "Scrape page 5?\n"
     ]
    }
   ],
   "source": [
    "# click through pages and save html\n",
    "c = 1\n",
    "page_data = []\n",
    "while True:\n",
    "    pages, pagelinks = get_page_links(driver)\n",
    "    try:\n",
    "        # click on the integer we want\n",
    "        i = pages.index(str(c))\n",
    "        link = pagelinks[i]\n",
    "    except:\n",
    "        # if it's not there and the list ends with '...', click on '...'\n",
    "        if pages[-1]=='...':\n",
    "            link = pagelinks[-1]\n",
    "        # if it's not there and the list starts with '...', we are done.\n",
    "        else:\n",
    "            break\n",
    "    link.click()\n",
    "    input('Scrape page {}?'.format(c))\n",
    "    page_data.append(driver.page_source)\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract table data\n",
    "table_id = '#ctl00_ContentPlaceHolder1_gridCalendar_ctl00'\n",
    "page_dfs = [extract_table(page, table_id) for page in tqdm(page_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat(page_dfs)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name Text', 'Meeting Date Text', ' Text', 'Meeting Time Text',\n",
       "       'Meeting Location Text', 'Meeting Details Text', 'Agenda Text',\n",
       "       'Minutes Text', 'Agenda Packet Text', 'Video or Audio Text',\n",
       "       'Caption Notes Text', 'eComment Text', 'Name URL', 'Meeting Date URL',\n",
       "       ' URL', 'Meeting Time URL', 'Meeting Location URL',\n",
       "       'Meeting Details URL', 'Agenda URL', 'Minutes URL', 'Agenda Packet URL',\n",
       "       'Video or Audio URL', 'Caption Notes URL', 'eComment URL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../data/scraping/scraped_tables/{}.csv'.format(city_name)\n",
    "data.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate results\n",
    "# submit_button_id = 'ctl00_ContentPlaceHolder1_btnSearch'\n",
    "# button = driver.find_element_by_id(submit_button_id)\n",
    "# button.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
