{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38 µs, sys: 1 µs, total: 39 µs\n",
      "Wall time: 43.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from io import BytesIO\n",
    "from autolocal.parsers.nlp import Tokenizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "from  tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "autolocal_docs_bucket = s3.Bucket('autolocal-documents')\n",
    "def read_doc(s3_path):\n",
    "    try:\n",
    "        return autolocal_docs_bucket.Object(s3_path).get()['Body'].read().decode(\"ascii\", \"ignore\")\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata():\n",
    "    table = boto3.resource('dynamodb', region_name='us-west-1').Table('autolocal-documents')\n",
    "    s3_client = boto3.client('s3')\n",
    "    metadata = pd.DataFrame(table.scan()[\"Items\"])\n",
    "    metadata[\"date\"] = [datetime.strptime(d, '%Y-%m-%d') for d in metadata[\"date\"]]\n",
    "    metadata['local_path_npy'] = metadata['local_path_txt'].apply(lambda x: x[:-3]+\"npy\")\n",
    "    return metadata\n",
    "metadata = read_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npy(s3_path):\n",
    "    obj = s3.get_object(Bucket='autolocal-documents', Key=s3_path)\n",
    "    array = np.load(BytesIO(obj['Body'].read()))\n",
    "    return array\n",
    "    \n",
    "def write_npy(array, s3_path):\n",
    "    np.save('tmp.npy', array)\n",
    "    autolocal_docs_bucket.put_object('tmp.npy', Key=s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_dates_for_filtering = {\n",
    "    'upcoming_only': datetime.now() + timedelta(days=0.5),\n",
    "    'upcoming': datetime.now() + timedelta(days=0.5),\n",
    "    'this_week': datetime.now() - timedelta(weeks=1),\n",
    "    'this_year': datetime.now() - timedelta(days=365),\n",
    "    'this_month': datetime.now() - timedelta(weeks=5),\n",
    "    'past_six_months':datetime.now() - timedelta(days=183),\n",
    "    'past': None,\n",
    "    'all': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_this_month = metadata[metadata[\"date\"] >= starting_dates_for_filtering['this_month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 10)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:00, 85.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792\n",
      "921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "19it [01:35,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [01:55, 12.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "21it [02:28, 18.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "22it [07:19, 100.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [07:50, 74.95s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "25it [08:07, 57.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "26it [08:19, 43.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "27it [09:40, 55.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "28it [10:04, 45.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "29it [10:14, 35.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "30it [11:00, 38.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "31it [11:55, 43.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "32it [12:17, 36.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "33it [12:25, 28.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "34it [12:26, 20.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "35it [12:40, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "36it [12:42, 13.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "37it [13:17, 19.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [13:40, 21.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i, row in tqdm(metadata_this_month.iterrows()):\n",
    "    if count >= 15:\n",
    "        txt_filename = row['local_path_txt']\n",
    "        npy_filename = row['local_path_npy']\n",
    "        doc_string = read_doc(txt_filename)\n",
    "        if doc_string:\n",
    "            doc_tokens = simple_tokenizer.tokenize(doc_string)\n",
    "            doc_vectors = elmo.embed_sentence(doc_tokens)\n",
    "            write_npy(doc_vectors, npy_filename)\n",
    "            print(i)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 336/336 [00:00<00:00, 770522.77B/s]\n",
      "100%|██████████| 374434792/374434792 [00:33<00:00, 11035846.17B/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = [\"I\", \"ate\", \"an\", \"apple\", \"for\", \"breakfast\"]\n",
    "vectors = elmo.embed_sentence(tokens)\n",
    "# dims: (LAYERS(3), TOKENS(6), DIMENSIONS(1024))\n",
    "\"\"\"\n",
    "https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f\n",
    "In the ELMo paper, there are 3 layers of word embedding,\n",
    "layer zero is the character-based context independent layer,\n",
    "followed by two Bi-LSTM layers. The authors have empirically\n",
    "shown that the word vectors generated from the first Bi-LSTM\n",
    "layer can better capture the syntax, and the second layer can\n",
    "capture the semantics better.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.2 s, sys: 1.03 s, total: 58.3 s\n",
      "Wall time: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_filename = metadata['local_path_txt'][0]\n",
    "npy_filename = metadata['local_path_npy'][0]\n",
    "doc_tokens = simple_tokenizer.tokenize(read_doc(text_filename))\n",
    "doc_vectors = elmo.embed_sentence(doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_npy(doc_vectors, npy_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'docs/cupertino/Cupertino_2019-07-30_Planning-Commission_Agenda.txt'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocTextReader():\n",
    "    def __init__(self, log_every=100):\n",
    "        self.log_every = log_every\n",
    "        s3 = boto3.resource('s3', region_name='us-west-1')\n",
    "        self.bucket = s3.Bucket('autolocal-documents')\n",
    "\n",
    "    def read_document_string(self, s3_path):\n",
    "        return self.\n",
    "\n",
    "    def read_docs(self, s3_paths):\n",
    "        # read all documents that we know about\n",
    "        # tokenize each document\n",
    "        # return list of documents\n",
    "\n",
    "        documents = {}\n",
    "        n_docs_total = len(s3_paths)\n",
    "\n",
    "        i = 0\n",
    "        n_docs_read = 0\n",
    "        for s3_path in s3_paths:\n",
    "            try:\n",
    "                doc_string = self.read_document_string(s3_path)\n",
    "                doc_tokens = preprocess_string(doc_string, filters=preprocess_filters)\n",
    "                documents[s3_path] = {\n",
    "                    \"original_text\": doc_string,\n",
    "                    \"tokens\": doc_tokens\n",
    "                }\n",
    "            except Exception as e:\n",
    "                if i < 10:\n",
    "                    print(\"Key not found: {}\".format(s3_path))\n",
    "                elif i == 10:\n",
    "                    print(\"More than 10 keys not found\")\n",
    "                    print(e)\n",
    "                    break\n",
    "                i+=1\n",
    "            if n_docs_read % self.log_every == 0:\n",
    "                print(\"{} of {} documents read\".format(n_docs_read, n_docs_total))\n",
    "            n_docs_read+=1\n",
    "\n",
    "        return documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
