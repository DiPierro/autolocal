{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.7 s, sys: 1 s, total: 3.7 s\n",
      "Wall time: 4.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from io import BytesIO\n",
    "from autolocal.parsers.nlp import Tokenizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "from  tqdm import tqdm\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "from autolocal.emailer import send_emails\n",
    "import editdistance\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 s, sys: 653 ms, total: 11 s\n",
      "Wall time: 9.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_vector_per_doc(vectors):\n",
    "    # vectors is a list of np arrays where:\n",
    "    # dims: (LAYERS(3), TOKENS(varies), DIMENSIONS(1024))\n",
    "    \"\"\"\n",
    "    https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f\n",
    "    In the ELMo paper, there are 3 layers of word embedding,\n",
    "    layer zero is the character-based context independent layer,\n",
    "    followed by two Bi-LSTM layers. The authors have empirically\n",
    "    shown that the word vectors generated from the first Bi-LSTM\n",
    "    layer can better capture the syntax, and the second layer can\n",
    "    capture the semantics better.\n",
    "    \"\"\"\n",
    "    vectors = np.concatenate([v[2] for v in vectors], 0)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "autolocal_docs_bucket = s3.Bucket('autolocal-documents')\n",
    "def read_doc(s3_path):\n",
    "    try:\n",
    "        return autolocal_docs_bucket.Object(s3_path).get()['Body'].read().decode(\"ascii\", \"ignore\")\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata():\n",
    "    table = boto3.resource('dynamodb', region_name='us-west-1').Table('autolocal-documents')\n",
    "    s3_client = boto3.client('s3')\n",
    "    metadata = pd.DataFrame(table.scan()[\"Items\"])\n",
    "    metadata[\"date\"] = [datetime.strptime(d, '%Y-%m-%d') for d in metadata[\"date\"]]\n",
    "    metadata['local_path_npy'] = metadata['local_path_txt'].apply(lambda x: x[:-3]+\"npy\")\n",
    "    return metadata\n",
    "metadata = read_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(s3_path):\n",
    "    return pickle.load(open(os.path.join(\"pkls/\", os.path.basename(s3_path)), 'rb'))\n",
    "\n",
    "def write(array, s3_path):\n",
    "    pickle.dump(array, open(os.path.join(\"pkls/\", os.path.basename(s3_path)), 'wb'))\n",
    "\n",
    "def sentence_split(s):\n",
    "    sentences = re.split('[.\\n!?\"\\f]', s)\n",
    "    return [s for s in sentences if len(s.strip())>0]\n",
    "\n",
    "def tokenize(s):\n",
    "    tokens = re.findall(r'\\w+', s)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_dates_for_filtering = {\n",
    "    'upcoming_only': datetime.now() + timedelta(days=0.5),\n",
    "    'upcoming': datetime.now() + timedelta(days=0.5),\n",
    "    'this_week': datetime.now() - timedelta(weeks=1),\n",
    "    'this_year': datetime.now() - timedelta(days=365),\n",
    "    'this_month': datetime.now() - timedelta(weeks=5),\n",
    "    'past_six_months':datetime.now() - timedelta(days=183),\n",
    "    'last_six_months':datetime.now() - timedelta(days=183),\n",
    "    'past': None,\n",
    "    'all': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries():\n",
    "    table = boto3.resource('dynamodb', region_name='us-west-2').Table('autoLocalNews')\n",
    "    queries = table.scan()[\"Items\"]\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_filenames(queries, metadata): \n",
    "    \n",
    "    # filter metadata to only those files that match the query municipality and time_window\n",
    "    municipalities_by_time_window = {}\n",
    "    for query in queries:\n",
    "        time_window = query['Time Window']\n",
    "        if time_window in municipalities_by_time_window:\n",
    "            municipalities_by_time_window[time_window].update(query['Municipalities'])\n",
    "        else:\n",
    "            municipalities_by_time_window[time_window] = set(query['Municipalities'])\n",
    "            \n",
    "    relevant_filenames = set()\n",
    "    for time_window in municipalities_by_time_window:\n",
    "        starting_date = starting_dates_for_filtering[time_window]\n",
    "        potential_documents = metadata\n",
    "        if starting_date:\n",
    "            potential_documents = potential_documents[potential_documents[\"date\"] >= starting_date]\n",
    "        cities = municipalities_by_time_window[time_window]\n",
    "        potential_documents = potential_documents[[(c in cities) for c in potential_documents[\"city\"]]]\n",
    "        relevant_filenames.update(potential_documents['local_path_txt'])\n",
    "    return relevant_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(s3_paths):\n",
    "    log_every = 100\n",
    "\n",
    "    documents = {}\n",
    "    n_docs_total = len(s3_paths)\n",
    "\n",
    "    i = 0\n",
    "    n_docs_read = 0\n",
    "    for s3_path in s3_paths:\n",
    "        try:\n",
    "            doc_string = read_doc(s3_path)\n",
    "            doc_sentences = sentence_split(doc_string)\n",
    "            doc_tokens = []\n",
    "            for sentence in doc_sentences:\n",
    "                sentence_tokens = tokenize(sentence)\n",
    "                doc_tokens.append(sentence_tokens)\n",
    "            filename_pkl = s3_path[:-3] + \"pkl\"\n",
    "            try:\n",
    "                vectors = read(filename_pkl)\n",
    "                documents[s3_path] = {\n",
    "                    \"original_text\": doc_string,\n",
    "                    \"sentences\": doc_sentences,\n",
    "#                     \"tokens\": doc_tokens,\n",
    "                    \"vectors\": vectors\n",
    "                }\n",
    "            except:\n",
    "                print('missing vectors for: {}'.format(s3_path))\n",
    "        except Exception as e:\n",
    "            if i < 10:\n",
    "                print(\"Key not found: {}\".format(s3_path))\n",
    "            elif i == 10:\n",
    "                print(\"More than 10 keys not found\")\n",
    "                print(e)\n",
    "                break\n",
    "            i+=1\n",
    "        if n_docs_read % log_every == 0:\n",
    "            print(\"{} of {} documents read\".format(n_docs_read, n_docs_total))\n",
    "        n_docs_read+=1\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_docs(municipalities, time_window, all_docs, metadata):\n",
    "    # filter metadata to only those files that match the query municipality and time_window\n",
    "    starting_date = starting_dates_for_filtering[time_window]\n",
    "    potential_documents = metadata\n",
    "    if starting_date:\n",
    "        potential_documents = potential_documents[potential_documents[\"date\"] >= starting_date]\n",
    "    potential_documents = potential_documents[[(c in municipalities) for c in potential_documents[\"city\"]]]\n",
    "    # filter all docs to only filenames in subset of metadata\n",
    "    filenames = list(potential_documents['local_path_txt'])\n",
    "    urls = list(potential_documents['url'])\n",
    "    docs_to_return = []\n",
    "    for i in range(len(filenames)):\n",
    "        f = filenames[i]\n",
    "        u = urls[i]\n",
    "        if f in all_docs:\n",
    "            docs_to_return.append({**all_docs[f], 'filename':f, 'url':u})\n",
    "    # return [{**all_docs[f], 'filename':f, 'url':\"example.com\"} for f in potential_documents['local_path_txt'] if f in all_docs]\n",
    "    return docs_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: [PRIORITY] include section numbers, extract overlapping sections, enforce no overlaps in returned content\n",
    "# TODO: Play with section length\n",
    "# TODO: smart sectioning that's sensitive to multiple line breaks and other section break signals\n",
    "def segment_docs(relevant_docs):\n",
    "    min_section_length = 50 # tokens\n",
    "    # TODO: this cuts of end of doc\n",
    "    \n",
    "    sections = []\n",
    "    for doc in relevant_docs:\n",
    "        original_text = doc[\"original_text\"]\n",
    "        pages = original_text.split('\\f')\n",
    "        page_numbers = []\n",
    "        for p, page in enumerate(pages):\n",
    "            page_sentences = sentence_split(page)\n",
    "            # for each sentence, what page was it on?\n",
    "            for sentence in page_sentences:\n",
    "                sentence_tokens = tokenize(sentence)\n",
    "                page_numbers.append(p+1)\n",
    "        doc_sentences = doc[\"sentences\"]\n",
    "        doc_sentences_with_extra = doc[\"vectors\"][\"sentences\"]\n",
    "        doc_vectors_with_extra = doc[\"vectors\"][\"vectors\"]\n",
    "        nonempty_sentence_indices = [i for i in range(len(doc_sentences_with_extra)) if len(doc_sentences_with_extra[i].strip())>0]\n",
    "        doc_vectors = [doc_vectors_with_extra[i] for i in nonempty_sentence_indices]\n",
    "        section = []\n",
    "        section_tokens = 0\n",
    "        if (len(doc_sentences) == len(doc_vectors)):\n",
    "            for i in range(len(doc_sentences)):\n",
    "                sentence = doc_sentences[i]\n",
    "                page = page_numbers[i]\n",
    "                sentence_vectors = doc_vectors[i]\n",
    "                sentence_tokens = tokenize(sentence)\n",
    "                section.append({\n",
    "                    \"sentence\": sentence,\n",
    "                    \"page\": page,\n",
    "                    \"sentence_vectors\": sentence_vectors,\n",
    "                    \"sentence_tokens\": sentence_tokens\n",
    "                })\n",
    "                section_tokens += len(sentence_tokens)\n",
    "                if section_tokens >= min_section_length:\n",
    "                    section_text = \". \".join([s[\"sentence\"].strip() for s in section])\n",
    "                    sections.append({\n",
    "                        \"sentences\": section,\n",
    "                        \"section_text\": section_text,\n",
    "                        \"filename\": doc[\"filename\"],\n",
    "                        \"url\": doc[\"url\"]\n",
    "                    })\n",
    "                    section = []\n",
    "                    section_tokens = 0\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#             page_tokens = simple_tokenizer.tokenize(page)\n",
    "#             for t in page_tokens:\n",
    "#                 page_numbers.append(p+1)\n",
    "#         doc_tokens = doc[\"tokens\"]\n",
    "#         vectors = select_layer(doc[\"vectors\"])\n",
    "# #         print(vectors.shape)\n",
    "#         filename = doc[\"filename\"]\n",
    "#         n_tokens = len(doc_tokens)\n",
    "# #         print(n_tokens)\n",
    "#         tokens_per_section = 100\n",
    "#         n_sections = n_tokens // tokens_per_section\n",
    "# #         n_sections = int(np.floor(tokens_per_section / n_tokens))\n",
    "# #         print(n_sections)\n",
    "#         for s in range(n_sections):\n",
    "#             start_index = s*tokens_per_section\n",
    "#             end_index = ((s+1)*tokens_per_section)\n",
    "#             section_tokens = doc_tokens[start_index:end_index]\n",
    "#             section_vectors = vectors[start_index:end_index,]\n",
    "#             section_start_page = page_numbers[start_index]\n",
    "#             section_end_page = page_numbers[min(len(page_numbers), end_index)-1]\n",
    "#             doc_sections.append({\n",
    "#                 'filename': doc['filename'],\n",
    "#                 'url': doc['url'],\n",
    "#                 'start_page': section_start_page,\n",
    "#                 'end_page': section_end_page,\n",
    "#                 'text': \" \".join(section_tokens),\n",
    "# #                 'tokens': section_tokens,\n",
    "#                 'vectors': section_vectors\n",
    "#             })\n",
    "            \n",
    "#     return doc_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'relevant_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-58ec47ab306c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc_sections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sections: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_sections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_sections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'section_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'relevant_docs' is not defined"
     ]
    }
   ],
   "source": [
    "# doc_sections = segment_docs(relevant_docs)\n",
    "# print(\"sections: {}\".format(len(doc_sections)))\n",
    "# print(doc_sections[0]['section_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "casing = \"lower_non_acronyms\"\n",
    "# casing = \"lower_non_acronyms\"\n",
    "# TODO: is lowercasing necessary?\n",
    "\n",
    "def casing_function():\n",
    "    if casing==\"cased\":\n",
    "        return lambda x: x\n",
    "    elif casing==\"lower\":\n",
    "        return lambda x: x.lower()\n",
    "    elif casing==\"lower_non_acronyms\":\n",
    "        return lambda x: x if x.isupper() else x.lower()\n",
    "    else:\n",
    "        raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use vectors to find closes words to keywords\n",
    "# TODO: why do shorter documents get higher scores?\n",
    "def score_doc_sections(doc_sections, orig_keywords):\n",
    "    orig_keywords = [k.strip() for k in orig_keywords]\n",
    "    keywords = []\n",
    "    for k in orig_keywords:\n",
    "        words = k.split(\" \")\n",
    "        for word in words:\n",
    "            keywords.append(word)\n",
    "    keyword_vectors = single_vector_per_doc([elmo.embed_sentence(keywords)])\n",
    "#     keyword_weights = []\n",
    "#     fix_case = casing_function()\n",
    "# #     idf_smoothing_count = 10\n",
    "#     for k in keywords:\n",
    "# #         words = k.split(\" \")\n",
    "# #         if len(words) > 1:\n",
    "# #             keyword_weights.append(1./idf_smoothing_count)\n",
    "# #         else:\n",
    "# #             k = fix_case(k)\n",
    "# #             if k in idf:\n",
    "# #                 keyword_weights.append(1./(1./idf[k]+idf_smoothing_count))\n",
    "# #             else:\n",
    "# #                 keyword_weights.append(1./idf_smoothing_count)\n",
    "    doc_sections_scores = []\n",
    "    for s, section in enumerate(doc_sections):\n",
    "        section_vectors = single_vector_per_doc([s[\"sentence_vectors\"] for s in section[\"sentences\"]])\n",
    "        section_text = section['section_text']\n",
    "        no_keywords_found = True\n",
    "        for k in orig_keywords:\n",
    "            if bool(re.search(\"([^\\w]|^)\" + k + \"([^\\w]|$)\", section_text)):\n",
    "#             if k in section_text:\n",
    "                # TODO: consider casing\n",
    "                no_keywords_found = False\n",
    "        for k in orig_keywords:\n",
    "            if k.islower():\n",
    "                if bool(re.search(\"([^\\w]|^)\" + k + \"([^\\w]|$)\", section_text.lower())):\n",
    "                    no_keywords_found = False\n",
    "        if no_keywords_found:\n",
    "            score = 0\n",
    "        elif section_vectors.shape[0]>0:\n",
    "            similarities = cosine_similarity(section_vectors, keyword_vectors)\n",
    "#             if threshold_similarity > -1:\n",
    "#                 similarities = similarities*(similarities>threshold_similarity)\n",
    "            keyword_similarities = np.mean(similarities, axis=0)\n",
    "#             score = np.sum(keyword_similarities*keyword_weights)\n",
    "            score = np.mean(keyword_similarities)\n",
    "        else:\n",
    "            score = 0\n",
    "        doc_sections_scores.append(score)\n",
    "\n",
    "    return doc_sections_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_sections_scores = score_doc_sections(\n",
    "#     doc_sections,\n",
    "#     keywords\n",
    "# )\n",
    "# # doc_sections_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_is_too_similar(a, b):\n",
    "    # if there are only 2 edits to get from one text to the other, it's not good\n",
    "    return editdistance.eval(a, b) < 50\n",
    "\n",
    "# make sure we're not giving similar text among the top k (e.g. shows up on both minutes and agenda)\n",
    "def check_repeated_text(top_k, section_text):\n",
    "    for old in top_k:\n",
    "        if text_is_too_similar(old[1][\"section_text\"], section_text):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def select_top_k(doc_sections, doc_sections_scores, k, user_history):\n",
    "    sorted_sections = sorted(zip(doc_sections_scores, doc_sections), key=lambda pair: pair[0], reverse=True)\n",
    "    top_k = []\n",
    "    text_returned = []\n",
    "    for x in sorted_sections:\n",
    "        score = x[0]\n",
    "        if score > 0:\n",
    "            filename = x[1][\"filename\"]\n",
    "            starting_page = x[1][\"sentences\"][0][\"page\"]\n",
    "            ending_page = x[1][\"sentences\"][-1][\"page\"]\n",
    "            section_text = x[1][\"section_text\"]\n",
    "            # debugging parameter -- DO send the same thing multiple times if we're debugging\n",
    "            if filename in [x[1]['filename'] for x in user_history]:\n",
    "                print(\"this user has already seen their top file ({})\".format(filename))\n",
    "            elif check_repeated_text(top_k, section_text):\n",
    "                print(\"this excpert has already been returned\")\n",
    "            else:\n",
    "                top_k.append(x)\n",
    "            if len(top_k) >= k:\n",
    "                break\n",
    "    return top_k\n",
    "\n",
    "def update_with_top_k(history, top_k_sections, query):\n",
    "    for section in top_k_sections:\n",
    "        x = section[1]\n",
    "        x.update(query)\n",
    "        history.append(x)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'original_text', 'tokens', 'filename', 'starting_page', 'starting_line', 'ending_page', 'ending_line', 'section_text', 'section_tokens', 'Municipalities', 'id', 'Keywords', 'Time Window'\n",
    "def reformat_results(results):\n",
    "    reformatted_results = {}\n",
    "    # one per query\n",
    "    for result in results:\n",
    "        username = result['id']\n",
    "        keywords = result['Keywords']\n",
    "        query_id = username + \",\".join(keywords) + \",\".join(result['Municipalities']) + \",\".join(result['Time Window'])\n",
    "        if query_id not in reformatted_results:\n",
    "            reformatted_results[query_id] = {\n",
    "                'user_id': username,\n",
    "                'document_sections': []\n",
    "            }\n",
    "        reformatted_results[query_id]['document_sections'].append({\n",
    "            # TODO\n",
    "            \"section_id\": \"000\",\n",
    "            # TODO\n",
    "            \"doc_url\": result['url'],\n",
    "            \"doc_name\": os.path.basename(result['filename']),\n",
    "            \"user_id\": username,\n",
    "            \"page_number\": result[\"sentences\"][0][\"page\"],\n",
    "            \"keywords\": keywords,\n",
    "            \"text\": result['section_text'].encode('ascii', errors='ignore').decode('ascii')\n",
    "        })\n",
    "    return [reformatted_results[r] for r in reformatted_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading queries\n",
      "[{'Status': 'just_submitted', 'Municipalities': ['Alameda', 'Burlingame', 'Cupertino', 'Hayward', 'Hercules', 'Metropolitan Transportation Commission', 'Mountain View', 'Oakland', 'San Francisco', 'San Jose', 'San Leandro', 'San Mateo County', 'Santa Clara', 'South San Francisco', 'Stockton', 'Sunnyvale'], 'id': 'christopher.h.stock@gmail.com', 'Keywords': ['Google', ' Apple', ' Facebook', ' Stanford', ' Intel', ' Cisco', ' Essex', ' Sobrato', ' Prometheus'], 'Time Window': 'this_month'}, {'Status': 'just_submitted', 'Municipalities': ['Alameda', 'Burlingame', 'Cupertino', 'Hayward', 'Hercules', 'Metropolitan Transportation Commission', 'Mountain View', 'Oakland', 'San Francisco', 'San Jose', 'San Leandro', 'San Mateo County', 'Santa Clara', 'South San Francisco', 'Stockton', 'Sunnyvale'], 'id': 'emily', 'Keywords': ['housing', ' affordable housing', ' homelessness', ' accessory dwelling unit', ' ADU'], 'Time Window': 'this_month'}, {'Status': 'just_submitted', 'Municipalities': ['Alameda', 'Burlingame', 'Cupertino', 'Hayward', 'Hercules', 'Metropolitan Transportation Commission', 'Mountain View', 'Oakland', 'San Francisco', 'San Jose', 'San Leandro', 'San Mateo County', 'Santa Clara', 'South San Francisco', 'Stockton', 'Sunnyvale'], 'id': 'dilcia19@stanford.edu', 'Keywords': ['housing'], 'Time Window': 'this_month'}, {'Status': 'just_submitted', 'Municipalities': ['Alameda', 'Burlingame', 'Cupertino', 'Hayward', 'Hercules', 'Metropolitan Transportation Commission', 'Mountain View', 'Oakland', 'San Francisco', 'San Jose', 'San Leandro', 'San Mateo County', 'Santa Clara', 'South San Francisco', 'Stockton', 'Sunnyvale'], 'id': 'chstock@stanford.edu', 'Keywords': ['Caltrain', ' Amtrak', ' BART', ' Muni', ' VTA', ' traffic', ' 280', ' 101', ' Samtrans', ' FerryACE', 'transportation', 'public transit'], 'Time Window': 'this_month'}, {'Status': 'just_submitted', 'Municipalities': ['Mountain View'], 'id': 'email@emails.com', 'Keywords': ['cars', 'electricity', 'money'], 'Time Window': 'this_month'}, {'Status': 'just_submitted', 'Municipalities': ['Alameda', 'Burlingame', 'Cupertino', 'Hayward', 'Hercules', 'Metropolitan Transportation Commission', 'Mountain View', 'Oakland', 'San Francisco', 'San Jose', 'San Leandro', 'San Mateo County', 'Santa Clara', 'South San Francisco', 'Stockton', 'Sunnyvale'], 'id': 'erindb@stanford.edu', 'Keywords': ['transit', 'traffic', 'highway', 'freeway', 'expressway', 'cars', 'transportation', 'bus', 'train', 'BART', 'MUNI'], 'Time Window': 'this_month'}, {'Status': 'just_submitted', 'Municipalities': ['Mountain View', 'San Jose', 'Santa Clara'], 'id': 'testing@testing.com', 'Keywords': ['Tesla', 'Apple', 'Google', 'Facebook'], 'Time Window': 'this_month'}, {'Status': 'just_submitted', 'Municipalities': ['Alameda', 'Burlingame', 'Cupertino', 'Hayward', 'Hercules', 'Metropolitan Transportation Commission', 'Mountain View', 'Oakland', 'San Francisco', 'San Jose', 'San Leandro', 'San Mateo County', 'Santa Clara', 'South San Francisco', 'Stockton', 'Sunnyvale'], 'id': 'promo@erindb.com', 'Keywords': ['affordable', ' housing', ' ADU', 'dwelling', ' residential', 'homelessness', ' homeless'], 'Time Window': 'this_month'}, {'Status': 'just_submitted', 'Municipalities': ['Alameda', 'Burlingame', 'Cupertino', 'Hayward', 'Hercules', 'Metropolitan Transportation Commission', 'Mountain View', 'Oakland', 'San Francisco', 'San Jose', 'San Leandro', 'San Mateo County', 'Santa Clara', 'South San Francisco', 'Stockton', 'Sunnyvale'], 'id': 'esagara@stanford.edu', 'Keywords': ['wildfire'], 'Time Window': 'last_six_months'}]\n",
      "reading metadata\n",
      "setting up reader\n",
      "finding relevant filenames\n",
      "reading relevant documents\n",
      "0 of 137 documents read\n",
      "100 of 137 documents read\n",
      "Key not found: docs/santa-clara/Santa-Clara_2019-12-02_Economic-Development,-Communications,-And-Marketing-Committee_Agenda.txt\n",
      "Key not found: docs/santa-clara/Santa-Clara_2019-12-05_Historical-&-Landmarks-Commission_Agenda.txt\n",
      "['Google', ' Apple', ' Facebook', ' Stanford', ' Intel', ' Cisco', ' Essex', ' Sobrato', ' Prometheus']\n",
      "docs: 35\n",
      "running query 0 of 9\n",
      "user id: christopher.h.stock@gmail.com\n",
      "['Google', ' Apple', ' Facebook', ' Stanford', ' Intel', ' Cisco', ' Essex', ' Sobrato', ' Prometheus']\n",
      "segmenting documents\n",
      "scoring documents\n",
      "running query 1 of 9\n",
      "user id: emily\n",
      "['housing', ' affordable housing', ' homelessness', ' accessory dwelling unit', ' ADU']\n",
      "segmenting documents\n",
      "scoring documents\n",
      "running query 2 of 9\n",
      "user id: dilcia19@stanford.edu\n",
      "['housing']\n",
      "segmenting documents\n",
      "scoring documents\n",
      "running query 3 of 9\n",
      "user id: chstock@stanford.edu\n",
      "['Caltrain', ' Amtrak', ' BART', ' Muni', ' VTA', ' traffic', ' 280', ' 101', ' Samtrans', ' FerryACE', 'transportation', 'public transit']\n",
      "segmenting documents\n",
      "scoring documents\n",
      "running query 4 of 9\n",
      "user id: email@emails.com\n",
      "['cars', 'electricity', 'money']\n",
      "segmenting documents\n",
      "scoring documents\n",
      "running query 5 of 9\n",
      "user id: erindb@stanford.edu\n",
      "['transit', 'traffic', 'highway', 'freeway', 'expressway', 'cars', 'transportation', 'bus', 'train', 'BART', 'MUNI']\n",
      "segmenting documents\n",
      "scoring documents\n",
      "running query 6 of 9\n",
      "user id: testing@testing.com\n",
      "['Tesla', 'Apple', 'Google', 'Facebook']\n",
      "segmenting documents\n",
      "scoring documents\n",
      "running query 7 of 9\n",
      "user id: promo@erindb.com\n",
      "['affordable', ' housing', ' ADU', 'dwelling', ' residential', 'homelessness', ' homeless']\n",
      "segmenting documents\n",
      "scoring documents\n",
      "running query 8 of 9\n",
      "user id: esagara@stanford.edu\n",
      "['wildfire']\n",
      "segmenting documents\n",
      "scoring documents\n",
      "sending emails\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n",
      "user id: christopher.h.stock@gmail.com\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "print(\"reading queries\")\n",
    "queries = read_queries()\n",
    "queries = [q for q in queries if ('Status' in q and q['Status'] == 'just_submitted')]\n",
    "print(queries)\n",
    "print(\"reading metadata\")\n",
    "metadata = read_metadata()\n",
    "print(\"setting up reader\")\n",
    "# used cached idf and only read relevant documents\n",
    "print(\"finding relevant filenames\")\n",
    "relevant_filenames = find_relevant_filenames(queries, metadata)\n",
    "# (not actually *all*, but all the ones we care about for queries)\n",
    "print(\"reading relevant documents\")\n",
    "all_docs = read_docs(relevant_filenames)\n",
    "\n",
    "query = queries[0]\n",
    "user_id = query[\"id\"]\n",
    "history = []\n",
    "user_history = [x for x in history if x['id'] == user_id]\n",
    "keywords = query[\"Keywords\"]\n",
    "print(keywords)\n",
    "time_window = query[\"Time Window\"]\n",
    "municipalities = query[\"Municipalities\"]\n",
    "relevant_docs = select_relevant_docs(municipalities, time_window, all_docs, metadata)\n",
    "print(\"docs: {}\".format(len(relevant_docs)))\n",
    "\n",
    "for q, query in enumerate(queries):\n",
    "    print(\"running query {} of {}\".format(q, len(queries)))\n",
    "    user_id = query[\"id\"]\n",
    "    print(\"user id: {}\".format(user_id))\n",
    "    user_history = [x for x in history if x['id'] == user_id]\n",
    "    keywords = query[\"Keywords\"]\n",
    "    print(keywords)\n",
    "    time_window = query[\"Time Window\"]\n",
    "    municipalities = query[\"Municipalities\"]\n",
    "    relevant_docs = select_relevant_docs(municipalities, time_window, all_docs, metadata)\n",
    "    print(\"segmenting documents\")\n",
    "    doc_sections = segment_docs(relevant_docs)\n",
    "    print(\"scoring documents\")\n",
    "    doc_sections_scores = score_doc_sections(\n",
    "        doc_sections,\n",
    "        keywords\n",
    "    )\n",
    "    top_k_sections = select_top_k(doc_sections, doc_sections_scores, 5, user_history)\n",
    "    results = update_with_top_k(results, top_k_sections, query)\n",
    "#     history = update_with_top_k(history, top_k_sections, query)\n",
    "\n",
    "print(\"sending emails\")\n",
    "send_emails(reformat_results(results))\n",
    "# write_history(history)\n",
    "# print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool(re.search(\"([^\\w]|^)\" + \"Apple\" + \"([^\\w]|$)\", \"asdfaf (Applebees) asdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
