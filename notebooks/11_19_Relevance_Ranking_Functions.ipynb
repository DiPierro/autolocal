{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "from datetime import *\n",
    "import os\n",
    "from autolocal.parsers.nlp import Tokenizer\n",
    "from gensim.parsing.preprocessing import *\n",
    "\n",
    "from autolocal.databases import S3DocumentManager\n",
    "import boto3\n",
    "from decimal import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up word vectors\n",
    "# (this takes a loooong time)\n",
    "def setup_word_vectors():\n",
    "  print(\"loading language model\")\n",
    "  # load language model (this takes a few minutes)\n",
    "  model = gensim.load('word2vec-google-news-300')\n",
    "  print(\"model loaded\")\n",
    "\n",
    "  vectors = model.wv\n",
    "  del model\n",
    "  vectors.init_sims(True) # normalize the vectors (!), so we can use the dot product as similarity measure\n",
    "\n",
    "  print('embeddings loaded ')\n",
    "  print('loading docs ... ')\n",
    "  return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata():\n",
    "    table = boto3.resource('dynamodb', region_name='us-west-1').Table('autolocal-documents')\n",
    "    s3_client = boto3.client('s3')\n",
    "    metadata = pd.DataFrame(table.scan()[\"Items\"])\n",
    "    metadata[\"date\"] = [datetime.strptime(d, '%Y-%m-%d') for d in metadata[\"date\"]]\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocTextReader():\n",
    "    def __init__(self, log_every=100):\n",
    "        self.log_every = log_every\n",
    "        s3 = boto3.resource('s3', region_name='us-west-1')\n",
    "        self.bucket = s3.Bucket('autolocal-documents')\n",
    "                \n",
    "        # TODO: is lowercasing necessary?\n",
    "        self.preprocess_filters = [\n",
    "            lambda x: x.lower(),\n",
    "            strip_punctuation,\n",
    "            strip_numeric,\n",
    "            strip_non_alphanum,\n",
    "            strip_multiple_whitespaces,\n",
    "            strip_numeric,\n",
    "            remove_stopwords,\n",
    "            strip_short\n",
    "        ]\n",
    "\n",
    "    def read_document_string(self, s3_path):\n",
    "        return self.bucket.Object(s3_path).get()['Body'].read()\n",
    "\n",
    "    def read_docs(self, s3_paths):\n",
    "        # read all documents that we know about\n",
    "        # tokenize each document\n",
    "        # return list of documents\n",
    "\n",
    "        documents = {}\n",
    "        n_docs_total = len(s3_paths)\n",
    "\n",
    "        i = 0\n",
    "        n_docs_read = 0\n",
    "        for s3_path in s3_paths:\n",
    "            try:\n",
    "                doc_string = self.read_document_string(s3_path)\n",
    "                doc_tokens = preprocess_string(doc_string, filters=self.preprocess_filters)\n",
    "                documents[s3_path] = {\n",
    "                    \"original_text\": doc_string,\n",
    "                    \"tokens\": doc_tokens\n",
    "                }\n",
    "            except Exception as e:\n",
    "                if i < 10:\n",
    "                    print(\"Key not found: {}\".format(s3_path))\n",
    "                elif i == 10:\n",
    "                    print(\"More than 10 keys not found\")\n",
    "                    print(e)\n",
    "                    break\n",
    "                i+=1\n",
    "            if n_docs_read % self.log_every == 0:\n",
    "                print(\"{} of {} documents read\".format(n_docs_read, n_docs_total))\n",
    "            n_docs_read+=1\n",
    "\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries(query_source):\n",
    "    if query_source == \"actual\":\n",
    "        table = boto3.resource('dynamodb', region_name='us-west-2').Table('autoLocalNews')\n",
    "    elif query_source == \"quick\":\n",
    "        table = boto3.resource('dynamodb', region_name='us-west-1').Table('quick_queries')\n",
    "    else:\n",
    "        raise Exception\n",
    "    queries = table.scan()[\"Items\"]\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_history():\n",
    "    try:\n",
    "        table = boto3.resource('dynamodb', region_name='us-west-1').Table('history')\n",
    "        history = table.scan()[\"Items\"]\n",
    "    except:\n",
    "        history = []\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cached_idf():\n",
    "    try:\n",
    "        table = boto3.resource('dynamodb', region_name='us-west-1').Table('idf')\n",
    "        cached_idf = table.scan()[\"Items\"]\n",
    "    except:\n",
    "        cached_idf = None\n",
    "    idf = {}\n",
    "    for word in cached_idf:\n",
    "        w = word['word']\n",
    "        idf[w] = float(word['idf'])\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_idf(idf):\n",
    "    dynamodb = boto3.resource('dynamodb')\n",
    "    try:\n",
    "        table = boto3.resource('dynamodb', region_name='us-west-1').Table('idf')\n",
    "    except:\n",
    "        table_args = {\n",
    "            'TableName': 'idf',\n",
    "            'KeySchema': [\n",
    "                {\n",
    "                    'AttributeName': 'word',\n",
    "                    'KeyType': 'HASH'\n",
    "                }\n",
    "            ],\n",
    "            'AttributeDefinitions': [\n",
    "                {\n",
    "                    'AttributeName': 'word',\n",
    "                    'AttributeType': 'S'\n",
    "                }        \n",
    "            ],\n",
    "            'ProvisionedThroughput': {\n",
    "                'ReadCapacityUnits': 5,\n",
    "                'WriteCapacityUnits': 5\n",
    "            }\n",
    "        }\n",
    "\n",
    "        table = dynamodb.create_table(**table_args)\n",
    "    with table.batch_writer() as batch:\n",
    "        for word in idf:\n",
    "            item = {'word': word, 'idf': idf[word]}\n",
    "            item_dump = json.dumps(item)\n",
    "            item = json.loads(item_dump, parse_float=Decimal)\n",
    "            batch.put_item(Item=item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf(all_docs): \n",
    "    # for each word, how many unique docs does it show up in?\n",
    "    from collections import Counter\n",
    "    \n",
    "    doc_freq = {}\n",
    "    for document in all_docs: \n",
    "        tokens = all_docs[document][\"tokens\"]\n",
    "        for token in tokens:\n",
    "            if token in doc_freq:\n",
    "                doc_freq[token] += 1\n",
    "            else:\n",
    "                doc_freq[token] = 1\n",
    "    \n",
    "    inverse_doc_freq = {}\n",
    "    for word in doc_freq:\n",
    "        inverse_doc_freq[word] = 1./doc_freq[word]\n",
    "    \n",
    "    return inverse_doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_windows = {\n",
    "    'this_week': datetime.now() - timedelta(weeks=1),\n",
    "    'this_year': datetime.now() - timedelta(days=365),\n",
    "    'past_six_months':datetime.now() - timedelta(days=183),\n",
    "    'all': None\n",
    "}\n",
    "\n",
    "def find_relevant_filenames(queries, metadata): \n",
    "    \n",
    "    # filter metadata to only those files that match the query municipality and time_window\n",
    "    municipalities_by_time_window = {}\n",
    "    for query in queries:\n",
    "        time_window = query['Time Window']\n",
    "        if time_window in municipalities_by_time_window:\n",
    "            municipalities_by_time_window[time_window].update(query['Municipalities'])\n",
    "        else:\n",
    "            municipalities_by_time_window[time_window] = set(query['Municipalities'])\n",
    "            \n",
    "    relevant_filenames = set()\n",
    "    for time_window in municipalities_by_time_window:\n",
    "        starting_date = time_windows[time_window]\n",
    "        potential_documents = metadata\n",
    "        if starting_date:\n",
    "            potential_documents = potential_documents[potential_documents[\"date\"] >= starting_date]\n",
    "        cities = municipalities_by_time_window[time_window]\n",
    "        potential_documents = potential_documents[[(c in cities) for c in potential_documents[\"city\"]]]\n",
    "        relevant_filenames.update(potential_documents['local_path_txt'])\n",
    "    return relevant_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_quick_queries():\n",
    "#     example_queries = [\n",
    "#       {\n",
    "#         \"id\": \"emily\",\n",
    "#         \"Keywords\": [\n",
    "#           \"affordable\",\n",
    "#           \"housing\",\n",
    "#           \"ADU\",\n",
    "#           \"vote\",\n",
    "#           \"residential\",\n",
    "#           \"homeless\"\n",
    "#         ],\n",
    "#         \"Municipalities\": [\n",
    "#           \"San Jose\",\n",
    "#           \"Cupertino\",\n",
    "#           \"Sunnyvale\",\n",
    "#           \"Palo Alto\",\n",
    "#           \"Mountain View\"\n",
    "#         ],\n",
    "#         \"Time Window\": \"this_week\"\n",
    "#       },\n",
    "#       {\n",
    "#         \"id\": \"some_other_journalist\",\n",
    "#         \"Keywords\": [\n",
    "#           \"affordable\",\n",
    "#           \"housing\",\n",
    "#           \"ADU\",\n",
    "#           \"vote\",\n",
    "#           \"residential\",\n",
    "#           \"homeless\"\n",
    "#         ],\n",
    "#         \"Time Window\": \"past_six_months\",\n",
    "#         \"Municipalities\": [\n",
    "#           \"Sunnyvale\",\n",
    "#           \"San Jose\"\n",
    "#         ]\n",
    "#       }\n",
    "#     ]\n",
    "#     dynamodb = boto3.resource('dynamodb')\n",
    "#     try:\n",
    "#         print('reading')\n",
    "#         table = dynamodb.Table('quick_queries')\n",
    "#         table.scan()\n",
    "#     except:\n",
    "#         print('making')\n",
    "#         table_args = {\n",
    "#             'TableName': 'quick_queries',\n",
    "#             'KeySchema': [\n",
    "#                 {\n",
    "#                     'AttributeName': 'id',\n",
    "#                     'KeyType': 'HASH'\n",
    "#                 }\n",
    "#             ],\n",
    "#             'AttributeDefinitions': [\n",
    "#                 {\n",
    "#                     'AttributeName': 'id',\n",
    "#                     'AttributeType': 'S'\n",
    "#                 }        \n",
    "#             ],\n",
    "#             'ProvisionedThroughput': {\n",
    "#                 'ReadCapacityUnits': 5,\n",
    "#                 'WriteCapacityUnits': 5\n",
    "#             }\n",
    "#         }\n",
    "\n",
    "#         table = dynamodb.create_table(**table_args)\n",
    "#     print('writing')\n",
    "#     with table.batch_writer() as batch:\n",
    "#         for item in example_queries:\n",
    "#             item_dump = json.dumps(item)\n",
    "#             item = json.loads(item_dump, parse_float=Decimal)\n",
    "#             batch.put_item(Item=item)\n",
    "# write_quick_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_results = [\n",
    "#   {\n",
    "#     \"user_id\": \"emily\",\n",
    "#    'special',\n",
    "#    'meeting',\n",
    "#    'subject',\n",
    "#    'approve',\n",
    "#    'august',\n",
    "#     \"document_sections\": [\n",
    "#       {\n",
    "#         \"section_id\": \"000\",\n",
    "#         \"doc_url\": \"https://sanjose.legistar.com/View.ashx?M=A&ID=709096&GUID=CF2165D1-28CB-4670-AB54-F35B649DE71D\",\n",
    "#         \"doc_name\": \"Agenda 2019-09-24\",\n",
    "#         \"user_id\": \"emily\",\n",
    "#         \"page_number\": \"11\",\n",
    "#         \"keywords\": [\"High-Rise\", \"Incentive\", \"Affordable\", \"Housing\", \"exemption\", \"tax\", \"reduction\"],\n",
    "#         \"text\": \"4.3 19-821 Downtown High-Rise Incentive Program.\\n\\nRecommendation: Accept the report on the Downtown High-Rise Feasibility Assessment and direct staff to return to Council with the appropriate ordinance and resolution to enact the following:\\n\\n(a) Extending the certificate of occupancy deadline for the Affordable Housing Impact Fee exemption to December 31, 2023.\\n(b) Amending Title 4.46 and align the construction tax reduction with the certificate of occupancy deadline for the Affordable Housing Impact Fee exemption, and removing the planning and build permit requirements.\\nCEQA: Not a Project, File No. PP17-009, Staff Reports, Assessments, Annual Reports, and Informational Memos that involve no approvals of any City action. (Economic Development)\"\n",
    "#       },\n",
    "#       {\n",
    "#         \"section_id\": \"001\",\n",
    "#         \"doc_url\": \"https://agendaonline.net/public/Meeting.aspx?AgencyID=123&MeetingID=20136&AgencyTypeID=1&IsArchived=True\",\n",
    "#         \"doc_name\": \"School Board Agenda, 2019-9-23\",\n",
    "#         \"user_id\": \"emily\",\n",
    "#         \"page_number\": \"NA\",\n",
    "#         \"keywords\": [\"employee\", \"housing\", \"affordable\"],\n",
    "#         \"text\": \"A.3. Master Plan for San Jose Unified Properties - Step 3 (ACTION)\\n\\nRECOMMENDATION: That San Jose Unified secure pre-development services to complete a pre-development analysis on the potential for employee housing opportunities at the following four locations: (1) 855 Lenzen Avenue, San Jose Unified District Offices, (2) 1088 Broadway, River Glen K-8 School, (3) 1325 Bouret Drive, Second Start-Pine Hill Non-Public School, and (4) 760 Hillsdale Avenue and 705-745 Capital Expressway, Metropolitan Education District.\"\n",
    "#       },\n",
    "#       {\n",
    "#         \"section_id\": \"002\",\n",
    "#         \"doc_url\": \"https://agendaonline.net/public/Meeting.aspx?AgencyID=123&MeetingID=20136&AgencyTypeID=1&IsArchived=True\",\n",
    "#         \"doc_name\": \"School Board Agenda, 2019-9-23\",\n",
    "#         \"user_id\": \"emily\",\n",
    "#         \"page_number\": \"NA\",\n",
    "#         \"keywords\": [\"employee\", \"housing\", \"affordable\"],\n",
    "#         \"text\": \"A.3. Master Plan for San Jose Unified Properties - Step 3 (ACTION)\\n\\nRECOMMENDATION: That San Jose Unified secure pre-development services to complete a pre-development analysis on the potential for employee housing opportunities at the following four locations: (1) 855 Lenzen Avenue, San Jose Unified District Offices, (2) 1088 Broadway, River Glen K-8 School, (3) 1325 Bouret Drive, Second Start-Pine Hill Non-Public School, and (4) 760 Hillsdale Avenue and 705-745 Capital Expressway, Metropolitan Education District.\"\n",
    "#       }\n",
    "#     ]\n",
    "#   }\n",
    "# ]\n",
    "# example_top_k_sections = []\n",
    "# example_query = {\n",
    "#     \"original_text\": \"I am a document document\",\n",
    "#     \"tokens\": [\"I\", \"am\"],\n",
    "#     \"filename\": \"filename1\",\n",
    "#     \"section_id\": \"section1\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_emails(results): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_history(history, top_k_sections, query): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_history(history): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_docs(municipalities, time_window, all_docs, metadata):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_docs(relevant_docs, keywords): \n",
    "    return [\n",
    "        {\n",
    "            \"original_text\": \"I am a document document\",\n",
    "            \"tokens\": [\"I\", \"am\"],\n",
    "            \"filename\": \"filename1\",\n",
    "            \"section_id\": \"section1\",\n",
    "        },\n",
    "        {\n",
    "            \"original_text\": \"I am a document document\",\n",
    "            \"tokens\": [\"a\", \"document\"],\n",
    "            \"filename\": \"filename1\",\n",
    "            \"section_id\": \"section2\",\n",
    "        },\n",
    "        {\n",
    "            \"original_text\": \"I am another doc\",\n",
    "            \"tokens\": [\"I\", \"am\", \"another\", \"doc\"],\n",
    "            \"filename\": \"filename2\",\n",
    "            \"section\": \"section3\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_doc_section(doc_section, keywords, idf):\n",
    "#     # vectorize etc.\n",
    "#     keyword_vectors = np.array([vectors[t] for t in keywords if t in inverse_doc_props])\n",
    "#     keyword_weights = np.array([inverse_doc_props[t] for t in keywords if t in inverse_doc_props])\n",
    "#     document_section_scores = []\n",
    "#     for s, section in enumerate(document_sections):\n",
    "#         score = None\n",
    "#         section_tokens = section[0]\n",
    "#         # TODO: Zipf to figure out what the cutoff should be for normal communication\n",
    "#         if len(set(section_tokens))<20:\n",
    "#             score = 0\n",
    "#         else:\n",
    "#             section_vectors = np.array([vectors[t] for t in section_tokens if t in inverse_doc_props])\n",
    "#             if section_vectors.shape[0]>0:\n",
    "#     #             section_weights = np.array([inverse_doc_props[t] for t in section_tokens if t in inverse_doc_props])\n",
    "#                 similarities = cosine_similarity(section_vectors, keyword_vectors)\n",
    "#     #             similarities = similarities * section_weights\n",
    "#     #             similarities = similarities*(similarities>0.2)\n",
    "#                 keyword_similarities = np.mean(similarities, axis=0)\n",
    "#     #             keyword_similarities = np.average(similarities, axis=0, weights=section_weights)\n",
    "#                 score = np.sum(keyword_similarities*keyword_weights)\n",
    "#         document_section_scores.append(score)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_k(doc_sections, doc_scores, k, history): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_results(results, top_k_sections, query): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading queries\n",
      "reading metadata\n",
      "setting up reader\n",
      "loading cached idf\n",
      "finding relevant filenames\n",
      "reading relevant documents\n",
      "0 of 34 documents read\n",
      "reading history\n",
      "running query 0 of 2\n",
      "scoring documents\n",
      "running query 1 of 2\n",
      "scoring documents\n",
      "sending emails\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# vectors = setup_word_vectors()\n",
    "def run_queries(use_cached_idf = False, query_source=\"actual\", k=3): \n",
    "    print(\"reading queries\")\n",
    "    queries = read_queries(query_source)\n",
    "    print(\"reading metadata\")\n",
    "    metadata = read_metadata()\n",
    "    print(\"setting up reader\")\n",
    "    doc_text_reader = DocTextReader(log_every=100)\n",
    "    if use_cached_idf:\n",
    "        # used cached idf and only read relevant documents\n",
    "        print(\"loading cached idf\")\n",
    "        idf = read_cached_idf()\n",
    "        print(\"finding relevant filenames\")\n",
    "        relevant_filenames = find_relevant_filenames(queries, metadata)\n",
    "        # (not actually *all*, but all the ones we care about for queries)\n",
    "        print(\"reading relevant documents\")\n",
    "        all_docs = doc_text_reader.read_docs(relevant_filenames)\n",
    "    else:\n",
    "        # read all documents and calculate inverse document frequency\n",
    "        all_filenames = metadata[\"local_path_txt\"]\n",
    "        print(\"reading all documents\")\n",
    "        all_docs = doc_text_reader.read_docs(all_filenames)\n",
    "        print(\"calculating idf\")\n",
    "        idf = calculate_idf(all_docs)\n",
    "        cache_idf(idf)\n",
    "    print(\"reading history\")\n",
    "    history = read_history()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for q, query in enumerate(queries): \n",
    "        print(\"running query {} of {}\".format(q, len(queries)))\n",
    "        user_id = query[\"id\"]\n",
    "        keywords = query[\"Keywords\"]\n",
    "        time_window = query[\"Time Window\"]\n",
    "        municipalities = query[\"Municipalities\"]\n",
    "        relevant_docs = select_relevant_docs(municipalities, time_window, all_docs, metadata)\n",
    "        doc_sections = segment_docs(relevant_docs, keywords)\n",
    "        print(\"scoring documents\")\n",
    "        doc_sections_scores = [score_doc_section(doc_section, keywords, idf) for doc_section in doc_sections]\n",
    "        top_k_sections = select_top_k(doc_sections, doc_sections_scores, k, history)\n",
    "        results = update_results(results, top_k_sections, query)\n",
    "        history = update_history(history, top_k_sections, query)\n",
    "        \n",
    "    print(\"sending emails\")\n",
    "    send_emails(results)\n",
    "    write_history(history)\n",
    "    print(\"finished\")\n",
    "\n",
    "\n",
    "run_queries(use_cached_idf=True, query_source=\"quick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
