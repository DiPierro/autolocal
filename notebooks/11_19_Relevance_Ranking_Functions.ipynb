{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "from autolocal.nlp import Tokenizer\n",
    "from gensim.parsing.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading language model\n",
      "model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/autolocal/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings loaded \n",
      "loading docs ... \n"
     ]
    }
   ],
   "source": [
    "# set up word vectors\n",
    "# (this takes a loooong time)\n",
    "\n",
    "print(\"loading language model\")\n",
    "# load language model (this takes a few minutes)\n",
    "model = api.load('word2vec-google-news-300')\n",
    "print(\"model loaded\")\n",
    "\n",
    "vectors = model.wv\n",
    "del model\n",
    "vectors.init_sims(True) # normalize the vectors (!), so we can use the dot product as similarity measure\n",
    "\n",
    "print('embeddings loaded ')\n",
    "print('loading docs ... ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt_name(f):\n",
    "    path_parts = os.path.split(f)\n",
    "    fname = path_parts[1]\n",
    "    local_dir = os.path.basename(path_parts[0])\n",
    "    return os.path.join(local_dir, fname)[:-4]+\".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata():\n",
    "    # return data frame with metadata\n",
    "    metadata = pd.read_csv(\"../data/index_sfbay_small/metadata.csv\")\n",
    "    metadata[\"txt_file\"] = [get_txt_name(f) for f in metadata[\"local_path_pdf\"]]\n",
    "    metadata = metadata[[\"txt_file\", \"city\", \"committee\", \"date\"]]\n",
    "    metadata[\"date\"] = [datetime.strptime(d, '%Y-%m-%d') for d in metadata[\"date\"]]\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5ba764848e00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mread_queries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-5ba764848e00>\u001b[0m in \u001b[0;36mread_queries\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# return list of queries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/user_data/sample_input_data.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "def read_queries():\n",
    "    # read from input file\n",
    "    # parse as json\n",
    "    # return list of queries\n",
    "    with open('../data/user_data/sample_input_data.json') as json_file:\n",
    "        queries = json.load(json_file)\n",
    "    return queries\n",
    "\n",
    "read_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5a25959e01f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mread_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-5a25959e01f8>\u001b[0m in \u001b[0;36mread_history\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/user_data/sample_history.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mread_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "def read_history(): \n",
    "    with open('../data/user_data/sample_history.json') as json_file:\n",
    "        history = json.load(json_file)\n",
    "    return history\n",
    "read_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# TODO: is lowercasing necessary?\n",
    "preprocess_filters = [\n",
    "    lambda x: x.lower(),\n",
    "    strip_punctuation,\n",
    "    strip_numeric,\n",
    "    strip_non_alphanum,\n",
    "    strip_multiple_whitespaces,\n",
    "    strip_numeric,\n",
    "    remove_stopwords,\n",
    "    strip_short\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(metadata):\n",
    "    # read all documents that we know about\n",
    "    # tokenize each document\n",
    "    # return list of documents\n",
    "    \n",
    "\n",
    "    documents = []\n",
    "\n",
    "    document_files = metadata[\"txt_file\"]\n",
    "    directory = '../data/docs'\n",
    "    for filename in document_files:\n",
    "        try:\n",
    "            f = open(os.path.join(directory, filename))\n",
    "            \n",
    "            doc_string = f.read()\n",
    "            doc_tokens = preprocess_string(doc_string, filters=preprocess_filters)\n",
    "            \n",
    "            documents.append({\"original_text\": doc_string,\n",
    "                              \"tokens\": doc_tokens,\n",
    "                              \"filename\": filename})\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cached_idf(): \n",
    "    # read cached idf file and conver to json\n",
    "    \n",
    "read_cached_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_results = [\n",
    "  {\n",
    "    \"user_id\": \"emily\",\n",
    "   'special',\n",
    "   'meeting',\n",
    "   'subject',\n",
    "   'approve',\n",
    "   'august',\n",
    "    \"document_sections\": [\n",
    "      {\n",
    "        \"section_id\": \"000\",\n",
    "        \"doc_url\": \"https://sanjose.legistar.com/View.ashx?M=A&ID=709096&GUID=CF2165D1-28CB-4670-AB54-F35B649DE71D\",\n",
    "        \"doc_name\": \"Agenda 2019-09-24\",\n",
    "        \"user_id\": \"emily\",\n",
    "        \"page_number\": \"11\",\n",
    "        \"keywords\": [\"High-Rise\", \"Incentive\", \"Affordable\", \"Housing\", \"exemption\", \"tax\", \"reduction\"],\n",
    "        \"text\": \"4.3 19-821 Downtown High-Rise Incentive Program.\\n\\nRecommendation: Accept the report on the Downtown High-Rise Feasibility Assessment and direct staff to return to Council with the appropriate ordinance and resolution to enact the following:\\n\\n(a) Extending the certificate of occupancy deadline for the Affordable Housing Impact Fee exemption to December 31, 2023.\\n(b) Amending Title 4.46 and align the construction tax reduction with the certificate of occupancy deadline for the Affordable Housing Impact Fee exemption, and removing the planning and build permit requirements.\\nCEQA: Not a Project, File No. PP17-009, Staff Reports, Assessments, Annual Reports, and Informational Memos that involve no approvals of any City action. (Economic Development)\"\n",
    "      },\n",
    "      {\n",
    "        \"section_id\": \"001\",\n",
    "        \"doc_url\": \"https://agendaonline.net/public/Meeting.aspx?AgencyID=123&MeetingID=20136&AgencyTypeID=1&IsArchived=True\",\n",
    "        \"doc_name\": \"School Board Agenda, 2019-9-23\",\n",
    "        \"user_id\": \"emily\",\n",
    "        \"page_number\": \"NA\",\n",
    "        \"keywords\": [\"employee\", \"housing\", \"affordable\"],\n",
    "        \"text\": \"A.3. Master Plan for San Jose Unified Properties - Step 3 (ACTION)\\n\\nRECOMMENDATION: That San Jose Unified secure pre-development services to complete a pre-development analysis on the potential for employee housing opportunities at the following four locations: (1) 855 Lenzen Avenue, San Jose Unified District Offices, (2) 1088 Broadway, River Glen K-8 School, (3) 1325 Bouret Drive, Second Start-Pine Hill Non-Public School, and (4) 760 Hillsdale Avenue and 705-745 Capital Expressway, Metropolitan Education District.\"\n",
    "      },\n",
    "      {\n",
    "        \"section_id\": \"002\",\n",
    "        \"doc_url\": \"https://agendaonline.net/public/Meeting.aspx?AgencyID=123&MeetingID=20136&AgencyTypeID=1&IsArchived=True\",\n",
    "        \"doc_name\": \"School Board Agenda, 2019-9-23\",\n",
    "        \"user_id\": \"emily\",\n",
    "        \"page_number\": \"NA\",\n",
    "        \"keywords\": [\"employee\", \"housing\", \"affordable\"],\n",
    "        \"text\": \"A.3. Master Plan for San Jose Unified Properties - Step 3 (ACTION)\\n\\nRECOMMENDATION: That San Jose Unified secure pre-development services to complete a pre-development analysis on the potential for employee housing opportunities at the following four locations: (1) 855 Lenzen Avenue, San Jose Unified District Offices, (2) 1088 Broadway, River Glen K-8 School, (3) 1325 Bouret Drive, Second Start-Pine Hill Non-Public School, and (4) 760 Hillsdale Avenue and 705-745 Capital Expressway, Metropolitan Education District.\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "example_top_k_sections = []\n",
    "example_query = {\n",
    "    \"original_text\": \"I am a document document\",\n",
    "    \"tokens\": [\"I\", \"am\"],\n",
    "    \"filename\": \"filename1\",\n",
    "    \"section_id\": \"section1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(results): \n",
    "    pass\n",
    "write_output(example_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_history(history, top_k_sections, query): \n",
    "    pass\n",
    "update_history(read_history(), example_top_k_sections, example_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_history(history): \n",
    "    pass\n",
    "write_history(read_history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ee13b80f9523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"am\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m\"filename\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"filename1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;34m\"section_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"section1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m })\n",
      "\u001b[0;32m<ipython-input-12-ee13b80f9523>\u001b[0m in \u001b[0;36mcalculate_idf\u001b[0;34m(all_docs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_docs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "def calculate_idf(all_docs): \n",
    "    # for each word, how many unique docs does it show up in?\n",
    "#     from collections import Counter\n",
    "#     word_counts = \n",
    "    \n",
    "#     idf = {}\n",
    "#     for document in all_docs: \n",
    "#         tokens = all_docs[document][\"tokens\"]\n",
    "#         for \n",
    "#         \"I\": 1./2,\n",
    "#         \"am\": 1./2,\n",
    "#         \"a\": 1./1,\n",
    "#         \"another\": 1./1,\n",
    "#         \"document\": 1./1,\n",
    "#         \"doc\": 1./1\n",
    "#     }\n",
    "    \n",
    "    # Version 1\n",
    "    idf = {}\n",
    "    for doc in all_docs: \n",
    "        tokens = all_docs[doc][\"tokens\"]\n",
    "        for token in tokens: \n",
    "            if token not in idf: \n",
    "                idf[\"token\"] = 0\n",
    "            if token in doc: \n",
    "                idf[\"token\"] += 1\n",
    "\n",
    "    for token in idf: \n",
    "        idf[token] = 1.0 / idf[token]\n",
    "    \n",
    "    print(idf) \n",
    "    return idf\n",
    "calculate_idf({\n",
    "    \"original_text\": \"I am a document document\",\n",
    "    \"tokens\": [\"I\", \"am\"],\n",
    "    \"filename\": \"filename1\",\n",
    "    \"section_id\": \"section1\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_idf(idf):\n",
    "    # write the idf dict to a file\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_docs(relevant_docs, keywords): \n",
    "    return [\n",
    "        {\n",
    "            \"original_text\": \"I am a document document\",\n",
    "            \"tokens\": [\"I\", \"am\"],\n",
    "            \"filename\": \"filename1\",\n",
    "            \"section_id\": \"section1\",\n",
    "        },\n",
    "        {\n",
    "            \"original_text\": \"I am a document document\",\n",
    "            \"tokens\": [\"a\", \"document\"],\n",
    "            \"filename\": \"filename1\",\n",
    "            \"section_id\": \"section2\",\n",
    "        },\n",
    "        {\n",
    "            \"original_text\": \"I am another doc\",\n",
    "            \"tokens\": [\"I\", \"am\", \"another\", \"doc\"],\n",
    "            \"filename\": \"filename2\",\n",
    "            \"section\": \"section3\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_docs(municipalities, time_window, all_docs, metadata): \n",
    "    # filter metadata to only those files that match the query municipality and time_window\n",
    "    # filter all docs to only filenames in subset of metadata\n",
    "    return [\n",
    "        {\n",
    "            \"original_text\": \"I am a document document\",\n",
    "            \"tokens\": [\"I\", \"am\", \"a\", \"document\"],\n",
    "            \"filename\": \"filename1\"\n",
    "        },\n",
    "        {\n",
    "            \"original_text\": \"I am another doc\",\n",
    "            \"tokens\": [\"I\", \"am\", \"another\", \"doc\"],\n",
    "            \"filename\": \"filename2\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_doc_section(doc_section, keywords, idf):\n",
    "    # vectorize etc.\n",
    "    keyword_vectors = np.array([vectors[t] for t in keywords if t in inverse_doc_props])\n",
    "    keyword_weights = np.array([inverse_doc_props[t] for t in keywords if t in inverse_doc_props])\n",
    "    document_section_scores = []\n",
    "    for s, section in enumerate(document_sections):\n",
    "        score = None\n",
    "        section_tokens = section[0]\n",
    "        # TODO: Zipf to figure out what the cutoff should be for normal communication\n",
    "        if len(set(section_tokens))<20:\n",
    "            score = 0\n",
    "        else:\n",
    "            section_vectors = np.array([vectors[t] for t in section_tokens if t in inverse_doc_props])\n",
    "            if section_vectors.shape[0]>0:\n",
    "    #             section_weights = np.array([inverse_doc_props[t] for t in section_tokens if t in inverse_doc_props])\n",
    "                similarities = cosine_similarity(section_vectors, keyword_vectors)\n",
    "    #             similarities = similarities * section_weights\n",
    "    #             similarities = similarities*(similarities>0.2)\n",
    "                keyword_similarities = np.mean(similarities, axis=0)\n",
    "    #             keyword_similarities = np.average(similarities, axis=0, weights=section_weights)\n",
    "                score = np.sum(keyword_similarities*keyword_weights)\n",
    "        document_section_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_k(doc_sections, doc_scores, k, history): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_results(results, top_k_sections, query): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_queries(use_cached_idf = False): \n",
    "    queries = read_queries()\n",
    "    metadata = read_metadata()\n",
    "    all_docs = read_docs(metadata)\n",
    "    if use_cached_idf:\n",
    "        idf = read_cached_idf()\n",
    "    else:\n",
    "        idf = calculate_idf(all_docs)\n",
    "        cache_idf(idf)\n",
    "    history = read_history()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for query in queries: \n",
    "        user_id = query[\"user_id\"]\n",
    "        query_id = query[\"query_id\"]\n",
    "        keywords = query[\"keywords\"]\n",
    "        time_window = query[\"time_window\"]\n",
    "        municipalities = query[\"municipalities\"]\n",
    "        relevant_docs = select_relevant_docs(municipalities, time_window, all_docs, metadata)\n",
    "        doc_sections = segment_docs(relevant_docs, keywords)\n",
    "        doc_sections_scores = score_doc_section(doc_section, keywords, idf)\n",
    "        top_k_sections = select_top_k(doc_sections, doc_scores, k, history)\n",
    "        results = update_results(results, top_k_sections, query)\n",
    "        history = update_history(history, top_k_sections, query)\n",
    "        \n",
    "    write_output(results)\n",
    "    write_history(history)\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autolocal",
   "language": "python",
   "name": "autolocal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
