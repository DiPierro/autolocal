{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "from datetime import *\n",
    "import os\n",
    "from autolocal.parsers.nlp import Tokenizer\n",
    "from gensim.parsing.preprocessing import *\n",
    "\n",
    "from autolocal.databases import S3DocumentManager\n",
    "import boto3\n",
    "from decimal import *\n",
    "from  tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up word vectors\n",
    "# (this takes a loooong time)\n",
    "def setup_word_vectors():\n",
    "    s3 = boto3.resource('s3', region_name='us-west-1')\n",
    "    bucket = s3.Bucket('autolocal-documents')\n",
    "    body = bucket.Object('gensim_data.p').get()['Body'].read()\n",
    "    return pickle.loads(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_v = setup_word_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.89365628e-02, -3.40450332e-02,  1.08787581e-01,  8.88301507e-02,\n",
       "       -1.92726199e-02, -7.00466782e-02, -3.79582569e-02, -1.51050389e-01,\n",
       "       -1.48702450e-02,  6.37855232e-02,  3.61973071e-03,  6.21224055e-03,\n",
       "       -9.73414071e-03, -3.81539166e-02, -1.40876010e-01, -8.33516344e-02,\n",
       "       -1.18179314e-01,  6.73074275e-02, -1.11918159e-01, -1.29918978e-01,\n",
       "        3.79582569e-02,  2.43598092e-02, -2.50446238e-02, -1.18179314e-01,\n",
       "        1.33049563e-02, -8.85366555e-03,  4.77413125e-02,  6.26115594e-03,\n",
       "        9.93958488e-02,  1.29136341e-02,  4.61760238e-02,  5.67417257e-02,\n",
       "        7.98297375e-02, -4.51977178e-02, -3.32623906e-03,  1.47724142e-02,\n",
       "        1.62398722e-02, -1.23266503e-02, -5.28285019e-02, -1.40876006e-02,\n",
       "        3.05231344e-02, -3.07187960e-02,  6.35898625e-03, -5.40024675e-02,\n",
       "        7.63078332e-02, -5.43937907e-02,  1.90769583e-02, -6.76987469e-02,\n",
       "        3.52190025e-02,  5.28285019e-02, -8.45256001e-02,  5.55677563e-02,\n",
       "        3.56103219e-02, -2.27945205e-02, -6.73074275e-02,  1.45767536e-02,\n",
       "       -7.04380032e-03, -2.91535072e-02,  5.23393508e-03,  1.26201417e-02,\n",
       "       -6.69161007e-02, -9.44064930e-03, -6.76987469e-02, -2.87621841e-02,\n",
       "       -3.87409031e-02,  7.55251944e-02, -4.53933813e-02,  1.11893704e-03,\n",
       "        6.05326612e-04,  5.55677563e-02,  1.00961134e-01,  5.36111481e-02,\n",
       "        7.70904794e-02, -7.90470913e-02,  1.29136341e-02,  7.45958008e-04,\n",
       "        1.04091719e-01,  7.23946169e-02, -7.82644451e-02,  1.40876010e-01,\n",
       "        5.79156913e-02, -1.00961134e-01, -4.79369750e-03, -5.79156913e-02,\n",
       "        1.73160098e-02, -1.41242868e-03, -9.90045294e-02,  3.22840847e-02,\n",
       "        4.73499894e-02, -1.06146159e-02,  1.16907516e-02,  4.38280925e-02,\n",
       "       -1.71203481e-03,  6.37855232e-02, -4.98935841e-02,  7.48403789e-03,\n",
       "        3.09144575e-02,  3.07187960e-02,  7.53295328e-03,  6.88727126e-02,\n",
       "       -5.60079934e-03, -2.00552642e-02,  5.04805669e-02, -3.04497615e-03,\n",
       "       -6.99488493e-03, -7.31772557e-02,  1.44006580e-01,  6.70139352e-03,\n",
       "       -3.93278860e-02, -2.06422489e-02, -3.15503543e-03, -1.16614029e-01,\n",
       "        2.18895869e-03,  7.90470913e-02,  9.07867625e-02,  1.09570231e-02,\n",
       "        3.67842913e-02,  6.26115575e-02, -1.89791285e-02,  9.93958488e-02,\n",
       "       -1.72181781e-02, -4.89152782e-02, -3.77625972e-02, -3.60016450e-02,\n",
       "        7.04380032e-03,  1.49680758e-02,  9.63630993e-03,  4.96979244e-02,\n",
       "        9.03954357e-02, -2.20118761e-02, -2.11314000e-02, -4.36324291e-02,\n",
       "        1.21309897e-02, -4.63716835e-02,  3.47298477e-03,  3.69799510e-02,\n",
       "       -1.88812986e-02,  2.10335702e-02,  4.46107350e-02, -1.52615672e-02,\n",
       "       -3.83495800e-02, -6.18289150e-02, -1.77073311e-02, -1.10548530e-02,\n",
       "        9.58739519e-02,  7.31772557e-02,  6.84813922e-03, -2.41641477e-02,\n",
       "       -1.50659066e-02, -6.33942038e-02,  1.60442125e-02,  1.49680758e-02,\n",
       "       -7.43512288e-02,  3.44363563e-02, -3.07187960e-02,  1.79029927e-02,\n",
       "       -4.67630066e-02, -1.05657004e-01,  3.54635785e-03, -1.52615672e-02,\n",
       "       -7.59165138e-02,  3.09144575e-02,  1.78442940e-01,  2.20118761e-02,\n",
       "       -4.12844978e-02, -1.28158033e-02,  9.27433670e-02, -3.40450332e-02,\n",
       "       -1.29166918e-04, -4.38280925e-02, -7.66991600e-02, -1.84899755e-02,\n",
       "       -2.79795397e-02,  6.14375919e-02, -8.92214701e-02,  9.68522578e-03,\n",
       "       -3.05231344e-02,  7.48403789e-03,  7.82644469e-03, -1.02526426e-01,\n",
       "       -1.66311953e-02, -4.38280925e-02, -6.80900663e-02,  3.81539166e-02,\n",
       "       -4.98324400e-04, -4.87196185e-02,  4.81815496e-03, -1.84899755e-02,\n",
       "       -5.59590794e-02,  9.34281852e-03, -1.43810920e-02, -8.64822119e-02,\n",
       "       -6.26115575e-02, -7.39599019e-02,  6.10462688e-02,  8.25689957e-02,\n",
       "        2.05444172e-02, -2.70012338e-02,  1.49680758e-02,  6.40790164e-03,\n",
       "        5.98723032e-02, -3.67842913e-02,  1.26201417e-02, -4.51977178e-02,\n",
       "        1.98009059e-01, -2.26966906e-02, -3.01318131e-02,  1.41854314e-02,\n",
       "       -2.13270616e-02, -6.88727126e-02, -4.48063947e-02, -1.18374974e-02,\n",
       "       -4.08442598e-03,  2.13270616e-02,  4.74478211e-03,  7.00466782e-02,\n",
       "        1.02526426e-01, -2.97404900e-02, -9.54826251e-02,  3.15014385e-02,\n",
       "       -7.00466782e-02,  1.95661113e-02, -2.42619794e-02,  5.28285019e-02,\n",
       "       -9.90045294e-02, -2.31858417e-02,  1.89791285e-02,  2.29901820e-02,\n",
       "        4.71543297e-02, -4.26541232e-02,  5.24371788e-02, -2.73925569e-02,\n",
       "        1.26201417e-02,  4.81326357e-02, -8.96127895e-02, -9.39173400e-02,\n",
       "       -3.34580503e-02, -6.02636263e-02,  7.39599019e-02, -3.05231344e-02,\n",
       "        1.43223941e-01, -4.95022647e-02, -5.67417257e-02,  4.98935841e-02,\n",
       "       -1.98596027e-02, -3.71756144e-02,  3.11101172e-02, -3.52190025e-02,\n",
       "        6.02636263e-02, -2.23053675e-02,  3.32623906e-03, -9.39173345e-03,\n",
       "        8.60908926e-02,  1.25223119e-02, -1.13483453e-02, -1.11918159e-01,\n",
       "       -5.90896569e-02, -2.40663178e-02,  5.08718891e-03,  3.54146622e-02,\n",
       "       -1.99329760e-03,  1.49680758e-02,  6.10462688e-02,  1.35006169e-02,\n",
       "        4.16758172e-02, -2.26966906e-02, -7.23946169e-02,  3.95235457e-02,\n",
       "        4.55890410e-02, -6.53508157e-02,  9.07867625e-02,  2.25988589e-02,\n",
       "       -7.78731257e-02, -9.29390360e-03,  3.24797444e-02,  2.21097060e-02,\n",
       "       -4.65673469e-02, -1.76095009e-01, -5.16545363e-02,  9.11780819e-02,\n",
       "        6.41768500e-02,  1.40876010e-01,  1.35397494e-01, -2.41641477e-02,\n",
       "       -3.09389154e-03,  4.69586700e-02,  1.57507192e-02, -2.33815033e-02,\n",
       "       -4.06975113e-02, -1.49485096e-01,  3.58059853e-02, -5.83070144e-02,\n",
       "       -4.96979244e-02,  6.55464735e-03,  5.47851138e-02, -1.01743778e-02,\n",
       "        1.92726199e-02,  1.84899755e-02,  5.90896569e-02,  5.75243682e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24041\n",
      "46741\n"
     ]
    }
   ],
   "source": [
    "print(len(vecs))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-2373d6704758>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"red\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "new_v[\"red\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata():\n",
    "    table = boto3.resource('dynamodb', region_name='us-west-1').Table('autolocal-documents')\n",
    "    s3_client = boto3.client('s3')\n",
    "    metadata = pd.DataFrame(table.scan()[\"Items\"])\n",
    "    metadata[\"date\"] = [datetime.strptime(d, '%Y-%m-%d') for d in metadata[\"date\"]]\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "casing = \"lower_non_acronyms\"\n",
    "# casing = \"lower_non_acronyms\"\n",
    "# TODO: is lowercasing necessary?\n",
    "\n",
    "def casing_function():\n",
    "    if casing==\"cased\":\n",
    "        return lambda x: x\n",
    "    elif casing==\"lower\":\n",
    "        return lambda x: x.lower()\n",
    "    elif casing==\"lower_non_acronyms\":\n",
    "        return lambda x: x if x.isupper() else x.lower()\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "preprocess_filters = [\n",
    "    casing_function(),\n",
    "    strip_punctuation,\n",
    "    strip_numeric,\n",
    "    strip_non_alphanum,\n",
    "    strip_multiple_whitespaces,\n",
    "#     strip_numeric,\n",
    "    remove_stopwords,\n",
    "#     strip_short\n",
    "]\n",
    "\n",
    "class DocTextReader():\n",
    "    def __init__(self, log_every=100):\n",
    "        self.log_every = log_every\n",
    "        s3 = boto3.resource('s3', region_name='us-west-1')\n",
    "        self.bucket = s3.Bucket('autolocal-documents')\n",
    "\n",
    "    def read_document_string(self, s3_path):\n",
    "        return self.bucket.Object(s3_path).get()['Body'].read()\n",
    "\n",
    "    def read_docs(self, s3_paths):\n",
    "        # read all documents that we know about\n",
    "        # tokenize each document\n",
    "        # return list of documents\n",
    "\n",
    "        documents = {}\n",
    "        n_docs_total = len(s3_paths)\n",
    "\n",
    "        i = 0\n",
    "        n_docs_read = 0\n",
    "        for s3_path in s3_paths:\n",
    "            try:\n",
    "                doc_string = self.read_document_string(s3_path)\n",
    "                doc_tokens = preprocess_string(doc_string, filters=preprocess_filters)\n",
    "                documents[s3_path] = {\n",
    "                    \"original_text\": doc_string,\n",
    "                    \"tokens\": doc_tokens\n",
    "                }\n",
    "            except Exception as e:\n",
    "                if i < 10:\n",
    "                    print(\"Key not found: {}\".format(s3_path))\n",
    "                elif i == 10:\n",
    "                    print(\"More than 10 keys not found\")\n",
    "                    print(e)\n",
    "                    break\n",
    "                i+=1\n",
    "            if n_docs_read % self.log_every == 0:\n",
    "                print(\"{} of {} documents read\".format(n_docs_read, n_docs_total))\n",
    "            n_docs_read+=1\n",
    "\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = DocTextReader()\n",
    "# all_docs = r.read_docs(read_metadata()[\"local_path_txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf = calculate_idf(all_docs)\n",
    "# len(idf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries(query_source):\n",
    "    if query_source == \"actual\":\n",
    "        table = boto3.resource('dynamodb', region_name='us-west-2').Table('autoLocalNews')\n",
    "    elif query_source == \"quick\":\n",
    "        table = boto3.resource('dynamodb', region_name='us-west-1').Table('quick_queries')\n",
    "    else:\n",
    "        raise Exception\n",
    "    queries = table.scan()[\"Items\"]\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_history():\n",
    "    try:\n",
    "        table = boto3.resource('dynamodb', region_name='us-west-1').Table('history')\n",
    "        history = table.scan()[\"Items\"]\n",
    "    except:\n",
    "        history = []\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cached_idf():\n",
    "    s3 = boto3.resource('s3', region_name='us-west-1')\n",
    "    bucket = s3.Bucket('autolocal-documents')\n",
    "    idf = json.load(bucket.Object('idf_{}.json'.format(casing)).get()['Body'])\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_idf(idf):\n",
    "    s3 = boto3.resource('s3')\n",
    "    object = s3.Object('autolocal-documents', 'idf_{}.json'.format(casing))\n",
    "    object.put(Body=json.dumps(idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf(all_docs): \n",
    "    # for each word, how many unique docs does it show up in?\n",
    "    from collections import Counter\n",
    "    \n",
    "    doc_freq = {}\n",
    "    for document in all_docs: \n",
    "        tokens = all_docs[document][\"tokens\"]\n",
    "        for token in tokens:\n",
    "            if token in doc_freq:\n",
    "                doc_freq[token] += 1\n",
    "            else:\n",
    "                doc_freq[token] = 1\n",
    "    \n",
    "    inverse_doc_freq = {}\n",
    "    for word in doc_freq:\n",
    "        inverse_doc_freq[word] = 1./doc_freq[word]\n",
    "    \n",
    "    return inverse_doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_windows = {\n",
    "    'upcoming_only': datetime.now() + timedelta(days=0.5),\n",
    "    'this_week': datetime.now() - timedelta(weeks=1),\n",
    "    'this_year': datetime.now() - timedelta(days=365),\n",
    "    'past_six_months':datetime.now() - timedelta(days=183),\n",
    "    'all': None\n",
    "}\n",
    "\n",
    "def find_relevant_filenames(queries, metadata): \n",
    "    \n",
    "    # filter metadata to only those files that match the query municipality and time_window\n",
    "    municipalities_by_time_window = {}\n",
    "    for query in queries:\n",
    "        time_window = query['Time Window']\n",
    "        if time_window in municipalities_by_time_window:\n",
    "            municipalities_by_time_window[time_window].update(query['Municipalities'])\n",
    "        else:\n",
    "            municipalities_by_time_window[time_window] = set(query['Municipalities'])\n",
    "            \n",
    "    relevant_filenames = set()\n",
    "    for time_window in municipalities_by_time_window:\n",
    "        starting_date = time_windows[time_window]\n",
    "        potential_documents = metadata\n",
    "        if starting_date:\n",
    "            potential_documents = potential_documents[potential_documents[\"date\"] >= starting_date]\n",
    "        cities = municipalities_by_time_window[time_window]\n",
    "        potential_documents = potential_documents[[(c in cities) for c in potential_documents[\"city\"]]]\n",
    "        relevant_filenames.update(potential_documents['local_path_txt'])\n",
    "    return relevant_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_docs(municipalities, time_window, all_docs, metadata):\n",
    "    # filter metadata to only those files that match the query municipality and time_window\n",
    "    starting_date = time_windows[time_window]\n",
    "    potential_documents = metadata\n",
    "    if starting_date:\n",
    "        potential_documents = potential_documents[potential_documents[\"date\"] >= starting_date]\n",
    "    potential_documents = potential_documents[[(c in municipalities) for c in potential_documents[\"city\"]]]\n",
    "    # filter all docs to only filenames in subset of metadata\n",
    "    return [{**all_docs[f], 'filename':f} for f in potential_documents['local_path_txt'] if f in all_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Play with section length\n",
    "# TODO: smart sectioning that's sensitive to multiple line breaks and other section break signals\n",
    "# TODO: extract sections that overlap with each other\n",
    "\n",
    "## for each starting line, keep going till we have >=50 tokens\n",
    "def segment_docs(relevant_docs):\n",
    "    doc_sections = []\n",
    "    approx_section_length = 50 # tokens\n",
    "    min_section_length = 5\n",
    "    \n",
    "    for doc in relevant_docs:\n",
    "        doc_tokens = doc[\"tokens\"]\n",
    "        original_text = doc[\"original_text\"].decode('utf-8')\n",
    "        filename = doc[\"filename\"]\n",
    "        \n",
    "        doc_section_lines = []\n",
    "        doc_section_tokens = []\n",
    "        starting_page = 0\n",
    "        starting_line = 0\n",
    "        pages = original_text.split('\\f')\n",
    "        for p, page in enumerate(pages):\n",
    "            lines = page.split('\\n')\n",
    "            for lnum, line in enumerate(lines):\n",
    "                line_tokens = preprocess_string(line, filters=preprocess_filters)\n",
    "                doc_section_tokens += line_tokens\n",
    "                doc_section_lines.append(line)\n",
    "                if len(doc_section_tokens) >= approx_section_length:\n",
    "                    doc_sections.append({\n",
    "                        **doc,\n",
    "                        'starting_page': starting_page,\n",
    "                        'starting_line': starting_line,\n",
    "                        'ending_page': p,\n",
    "                        'ending_line': lnum,\n",
    "                        'section_text': '\\n'.join(doc_section_lines),\n",
    "                        'section_tokens': doc_section_tokens\n",
    "                    })\n",
    "                    doc_section_lines = []\n",
    "                    doc_section_tokens = []\n",
    "                    # have we reached the last line of this page?\n",
    "                    if lnum == (len(lines)-1):\n",
    "                        # next section starts at top of next page\n",
    "                        starting_page = p+1\n",
    "                        starting_line = 0\n",
    "                    else:\n",
    "                        # next section starts on next line of this page\n",
    "                        starting_page = p\n",
    "                        starting_line = lnum+1\n",
    "        # end of the document\n",
    "        if len(doc_section_tokens) >= min_section_length:\n",
    "            doc_sections.append({\n",
    "                **doc,\n",
    "                'starting_page': starting_page,\n",
    "                'starting_line': starting_line,\n",
    "                'ending_page': p,\n",
    "                'ending_line': lnum,\n",
    "                'section_text': '\\n'.join(doc_section_lines),\n",
    "                'section_tokens': doc_section_tokens\n",
    "            })\n",
    "            \n",
    "    return doc_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Play with section length\n",
    "# # TODO: smart sectioning that's sensitive to multiple line breaks and other section break signals\n",
    "# # TODO: extract sections that overlap with each other\n",
    "# def segment_docs(relevant_docs):\n",
    "#     doc_sections = []\n",
    "#     approx_section_length = 50 # tokens\n",
    "#     min_section_length = 5\n",
    "    \n",
    "#     for doc in relevant_docs:\n",
    "#         doc_tokens = doc[\"tokens\"]\n",
    "#         original_text = doc[\"original_text\"].decode('utf-8')\n",
    "#         filename = doc[\"filename\"]\n",
    "        \n",
    "#         doc_section_lines = []\n",
    "#         doc_section_tokens = []\n",
    "#         starting_page = 0\n",
    "#         starting_line = 0\n",
    "#         pages = original_text.split('\\f')\n",
    "#         for p, page in enumerate(pages):\n",
    "#             lines = page.split('\\n')\n",
    "#             for lnum, line in enumerate(lines):\n",
    "#                 line_tokens = preprocess_string(line, filters=preprocess_filters)\n",
    "#                 doc_section_tokens += line_tokens\n",
    "#                 doc_section_lines.append(line)\n",
    "#                 if len(doc_section_tokens) >= approx_section_length:\n",
    "#                     doc_sections.append({\n",
    "#                         **doc,\n",
    "#                         'starting_page': starting_page,\n",
    "#                         'starting_line': starting_line,\n",
    "#                         'ending_page': p,\n",
    "#                         'ending_line': lnum,\n",
    "#                         'section_text': '\\n'.join(doc_section_lines),\n",
    "#                         'section_tokens': doc_section_tokens\n",
    "#                     })\n",
    "#                     doc_section_lines = []\n",
    "#                     doc_section_tokens = []\n",
    "#                     # have we reached the last line of this page?\n",
    "#                     if lnum == (len(lines)-1):\n",
    "#                         # next section starts at top of next page\n",
    "#                         starting_page = p+1\n",
    "#                         starting_line = 0\n",
    "#                     else:\n",
    "#                         # next section starts on next line of this page\n",
    "#                         starting_page = p\n",
    "#                         starting_line = lnum+1\n",
    "#         # end of the document\n",
    "#         if len(doc_section_tokens) >= min_section_length:\n",
    "#             doc_sections.append({\n",
    "#                 **doc,\n",
    "#                 'starting_page': starting_page,\n",
    "#                 'starting_line': starting_line,\n",
    "#                 'ending_page': p,\n",
    "#                 'ending_line': lnum,\n",
    "#                 'section_text': '\\n'.join(doc_section_lines),\n",
    "#                 'section_tokens': doc_section_tokens\n",
    "#             })\n",
    "            \n",
    "#     return doc_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use vectors to find closes words to keywords\n",
    "# TODO: why do shorter documents get higher scores?\n",
    "def score_doc_sections(doc_sections, keywords, idf, use_idf_for_doc_tokens=False, threshold_similarity=-1):\n",
    "    # vectorize etc.\n",
    "    # only consider keywords that have idf weights\n",
    "    keywords = [keyword for keyword in keywords if (keyword in idf and keyword in vectors)]\n",
    "    keyword_vectors = np.array([vectors[keyword] for keyword in keywords])\n",
    "    keyword_weights = np.array([idf[keyword] for keyword in keywords])\n",
    "    doc_sections_scores = []\n",
    "    for s, section in enumerate(doc_sections):\n",
    "        score = None\n",
    "        section_tokens = section[\"section_tokens\"]\n",
    "        # TODO: Zipf to figure out what the cutoff should be for normal communication\n",
    "        # If the number of unique tokens in the section is too small, it's probably not an interesting section\n",
    "        if len(set(section_tokens))<20:\n",
    "            score = 0\n",
    "        else:\n",
    "            section_vectors = np.array([vectors[t] for t in section_tokens if (t in idf and t in vectors)])\n",
    "            if section_vectors.shape[0]>0:\n",
    "                similarities = cosine_similarity(section_vectors, keyword_vectors)\n",
    "                similarities = similarities*(similarities>threshold_similarity)\n",
    "                if use_idf_for_doc_tokens:\n",
    "                    section_weights = np.array([idf[t] for t in section_tokens if (t in idf and t in vectors)])\n",
    "                    keyword_similarities = np.average(similarities, axis=0, weights=section_weights)\n",
    "                else:\n",
    "                    keyword_similarities = np.mean(similarities, axis=0)\n",
    "                score = np.sum(keyword_similarities*keyword_weights)\n",
    "        doc_sections_scores.append(score)\n",
    "\n",
    "    return doc_sections_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_is_too_similar(a, b):\n",
    "    # if there are only 2 edits to get from one text to the other, it's not good\n",
    "    return editdistance.eval(a, b) < 20\n",
    "\n",
    "# make sure we're not giving similar text among the top k (e.g. shows up on both minutes and agenda)\n",
    "def check_repeated_text(top_k, section_text):\n",
    "    for old in top_k:\n",
    "        if text_is_too_similar(old[1][\"section_text\"], section_text):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def check_overlap(top_k, filename, starting_page, starting_line, ending_page, ending_line):\n",
    "    for old in top_k:\n",
    "        old_data = old[1]\n",
    "        if old_data[\"filename\"] == filename:\n",
    "            if old_data[\"startin_page\"] == starting_page:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def select_top_k(doc_sections, doc_sections_scores, k, user_history):\n",
    "    sorted_sections = sorted(zip(doc_sections_scores, doc_sections), key=lambda pair: pair[0], reverse=True)\n",
    "    top_k = []\n",
    "    text_returned = []\n",
    "    for x in sorted_sections:\n",
    "        filename = x[1][\"filename\"]\n",
    "        starting_page = x[1][\"starting_page\"]\n",
    "        starting_line = x[1][\"starting_line\"]\n",
    "        ending_page = x[1][\"ending_page\"]\n",
    "        ending_line = x[1][\"ending_line\"]\n",
    "        section_text = x[1][\"section_text\"]\n",
    "        if check_overlap(top_k, filename, starting_page, starting_line, ending_page, ending_line):\n",
    "            print(\"this user has already seen a story on this page\")\n",
    "        elif check_repeated_text(top_k, section_text):\n",
    "            print(\"this excpert has already been returned\")\n",
    "        else:\n",
    "            top_k.append(x)\n",
    "        if len(top_k) >= k:\n",
    "            break\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_with_top_k(history, top_k_sections, query):\n",
    "    for section in top_k_sections:\n",
    "        x = section[1]\n",
    "        x.update(query)\n",
    "        history.append(x)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row2item(row):\n",
    "    row = dict(row)\n",
    "    item = {}\n",
    "    for k,v in row.items():\n",
    "        if k in ['local_path_pdf', 'local_path_txt']:\n",
    "            v = v[8:]\n",
    "        if isinstance(v, str):\n",
    "            item[k] = v\n",
    "        elif np.isnan(v):\n",
    "            pass\n",
    "    return item\n",
    "\n",
    "def write_history(history):\n",
    "    try:\n",
    "        table = boto3.resource('dynamodb', region_name='us-west-1').Table('history')\n",
    "        table.scan()\n",
    "    except:\n",
    "        table_args = {\n",
    "            'TableName': 'autolocal-documents',\n",
    "            'KeySchema': [\n",
    "                {\n",
    "                    'AttributeName': 'doc_id',\n",
    "                    'KeyType': 'HASH'\n",
    "                }\n",
    "            ],\n",
    "            'AttributeDefinitions': [\n",
    "                {\n",
    "                    'AttributeName': 'doc_id',\n",
    "                    'AttributeType': 'S'\n",
    "                }        \n",
    "            ],\n",
    "            'ProvisionedThroughput':\n",
    "            {\n",
    "                'ReadCapacityUnits': 5,\n",
    "                'WriteCapacityUnits': 5\n",
    "            }\n",
    "        }\n",
    "        table = dynamodb.create_table(**table_args)\n",
    "    with table.batch_writer() as batch:\n",
    "        for i, row in tqdm(history.iterrows()):\n",
    "            item = row2item(row)\n",
    "            batch.put_item(Item=item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_emails(results): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading language model\n",
      "model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erindb/miniconda3/envs/ecj/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings loaded \n",
      "loading docs ... \n"
     ]
    }
   ],
   "source": [
    "# TODO: contextual vectors\n",
    "vectors = setup_word_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only send emails to one user\n",
    "# send immediately and then wait a certain amount of time for that user for that query\n",
    "\n",
    "idf = read_cached_idf()\n",
    "vocab = [v for v in idf if v in vectors]\n",
    "vecs = [vectors[v] for v in idf if v in vectors]\n",
    "gensim_data = {vocab[i]: vecs[i] for i in range(len(vocab))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( gensim_data, open( \"gensim_data.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pickle.load(open(\"gensim_data.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.89365628e-02, -3.40450332e-02,  1.08787581e-01,  8.88301507e-02,\n",
       "       -1.92726199e-02, -7.00466782e-02, -3.79582569e-02, -1.51050389e-01,\n",
       "       -1.48702450e-02,  6.37855232e-02,  3.61973071e-03,  6.21224055e-03,\n",
       "       -9.73414071e-03, -3.81539166e-02, -1.40876010e-01, -8.33516344e-02,\n",
       "       -1.18179314e-01,  6.73074275e-02, -1.11918159e-01, -1.29918978e-01,\n",
       "        3.79582569e-02,  2.43598092e-02, -2.50446238e-02, -1.18179314e-01,\n",
       "        1.33049563e-02, -8.85366555e-03,  4.77413125e-02,  6.26115594e-03,\n",
       "        9.93958488e-02,  1.29136341e-02,  4.61760238e-02,  5.67417257e-02,\n",
       "        7.98297375e-02, -4.51977178e-02, -3.32623906e-03,  1.47724142e-02,\n",
       "        1.62398722e-02, -1.23266503e-02, -5.28285019e-02, -1.40876006e-02,\n",
       "        3.05231344e-02, -3.07187960e-02,  6.35898625e-03, -5.40024675e-02,\n",
       "        7.63078332e-02, -5.43937907e-02,  1.90769583e-02, -6.76987469e-02,\n",
       "        3.52190025e-02,  5.28285019e-02, -8.45256001e-02,  5.55677563e-02,\n",
       "        3.56103219e-02, -2.27945205e-02, -6.73074275e-02,  1.45767536e-02,\n",
       "       -7.04380032e-03, -2.91535072e-02,  5.23393508e-03,  1.26201417e-02,\n",
       "       -6.69161007e-02, -9.44064930e-03, -6.76987469e-02, -2.87621841e-02,\n",
       "       -3.87409031e-02,  7.55251944e-02, -4.53933813e-02,  1.11893704e-03,\n",
       "        6.05326612e-04,  5.55677563e-02,  1.00961134e-01,  5.36111481e-02,\n",
       "        7.70904794e-02, -7.90470913e-02,  1.29136341e-02,  7.45958008e-04,\n",
       "        1.04091719e-01,  7.23946169e-02, -7.82644451e-02,  1.40876010e-01,\n",
       "        5.79156913e-02, -1.00961134e-01, -4.79369750e-03, -5.79156913e-02,\n",
       "        1.73160098e-02, -1.41242868e-03, -9.90045294e-02,  3.22840847e-02,\n",
       "        4.73499894e-02, -1.06146159e-02,  1.16907516e-02,  4.38280925e-02,\n",
       "       -1.71203481e-03,  6.37855232e-02, -4.98935841e-02,  7.48403789e-03,\n",
       "        3.09144575e-02,  3.07187960e-02,  7.53295328e-03,  6.88727126e-02,\n",
       "       -5.60079934e-03, -2.00552642e-02,  5.04805669e-02, -3.04497615e-03,\n",
       "       -6.99488493e-03, -7.31772557e-02,  1.44006580e-01,  6.70139352e-03,\n",
       "       -3.93278860e-02, -2.06422489e-02, -3.15503543e-03, -1.16614029e-01,\n",
       "        2.18895869e-03,  7.90470913e-02,  9.07867625e-02,  1.09570231e-02,\n",
       "        3.67842913e-02,  6.26115575e-02, -1.89791285e-02,  9.93958488e-02,\n",
       "       -1.72181781e-02, -4.89152782e-02, -3.77625972e-02, -3.60016450e-02,\n",
       "        7.04380032e-03,  1.49680758e-02,  9.63630993e-03,  4.96979244e-02,\n",
       "        9.03954357e-02, -2.20118761e-02, -2.11314000e-02, -4.36324291e-02,\n",
       "        1.21309897e-02, -4.63716835e-02,  3.47298477e-03,  3.69799510e-02,\n",
       "       -1.88812986e-02,  2.10335702e-02,  4.46107350e-02, -1.52615672e-02,\n",
       "       -3.83495800e-02, -6.18289150e-02, -1.77073311e-02, -1.10548530e-02,\n",
       "        9.58739519e-02,  7.31772557e-02,  6.84813922e-03, -2.41641477e-02,\n",
       "       -1.50659066e-02, -6.33942038e-02,  1.60442125e-02,  1.49680758e-02,\n",
       "       -7.43512288e-02,  3.44363563e-02, -3.07187960e-02,  1.79029927e-02,\n",
       "       -4.67630066e-02, -1.05657004e-01,  3.54635785e-03, -1.52615672e-02,\n",
       "       -7.59165138e-02,  3.09144575e-02,  1.78442940e-01,  2.20118761e-02,\n",
       "       -4.12844978e-02, -1.28158033e-02,  9.27433670e-02, -3.40450332e-02,\n",
       "       -1.29166918e-04, -4.38280925e-02, -7.66991600e-02, -1.84899755e-02,\n",
       "       -2.79795397e-02,  6.14375919e-02, -8.92214701e-02,  9.68522578e-03,\n",
       "       -3.05231344e-02,  7.48403789e-03,  7.82644469e-03, -1.02526426e-01,\n",
       "       -1.66311953e-02, -4.38280925e-02, -6.80900663e-02,  3.81539166e-02,\n",
       "       -4.98324400e-04, -4.87196185e-02,  4.81815496e-03, -1.84899755e-02,\n",
       "       -5.59590794e-02,  9.34281852e-03, -1.43810920e-02, -8.64822119e-02,\n",
       "       -6.26115575e-02, -7.39599019e-02,  6.10462688e-02,  8.25689957e-02,\n",
       "        2.05444172e-02, -2.70012338e-02,  1.49680758e-02,  6.40790164e-03,\n",
       "        5.98723032e-02, -3.67842913e-02,  1.26201417e-02, -4.51977178e-02,\n",
       "        1.98009059e-01, -2.26966906e-02, -3.01318131e-02,  1.41854314e-02,\n",
       "       -2.13270616e-02, -6.88727126e-02, -4.48063947e-02, -1.18374974e-02,\n",
       "       -4.08442598e-03,  2.13270616e-02,  4.74478211e-03,  7.00466782e-02,\n",
       "        1.02526426e-01, -2.97404900e-02, -9.54826251e-02,  3.15014385e-02,\n",
       "       -7.00466782e-02,  1.95661113e-02, -2.42619794e-02,  5.28285019e-02,\n",
       "       -9.90045294e-02, -2.31858417e-02,  1.89791285e-02,  2.29901820e-02,\n",
       "        4.71543297e-02, -4.26541232e-02,  5.24371788e-02, -2.73925569e-02,\n",
       "        1.26201417e-02,  4.81326357e-02, -8.96127895e-02, -9.39173400e-02,\n",
       "       -3.34580503e-02, -6.02636263e-02,  7.39599019e-02, -3.05231344e-02,\n",
       "        1.43223941e-01, -4.95022647e-02, -5.67417257e-02,  4.98935841e-02,\n",
       "       -1.98596027e-02, -3.71756144e-02,  3.11101172e-02, -3.52190025e-02,\n",
       "        6.02636263e-02, -2.23053675e-02,  3.32623906e-03, -9.39173345e-03,\n",
       "        8.60908926e-02,  1.25223119e-02, -1.13483453e-02, -1.11918159e-01,\n",
       "       -5.90896569e-02, -2.40663178e-02,  5.08718891e-03,  3.54146622e-02,\n",
       "       -1.99329760e-03,  1.49680758e-02,  6.10462688e-02,  1.35006169e-02,\n",
       "        4.16758172e-02, -2.26966906e-02, -7.23946169e-02,  3.95235457e-02,\n",
       "        4.55890410e-02, -6.53508157e-02,  9.07867625e-02,  2.25988589e-02,\n",
       "       -7.78731257e-02, -9.29390360e-03,  3.24797444e-02,  2.21097060e-02,\n",
       "       -4.65673469e-02, -1.76095009e-01, -5.16545363e-02,  9.11780819e-02,\n",
       "        6.41768500e-02,  1.40876010e-01,  1.35397494e-01, -2.41641477e-02,\n",
       "       -3.09389154e-03,  4.69586700e-02,  1.57507192e-02, -2.33815033e-02,\n",
       "       -4.06975113e-02, -1.49485096e-01,  3.58059853e-02, -5.83070144e-02,\n",
       "       -4.96979244e-02,  6.55464735e-03,  5.47851138e-02, -1.01743778e-02,\n",
       "        1.92726199e-02,  1.84899755e-02,  5.90896569e-02,  5.75243682e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g['red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24041\n",
      "24041\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print(len(vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00435863,  0.02483096,  0.07945908, ..., -0.05304316,\n",
       "         0.03296706, -0.10143712],\n",
       "       [ 0.01141298,  0.04934435, -0.00885345, ..., -0.02056014,\n",
       "         0.01669987, -0.03222488],\n",
       "       [ 0.05176774,  0.07251675, -0.00623518, ..., -0.08048102,\n",
       "         0.04841436,  0.07084007],\n",
       "       ...,\n",
       "       [-0.0788236 , -0.11175675, -0.100959  , ..., -0.06910562,\n",
       "         0.00884066,  0.01957093],\n",
       "       [ 0.02662049, -0.03513905, -0.08412076, ..., -0.07506979,\n",
       "         0.04658586, -0.0005948 ],\n",
       "       [-0.03108003, -0.0027604 ,  0.05351374, ...,  0.00365132,\n",
       "         0.0055208 ,  0.01454686]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"gensim.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"gensim.npy\", np.array(vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379100"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"vocab.txt\", \"w\").write(\"\\n\".join(list(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading queries\n",
      "reading metadata\n",
      "setting up reader\n",
      "loading cached idf\n",
      "finding relevant filenames\n",
      "reading relevant documents\n",
      "0 of 34 documents read\n",
      "reading history\n",
      "running query 0 of 2\n",
      "user id: emily\n",
      "['affordable', 'housing', 'ADU', 'vote', 'residential', 'dwelling', 'residences', 'homelessness', 'family', 'homes', 'homeless']\n",
      "segmenting documents\n",
      "scoring documents\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'startin_page'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3bea7d43933d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mquery_source\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"quick\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0muse_idf_for_doc_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# this makes a difference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mthreshold_similarity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;31m# this doesn't make much of a difference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-22-3bea7d43933d>\u001b[0m in \u001b[0;36mrun_queries\u001b[0;34m(use_cached_idf, query_source, k, use_idf_for_doc_tokens, threshold_similarity)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0muse_idf_for_doc_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_idf_for_doc_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             threshold_similarity=threshold_similarity)\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtop_k_sections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_top_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_sections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_sections_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~~~~\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~~~~\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-a19b86d785de>\u001b[0m in \u001b[0;36mselect_top_k\u001b[0;34m(doc_sections, doc_sections_scores, k, user_history)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mending_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ending_line\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0msection_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"section_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_overlap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarting_page\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarting_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mending_page\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mending_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"this user has already seen a story on this page\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcheck_repeated_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msection_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-a19b86d785de>\u001b[0m in \u001b[0;36mcheck_overlap\u001b[0;34m(top_k, filename, starting_page, starting_line, ending_page, ending_line)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mold_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mold_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"filename\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mold_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"startin_page\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstarting_page\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'startin_page'"
     ]
    }
   ],
   "source": [
    "# vectors = setup_word_vectors()\n",
    "def run_queries(use_cached_idf = False, query_source=\"actual\", k=3, use_idf_for_doc_tokens=False, threshold_similarity=-1): \n",
    "    print(\"reading queries\")\n",
    "    queries = read_queries(query_source)\n",
    "    print(\"reading metadata\")\n",
    "    metadata = read_metadata()\n",
    "    print(\"setting up reader\")\n",
    "    doc_text_reader = DocTextReader(log_every=100)\n",
    "    if use_cached_idf:\n",
    "        # used cached idf and only read relevant documents\n",
    "        print(\"loading cached idf\")\n",
    "        idf = read_cached_idf()\n",
    "        print(\"finding relevant filenames\")\n",
    "        relevant_filenames = find_relevant_filenames(queries, metadata)\n",
    "        # (not actually *all*, but all the ones we care about for queries)\n",
    "        print(\"reading relevant documents\")\n",
    "        all_docs = doc_text_reader.read_docs(relevant_filenames)\n",
    "    else:\n",
    "        # read all documents and calculate inverse document frequency\n",
    "        all_filenames = metadata[\"local_path_txt\"]\n",
    "        print(\"reading all documents\")\n",
    "        all_docs = doc_text_reader.read_docs(all_filenames)\n",
    "        print(\"calculating idf\")\n",
    "        idf = calculate_idf(all_docs)\n",
    "        cache_idf(idf)\n",
    "    print(\"reading history\")\n",
    "    history = read_history()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for q, query in enumerate(queries):\n",
    "        print(\"running query {} of {}\".format(q, len(queries)))\n",
    "        user_id = query[\"id\"]\n",
    "        print(\"user id: {}\".format(user_id))\n",
    "        user_history = [x for x in history if x['id'] == user_id]\n",
    "        keywords = query[\"Keywords\"]\n",
    "        print(keywords)\n",
    "        time_window = query[\"Time Window\"]\n",
    "        municipalities = query[\"Municipalities\"]\n",
    "        relevant_docs = select_relevant_docs(municipalities, time_window, all_docs, metadata)\n",
    "        print(\"segmenting documents\")\n",
    "        doc_sections = segment_docs(relevant_docs)\n",
    "        print(\"scoring documents\")\n",
    "        doc_sections_scores = score_doc_sections(\n",
    "            doc_sections,\n",
    "            keywords, idf,\n",
    "            use_idf_for_doc_tokens=use_idf_for_doc_tokens,\n",
    "            threshold_similarity=threshold_similarity)\n",
    "        top_k_sections = select_top_k(doc_sections, doc_sections_scores, k, user_history)\n",
    "        print(\"~~~~\")\n",
    "        print(\"~~~~\")\n",
    "        print(re.sub(\"(\\n(\\n)+)\", \"\\n\\n\",\n",
    "                     re.sub(\"\\u2022\", \"\", top_k_sections[0][1][\"section_text\"])))\n",
    "        print(\"====\")\n",
    "        print(\"====\")\n",
    "        print(\"~~~~\")\n",
    "        print(\"~~~~\")\n",
    "        print(re.sub(\"(\\n(\\n)+)\", \"\\n\\n\",\n",
    "                     re.sub(\"\\u2022\", \"\", top_k_sections[1][1][\"section_text\"])))\n",
    "        print(\"====\")\n",
    "        print(\"====\")\n",
    "        results = update_with_top_k(results, top_k_sections, query)\n",
    "        history = update_with_top_k(history, top_k_sections, query)\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "#     print(\"sending emails\")\n",
    "#     send_emails(results)\n",
    "#     write_history(history)\n",
    "    print(\"finished\")\n",
    "\n",
    "\n",
    "run_queries(\n",
    "    use_cached_idf=True,\n",
    "    query_source=\"quick\",\n",
    "    use_idf_for_doc_tokens=False, # this makes a difference\n",
    "    threshold_similarity=0 # this doesn't make much of a difference\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
