{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "from autolocal.nlp import Tokenizer\n",
    "from gensim.parsing.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading language model\n",
      "model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/autolocal/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings loaded \n",
      "loading docs ... \n"
     ]
    }
   ],
   "source": [
    "# set up word vectors\n",
    "# (this takes a loooong time)\n",
    "\n",
    "print(\"loading language model\")\n",
    "# load language model (this takes a few minutes)\n",
    "model = api.load('word2vec-google-news-300')\n",
    "print(\"model loaded\")\n",
    "\n",
    "vectors = model.wv\n",
    "del model\n",
    "vectors.init_sims(True) # normalize the vectors (!), so we can use the dot product as similarity measure\n",
    "\n",
    "print('embeddings loaded ')\n",
    "print('loading docs ... ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt_name(f):\n",
    "    path_parts = os.path.split(f)\n",
    "    fname = path_parts[1]\n",
    "    local_dir = os.path.basename(path_parts[0])\n",
    "    return os.path.join(local_dir, fname)[:-4]+\".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt_file</th>\n",
       "      <th>city</th>\n",
       "      <th>committee</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>cupertino/Cupertino_2019-10-29_Legislative-Rev...</td>\n",
       "      <td>Cupertino</td>\n",
       "      <td>Legislative Review Committee</td>\n",
       "      <td>2019-10-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>cupertino/Cupertino_2019-10-28_Planning-Commis...</td>\n",
       "      <td>Cupertino</td>\n",
       "      <td>Planning Commission</td>\n",
       "      <td>2019-10-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>cupertino/Cupertino_2019-10-24_Administrative-...</td>\n",
       "      <td>Cupertino</td>\n",
       "      <td>Administrative Hearing</td>\n",
       "      <td>2019-10-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>cupertino/Cupertino_2019-10-24_Sustainability-...</td>\n",
       "      <td>Cupertino</td>\n",
       "      <td>Sustainability Commission</td>\n",
       "      <td>2019-10-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>cupertino/Cupertino_2019-10-23_Teen-Commission...</td>\n",
       "      <td>Cupertino</td>\n",
       "      <td>Teen Commission</td>\n",
       "      <td>2019-10-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17204</td>\n",
       "      <td>burlingame/Burlingame_2019-09-05_Beautificatio...</td>\n",
       "      <td>Burlingame</td>\n",
       "      <td>Beautification Commission</td>\n",
       "      <td>2019-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17205</td>\n",
       "      <td>burlingame/Burlingame_2019-09-03_City-Council_...</td>\n",
       "      <td>Burlingame</td>\n",
       "      <td>City Council</td>\n",
       "      <td>2019-09-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17206</td>\n",
       "      <td>burlingame/Burlingame_2019-08-26_Planning-Comm...</td>\n",
       "      <td>Burlingame</td>\n",
       "      <td>Planning Commission</td>\n",
       "      <td>2019-08-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17207</td>\n",
       "      <td>burlingame/Burlingame_2019-08-20_Library-Board...</td>\n",
       "      <td>Burlingame</td>\n",
       "      <td>Library Board of Trustees</td>\n",
       "      <td>2019-08-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17208</td>\n",
       "      <td>burlingame/Burlingame_2019-08-19_City-Council_...</td>\n",
       "      <td>Burlingame</td>\n",
       "      <td>City Council</td>\n",
       "      <td>2019-08-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17209 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                txt_file        city  \\\n",
       "0      cupertino/Cupertino_2019-10-29_Legislative-Rev...   Cupertino   \n",
       "1      cupertino/Cupertino_2019-10-28_Planning-Commis...   Cupertino   \n",
       "2      cupertino/Cupertino_2019-10-24_Administrative-...   Cupertino   \n",
       "3      cupertino/Cupertino_2019-10-24_Sustainability-...   Cupertino   \n",
       "4      cupertino/Cupertino_2019-10-23_Teen-Commission...   Cupertino   \n",
       "...                                                  ...         ...   \n",
       "17204  burlingame/Burlingame_2019-09-05_Beautificatio...  Burlingame   \n",
       "17205  burlingame/Burlingame_2019-09-03_City-Council_...  Burlingame   \n",
       "17206  burlingame/Burlingame_2019-08-26_Planning-Comm...  Burlingame   \n",
       "17207  burlingame/Burlingame_2019-08-20_Library-Board...  Burlingame   \n",
       "17208  burlingame/Burlingame_2019-08-19_City-Council_...  Burlingame   \n",
       "\n",
       "                          committee       date  \n",
       "0      Legislative Review Committee 2019-10-29  \n",
       "1               Planning Commission 2019-10-28  \n",
       "2            Administrative Hearing 2019-10-24  \n",
       "3         Sustainability Commission 2019-10-24  \n",
       "4                   Teen Commission 2019-10-23  \n",
       "...                             ...        ...  \n",
       "17204     Beautification Commission 2019-09-05  \n",
       "17205                  City Council 2019-09-03  \n",
       "17206           Planning Commission 2019-08-26  \n",
       "17207     Library Board of Trustees 2019-08-20  \n",
       "17208                  City Council 2019-08-19  \n",
       "\n",
       "[17209 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_metadata():\n",
    "    # return data frame with metadata\n",
    "    metadata = pd.read_csv(\"../data/index_sfbay_small/metadata.csv\")\n",
    "    metadata[\"txt_file\"] = [get_txt_name(f) for f in metadata[\"local_path_pdf\"]]\n",
    "    metadata = metadata[[\"txt_file\", \"city\", \"committee\", \"date\"]]\n",
    "    metadata[\"date\"] = [datetime.strptime(d, '%Y-%m-%d') for d in metadata[\"date\"]]\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_id': 'emily',\n",
       "  'query_id': 1,\n",
       "  'keywords': ['affordable',\n",
       "   'housing',\n",
       "   'ADU',\n",
       "   'vote',\n",
       "   'residential',\n",
       "   'homeless'],\n",
       "  'municipalities': ['San Jose',\n",
       "   'Cupertino',\n",
       "   'Sunnyvale',\n",
       "   'Palo Alto',\n",
       "   'Mountain View'],\n",
       "  'time_window': 'upcoming_only'},\n",
       " {'user_id': 'some_other_journalist',\n",
       "  'query_id': 2,\n",
       "  'keywords': ['affordable',\n",
       "   'housing',\n",
       "   'ADU',\n",
       "   'vote',\n",
       "   'residential',\n",
       "   'homeless'],\n",
       "  'time_window': 'upcoming_only',\n",
       "  'municipalities': ['Biggs', 'Gridley']}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_queries():\n",
    "    # read from input file\n",
    "    # parse as json\n",
    "    # return list of queries\n",
    "    with open('../data/user_data/sample_input_data.json') as json_file:\n",
    "        queries = json.load(json_file)\n",
    "    return queries\n",
    "\n",
    "read_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': [{'filename': 'filename1', 'segment': 'segment1'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_history(): \n",
    "    with open('../data/user_data/sample_history.json') as json_file:\n",
    "        history = json.load(json_file)\n",
    "    return history\n",
    "read_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# TODO: is lowercasing necessary?\n",
    "preprocess_filters = [\n",
    "    lambda x: x.lower(),\n",
    "    strip_punctuation,\n",
    "    strip_numeric,\n",
    "    strip_non_alphanum,\n",
    "    strip_multiple_whitespaces,\n",
    "    strip_numeric,\n",
    "    remove_stopwords,\n",
    "    strip_short\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(metadata):\n",
    "    # read all documents that we know about\n",
    "    # tokenize each document\n",
    "    # return list of documents\n",
    "    \n",
    "\n",
    "    documents = []\n",
    "\n",
    "    document_files = metadata[\"txt_file\"]\n",
    "    directory = '../data/docs'\n",
    "    for filename in document_files:\n",
    "        try:\n",
    "            f = open(os.path.join(directory, filename))\n",
    "            \n",
    "            doc_string = f.read()\n",
    "            doc_tokens = preprocess_string(doc_string, filters=preprocess_filters)\n",
    "            \n",
    "            documents.append({\"original_text\": doc_string,\n",
    "                              \"tokens\": doc_tokens,\n",
    "                              \"filename\": filename})\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cached_idf(): \n",
    "    # read cached idf file and conver to json\n",
    "    \n",
    "read_cached_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_results = [\n",
    "  {\n",
    "    \"user_id\": \"emily\",\n",
    "   'special',\n",
    "   'meeting',\n",
    "   'subject',\n",
    "   'approve',\n",
    "   'august',\n",
    "    \"document_sections\": [\n",
    "      {\n",
    "        \"section_id\": \"000\",\n",
    "        \"doc_url\": \"https://sanjose.legistar.com/View.ashx?M=A&ID=709096&GUID=CF2165D1-28CB-4670-AB54-F35B649DE71D\",\n",
    "        \"doc_name\": \"Agenda 2019-09-24\",\n",
    "        \"user_id\": \"emily\",\n",
    "        \"page_number\": \"11\",\n",
    "        \"keywords\": [\"High-Rise\", \"Incentive\", \"Affordable\", \"Housing\", \"exemption\", \"tax\", \"reduction\"],\n",
    "        \"text\": \"4.3 19-821 Downtown High-Rise Incentive Program.\\n\\nRecommendation: Accept the report on the Downtown High-Rise Feasibility Assessment and direct staff to return to Council with the appropriate ordinance and resolution to enact the following:\\n\\n(a) Extending the certificate of occupancy deadline for the Affordable Housing Impact Fee exemption to December 31, 2023.\\n(b) Amending Title 4.46 and align the construction tax reduction with the certificate of occupancy deadline for the Affordable Housing Impact Fee exemption, and removing the planning and build permit requirements.\\nCEQA: Not a Project, File No. PP17-009, Staff Reports, Assessments, Annual Reports, and Informational Memos that involve no approvals of any City action. (Economic Development)\"\n",
    "      },\n",
    "      {\n",
    "        \"section_id\": \"001\",\n",
    "        \"doc_url\": \"https://agendaonline.net/public/Meeting.aspx?AgencyID=123&MeetingID=20136&AgencyTypeID=1&IsArchived=True\",\n",
    "        \"doc_name\": \"School Board Agenda, 2019-9-23\",\n",
    "        \"user_id\": \"emily\",\n",
    "        \"page_number\": \"NA\",\n",
    "        \"keywords\": [\"employee\", \"housing\", \"affordable\"],\n",
    "        \"text\": \"A.3. Master Plan for San Jose Unified Properties - Step 3 (ACTION)\\n\\nRECOMMENDATION: That San Jose Unified secure pre-development services to complete a pre-development analysis on the potential for employee housing opportunities at the following four locations: (1) 855 Lenzen Avenue, San Jose Unified District Offices, (2) 1088 Broadway, River Glen K-8 School, (3) 1325 Bouret Drive, Second Start-Pine Hill Non-Public School, and (4) 760 Hillsdale Avenue and 705-745 Capital Expressway, Metropolitan Education District.\"\n",
    "      },\n",
    "      {\n",
    "        \"section_id\": \"002\",\n",
    "        \"doc_url\": \"https://agendaonline.net/public/Meeting.aspx?AgencyID=123&MeetingID=20136&AgencyTypeID=1&IsArchived=True\",\n",
    "        \"doc_name\": \"School Board Agenda, 2019-9-23\",\n",
    "        \"user_id\": \"emily\",\n",
    "        \"page_number\": \"NA\",\n",
    "        \"keywords\": [\"employee\", \"housing\", \"affordable\"],\n",
    "        \"text\": \"A.3. Master Plan for San Jose Unified Properties - Step 3 (ACTION)\\n\\nRECOMMENDATION: That San Jose Unified secure pre-development services to complete a pre-development analysis on the potential for employee housing opportunities at the following four locations: (1) 855 Lenzen Avenue, San Jose Unified District Offices, (2) 1088 Broadway, River Glen K-8 School, (3) 1325 Bouret Drive, Second Start-Pine Hill Non-Public School, and (4) 760 Hillsdale Avenue and 705-745 Capital Expressway, Metropolitan Education District.\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "example_top_k_sections = []\n",
    "example_query = {\n",
    "    \"original_text\": \"I am a document document\",\n",
    "    \"tokens\": [\"I\", \"am\"],\n",
    "    \"filename\": \"filename1\",\n",
    "    \"section_id\": \"section1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(results): \n",
    "    pass\n",
    "write_output(example_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_history(history, top_k_sections, query): \n",
    "    pass\n",
    "update_history(read_history(), example_top_k_sections, example_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_history(history): \n",
    "    pass\n",
    "write_history(read_history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf(all_docs): \n",
    "    # for each word, how many unique docs does it show up in?\n",
    "    from collections import Counter\n",
    "    word_counts = \n",
    "    \n",
    "    idf = {}\n",
    "    for document in all_docs: \n",
    "        tokens = all_docs[document][\"tokens\"]\n",
    "        for \n",
    "        \"I\": 1./2,\n",
    "        \"am\": 1./2,\n",
    "        \"a\": 1./1,\n",
    "        \"another\": 1./1,\n",
    "        \"document\": 1./1,\n",
    "        \"doc\": 1./1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_idf(idf):\n",
    "    # write the idf dict to a file\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_docs(relevant_docs, keywords): \n",
    "    return [\n",
    "        {\n",
    "            \"original_text\": \"I am a document document\",\n",
    "            \"tokens\": [\"I\", \"am\"],\n",
    "            \"filename\": \"filename1\",\n",
    "            \"section_id\": \"section1\",\n",
    "        },\n",
    "        {\n",
    "            \"original_text\": \"I am a document document\",\n",
    "            \"tokens\": [\"a\", \"document\"],\n",
    "            \"filename\": \"filename1\",\n",
    "            \"section_id\": \"section2\",\n",
    "        },\n",
    "        {\n",
    "            \"original_text\": \"I am another doc\",\n",
    "            \"tokens\": [\"I\", \"am\", \"another\", \"doc\"],\n",
    "            \"filename\": \"filename2\",\n",
    "            \"section\": \"section3\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_docs(municipalities, time_window, all_docs, metadata): \n",
    "    # filter metadata to only those files that match the query municipality and time_window\n",
    "    # filter all docs to only filenames in subset of metadata\n",
    "    return [\n",
    "        {\n",
    "            \"original_text\": \"I am a document document\",\n",
    "            \"tokens\": [\"I\", \"am\", \"a\", \"document\"],\n",
    "            \"filename\": \"filename1\"\n",
    "        },\n",
    "        {\n",
    "            \"original_text\": \"I am another doc\",\n",
    "            \"tokens\": [\"I\", \"am\", \"another\", \"doc\"],\n",
    "            \"filename\": \"filename2\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_doc_section(doc_section, keywords, idf):\n",
    "    # vectorize etc.\n",
    "    keyword_vectors = np.array([vectors[t] for t in keywords if t in inverse_doc_props])\n",
    "    keyword_weights = np.array([inverse_doc_props[t] for t in keywords if t in inverse_doc_props])\n",
    "    document_section_scores = []\n",
    "    for s, section in enumerate(document_sections):\n",
    "        score = None\n",
    "        section_tokens = section[0]\n",
    "        # TODO: Zipf to figure out what the cutoff should be for normal communication\n",
    "        if len(set(section_tokens))<20:\n",
    "            score = 0\n",
    "        else:\n",
    "            section_vectors = np.array([vectors[t] for t in section_tokens if t in inverse_doc_props])\n",
    "            if section_vectors.shape[0]>0:\n",
    "    #             section_weights = np.array([inverse_doc_props[t] for t in section_tokens if t in inverse_doc_props])\n",
    "                similarities = cosine_similarity(section_vectors, keyword_vectors)\n",
    "    #             similarities = similarities * section_weights\n",
    "    #             similarities = similarities*(similarities>0.2)\n",
    "                keyword_similarities = np.mean(similarities, axis=0)\n",
    "    #             keyword_similarities = np.average(similarities, axis=0, weights=section_weights)\n",
    "                score = np.sum(keyword_similarities*keyword_weights)\n",
    "        document_section_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_k(doc_sections, doc_scores, k, history): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_results(results, top_k_sections, query): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_queries(use_cached_idf = False): \n",
    "    queries = read_queries()\n",
    "    metadata = read_metadata()\n",
    "    all_docs = read_docs(metadata)\n",
    "    if use_cached_idf:\n",
    "        idf = read_cached_idf()\n",
    "    else:\n",
    "        idf = calculate_idf(all_docs)\n",
    "        cache_idf(idf)\n",
    "    history = read_history()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for query in queries: \n",
    "        user_id = query[\"user_id\"]\n",
    "        query_id = query[\"query_id\"]\n",
    "        keywords = query[\"keywords\"]\n",
    "        time_window = query[\"time_window\"]\n",
    "        municipalities = query[\"municipalities\"]\n",
    "        relevant_docs = select_relevant_docs(municipalities, time_window, all_docs, metadata)\n",
    "        doc_sections = segment_docs(relevant_docs, keywords)\n",
    "        doc_sections_scores = score_doc_section(doc_section, keywords, idf)\n",
    "        top_k_sections = select_top_k(doc_sections, doc_scores, k, history)\n",
    "        results = update_results(results, top_k_sections, query)\n",
    "        history = update_history(history, top_k_sections, query)\n",
    "        \n",
    "    write_output(results)\n",
    "    write_history(history)\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
