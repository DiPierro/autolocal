{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "from datetime import *\n",
    "import os\n",
    "from autolocal.parsers.nlp import Tokenizer\n",
    "from gensim.parsing.preprocessing import *\n",
    "\n",
    "from autolocal.databases import S3DocumentManager\n",
    "import boto3\n",
    "from decimal import *\n",
    "from  tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up word vectors\n",
    "# (this takes a loooong time)\n",
    "def setup_word_vectors():\n",
    "  print(\"loading language model\")\n",
    "  # load language model (this takes a few minutes)\n",
    "  model = gensim.load('word2vec-google-news-300')\n",
    "  print(\"model loaded\")\n",
    "\n",
    "  vectors = model.wv\n",
    "  del model\n",
    "  vectors.init_sims(True) # normalize the vectors (!), so we can use the dot product as similarity measure\n",
    "\n",
    "  print('embeddings loaded ')\n",
    "  print('loading docs ... ')\n",
    "  return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata():\n",
    "    table = boto3.resource('dynamodb', region_name='us-west-1').Table('autolocal-documents')\n",
    "    s3_client = boto3.client('s3')\n",
    "    metadata = pd.DataFrame(table.scan()[\"Items\"])\n",
    "    metadata[\"date\"] = [datetime.strptime(d, '%Y-%m-%d') for d in metadata[\"date\"]]\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "casing = \"lower_non_acronyms\"\n",
    "# casing = \"lower_non_acronyms\"\n",
    "# TODO: is lowercasing necessary?\n",
    "\n",
    "def casing_function():\n",
    "    if casing==\"cased\":\n",
    "        return lambda x: x\n",
    "    elif casing==\"lower\":\n",
    "        return lambda x: x.lower()\n",
    "    elif casing==\"lower_non_acronyms\":\n",
    "        return lambda x: x if x.isupper() else x.lower()\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "preprocess_filters = [\n",
    "    casing_function(),\n",
    "    strip_punctuation,\n",
    "    strip_numeric,\n",
    "    strip_non_alphanum,\n",
    "    strip_multiple_whitespaces,\n",
    "#     strip_numeric,\n",
    "    remove_stopwords,\n",
    "#     strip_short\n",
    "]\n",
    "\n",
    "class DocTextReader():\n",
    "    def __init__(self, log_every=100):\n",
    "        self.log_every = log_every\n",
    "        s3 = boto3.resource('s3', region_name='us-west-1')\n",
    "        self.bucket = s3.Bucket('autolocal-documents')\n",
    "\n",
    "    def read_document_string(self, s3_path):\n",
    "        return self.bucket.Object(s3_path).get()['Body'].read()\n",
    "\n",
    "    def read_docs(self, s3_paths):\n",
    "        # read all documents that we know about\n",
    "        # tokenize each document\n",
    "        # return list of documents\n",
    "\n",
    "        documents = {}\n",
    "        n_docs_total = len(s3_paths)\n",
    "\n",
    "        i = 0\n",
    "        n_docs_read = 0\n",
    "        for s3_path in s3_paths:\n",
    "            try:\n",
    "                doc_string = self.read_document_string(s3_path)\n",
    "                doc_tokens = preprocess_string(doc_string, filters=preprocess_filters)\n",
    "                documents[s3_path] = {\n",
    "                    \"original_text\": doc_string,\n",
    "                    \"tokens\": doc_tokens\n",
    "                }\n",
    "            except Exception as e:\n",
    "                if i < 10:\n",
    "                    print(\"Key not found: {}\".format(s3_path))\n",
    "                elif i == 10:\n",
    "                    print(\"More than 10 keys not found\")\n",
    "                    print(e)\n",
    "                    break\n",
    "                i+=1\n",
    "            if n_docs_read % self.log_every == 0:\n",
    "                print(\"{} of {} documents read\".format(n_docs_read, n_docs_total))\n",
    "            n_docs_read+=1\n",
    "\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = DocTextReader()\n",
    "# all_docs = r.read_docs(read_metadata()[\"local_path_txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf = calculate_idf(all_docs)\n",
    "# len(idf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries(query_source):\n",
    "    if query_source == \"actual\":\n",
    "        table = boto3.resource('dynamodb', region_name='us-west-2').Table('autoLocalNews')\n",
    "    elif query_source == \"quick\":\n",
    "        table = boto3.resource('dynamodb', region_name='us-west-1').Table('quick_queries')\n",
    "    else:\n",
    "        raise Exception\n",
    "    queries = table.scan()[\"Items\"]\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_history():\n",
    "    try:\n",
    "        table = boto3.resource('dynamodb', region_name='us-west-1').Table('history')\n",
    "        history = table.scan()[\"Items\"]\n",
    "    except:\n",
    "        history = []\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cached_idf():\n",
    "    s3 = boto3.resource('s3', region_name='us-west-1')\n",
    "    bucket = s3.Bucket('autolocal-documents')\n",
    "    idf = json.load(bucket.Object('idf_{}.json'.format(casing)).get()['Body'])\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_idf(idf):\n",
    "    s3 = boto3.resource('s3')\n",
    "    object = s3.Object('autolocal-documents', 'idf_{}.json'.format(casing))\n",
    "    object.put(Body=json.dumps(idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf(all_docs): \n",
    "    # for each word, how many unique docs does it show up in?\n",
    "    from collections import Counter\n",
    "    \n",
    "    doc_freq = {}\n",
    "    for document in all_docs: \n",
    "        tokens = all_docs[document][\"tokens\"]\n",
    "        for token in tokens:\n",
    "            if token in doc_freq:\n",
    "                doc_freq[token] += 1\n",
    "            else:\n",
    "                doc_freq[token] = 1\n",
    "    \n",
    "    inverse_doc_freq = {}\n",
    "    for word in doc_freq:\n",
    "        inverse_doc_freq[word] = 1./doc_freq[word]\n",
    "    \n",
    "    return inverse_doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_windows = {\n",
    "    'upcoming_only': datetime.now() + timedelta(days=0.5),\n",
    "    'this_week': datetime.now() - timedelta(weeks=1),\n",
    "    'this_year': datetime.now() - timedelta(days=365),\n",
    "    'past_six_months':datetime.now() - timedelta(days=183),\n",
    "    'all': None\n",
    "}\n",
    "\n",
    "def find_relevant_filenames(queries, metadata): \n",
    "    \n",
    "    # filter metadata to only those files that match the query municipality and time_window\n",
    "    municipalities_by_time_window = {}\n",
    "    for query in queries:\n",
    "        time_window = query['Time Window']\n",
    "        if time_window in municipalities_by_time_window:\n",
    "            municipalities_by_time_window[time_window].update(query['Municipalities'])\n",
    "        else:\n",
    "            municipalities_by_time_window[time_window] = set(query['Municipalities'])\n",
    "            \n",
    "    relevant_filenames = set()\n",
    "    for time_window in municipalities_by_time_window:\n",
    "        starting_date = time_windows[time_window]\n",
    "        potential_documents = metadata\n",
    "        if starting_date:\n",
    "            potential_documents = potential_documents[potential_documents[\"date\"] >= starting_date]\n",
    "        cities = municipalities_by_time_window[time_window]\n",
    "        potential_documents = potential_documents[[(c in cities) for c in potential_documents[\"city\"]]]\n",
    "        relevant_filenames.update(potential_documents['local_path_txt'])\n",
    "    return relevant_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_docs(municipalities, time_window, all_docs, metadata):\n",
    "    # filter metadata to only those files that match the query municipality and time_window\n",
    "    starting_date = time_windows[time_window]\n",
    "    potential_documents = metadata\n",
    "    if starting_date:\n",
    "        potential_documents = potential_documents[potential_documents[\"date\"] >= starting_date]\n",
    "    potential_documents = potential_documents[[(c in municipalities) for c in potential_documents[\"city\"]]]\n",
    "    # filter all docs to only filenames in subset of metadata\n",
    "    return [{**all_docs[f], 'filename':f} for f in potential_documents['local_path_txt'] if f in all_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Play with section length\n",
    "# TODO: smart sectioning that's sensitive to multiple line breaks and other section break signals\n",
    "# TODO: extract sections that overlap with each other\n",
    "def segment_docs(relevant_docs):\n",
    "    doc_sections = []\n",
    "    approx_section_length = 100 # tokens\n",
    "    min_section_length = 5\n",
    "    \n",
    "    for doc in relevant_docs:\n",
    "        doc_tokens = doc[\"tokens\"]\n",
    "        original_text = doc[\"original_text\"].decode('utf-8')\n",
    "        filename = doc[\"filename\"]\n",
    "        \n",
    "        doc_section_lines = []\n",
    "        doc_section_tokens = []\n",
    "        starting_page = 0\n",
    "        starting_line = 0\n",
    "        pages = original_text.split('\\f')\n",
    "        for p, page in enumerate(pages):\n",
    "            lines = page.split('\\n')\n",
    "            for lnum, line in enumerate(lines):\n",
    "                line_tokens = preprocess_string(line, filters=preprocess_filters)\n",
    "                doc_section_tokens += line_tokens\n",
    "                doc_section_lines.append(line)\n",
    "                if len(doc_section_tokens) >= approx_section_length:\n",
    "                    doc_sections.append({\n",
    "                        **doc,\n",
    "                        'starting_page': starting_page,\n",
    "                        'starting_line': starting_line,\n",
    "                        'ending_page': p,\n",
    "                        'ending_line': lnum,\n",
    "                        'section_text': '\\n'.join(doc_section_lines),\n",
    "                        'section_tokens': doc_section_tokens\n",
    "                    })\n",
    "                    doc_section_lines = []\n",
    "                    doc_section_tokens = []\n",
    "                    # have we reached the last line of this page?\n",
    "                    if lnum == (len(lines)-1):\n",
    "                        # next section starts at top of next page\n",
    "                        starting_page = p+1\n",
    "                        starting_line = 0\n",
    "                    else:\n",
    "                        # next section starts on next line of this page\n",
    "                        starting_page = p\n",
    "                        starting_line = lnum+1\n",
    "        # end of the document\n",
    "        if len(doc_section_tokens) >= min_section_length:\n",
    "            doc_sections.append({\n",
    "                **doc,\n",
    "                'starting_page': starting_page,\n",
    "                'starting_line': starting_line,\n",
    "                'ending_page': p,\n",
    "                'ending_line': lnum,\n",
    "                'section_text': '\\n'.join(doc_section_lines),\n",
    "                'section_tokens': doc_section_tokens\n",
    "            })\n",
    "            \n",
    "    return doc_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use vectors to find closes words to keywords\n",
    "def score_doc_sections(doc_sections, keywords, idf):\n",
    "    # vectorize etc.\n",
    "    # only consider keywords that have idf weights\n",
    "    keywords = [keyword for keyword in keywords if (keyword in idf and keyword in vectors)]\n",
    "    keyword_vectors = np.array([vectors[keyword] for keyword in keywords])\n",
    "    keyword_weights = np.array([idf[keyword] for keyword in keywords])\n",
    "    doc_sections_scores = []\n",
    "    for s, section in enumerate(doc_sections):\n",
    "        score = None\n",
    "        section_tokens = section[\"section_tokens\"]\n",
    "        # TODO: Zipf to figure out what the cutoff should be for normal communication\n",
    "        # If the number of unique tokens in the section is too small, it's probably not an interesting section\n",
    "        if len(set(section_tokens))<20:\n",
    "            score = 0\n",
    "        else:\n",
    "            section_vectors = np.array([vectors[t] for t in section_tokens if (t in idf and t in vectors)])\n",
    "            if section_vectors.shape[0]>0:\n",
    "    #             section_weights = np.array([inverse_doc_props[t] for t in section_tokens if t in inverse_doc_props])\n",
    "                similarities = cosine_similarity(section_vectors, keyword_vectors)\n",
    "    #             similarities = similarities * section_weights\n",
    "    #             similarities = similarities*(similarities>0.2)\n",
    "                keyword_similarities = np.mean(similarities, axis=0)\n",
    "    #             keyword_similarities = np.average(similarities, axis=0, weights=section_weights)\n",
    "                score = np.sum(keyword_similarities*keyword_weights)\n",
    "        doc_sections_scores.append(score)\n",
    "\n",
    "    return doc_sections_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_is_too_similar(a, b):\n",
    "    # if there are only 2 edits to get from one text to the other, it's not good\n",
    "    return editdistance.eval(a, b) < 20\n",
    "\n",
    "# make sure we're not giving similar text among the top k (e.g. shows up on both minutes and agenda)\n",
    "def check_repeated_text(top_k, section_text):\n",
    "    for old in top_k:\n",
    "        if text_is_too_similar(old[1][\"section_text\"], section_text):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def select_top_k(doc_sections, doc_sections_scores, k, user_history):\n",
    "    sorted_sections = sorted(zip(doc_sections_scores, doc_sections), key=lambda pair: pair[0], reverse=True)\n",
    "    top_k = []\n",
    "    text_returned = []\n",
    "    for x in sorted_sections:\n",
    "        filename = x[1][\"filename\"]\n",
    "        starting_page = x[1][\"starting_page\"]\n",
    "        starting_line = x[1][\"starting_line\"]\n",
    "        ending_page = x[1][\"ending_page\"]\n",
    "        ending_line = x[1][\"ending_line\"]\n",
    "        section_text = x[1][\"section_text\"]\n",
    "        if filename in [x['filename'] for x in user_history]:\n",
    "            print(\"this user has already seen their top file ({})\".format(filename))\n",
    "        elif check_repeated_text(top_k, section_text):\n",
    "            print(\"this excpert has already been returned\")\n",
    "        else:\n",
    "            top_k.append(x)\n",
    "        if len(top_k) >= k:\n",
    "            break\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_with_top_k(history, top_k_sections, query):\n",
    "    for section in top_k_sections:\n",
    "        x = section[1]\n",
    "        x.update(query)\n",
    "        history.append(x)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row2item(row):\n",
    "    row = dict(row)\n",
    "    item = {}\n",
    "    for k,v in row.items():\n",
    "        if k in ['local_path_pdf', 'local_path_txt']:\n",
    "            v = v[8:]\n",
    "        if isinstance(v, str):\n",
    "            item[k] = v\n",
    "        elif np.isnan(v):\n",
    "            pass\n",
    "    return item\n",
    "\n",
    "def write_history(history):\n",
    "    try:\n",
    "        table = boto3.resource('dynamodb', region_name='us-west-1').Table('history')\n",
    "        table.scan()\n",
    "    except:\n",
    "        table_args = {\n",
    "            'TableName': 'autolocal-documents',\n",
    "            'KeySchema': [\n",
    "                {\n",
    "                    'AttributeName': 'doc_id',\n",
    "                    'KeyType': 'HASH'\n",
    "                }\n",
    "            ],\n",
    "            'AttributeDefinitions': [\n",
    "                {\n",
    "                    'AttributeName': 'doc_id',\n",
    "                    'AttributeType': 'S'\n",
    "                }        \n",
    "            ],\n",
    "            'ProvisionedThroughput':\n",
    "            {\n",
    "                'ReadCapacityUnits': 5,\n",
    "                'WriteCapacityUnits': 5\n",
    "            }\n",
    "        }\n",
    "        table = dynamodb.create_table(**table_args)\n",
    "    with table.batch_writer() as batch:\n",
    "        for i, row in tqdm(history.iterrows()):\n",
    "            item = row2item(row)\n",
    "            batch.put_item(Item=item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_emails(results): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading language model\n",
      "model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erindb/miniconda3/envs/ecj/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings loaded \n",
      "loading docs ... \n"
     ]
    }
   ],
   "source": [
    "# TODO: contextual vectors\n",
    "vectors = setup_word_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading queries\n",
      "reading metadata\n",
      "setting up reader\n",
      "loading cached idf\n",
      "finding relevant filenames\n",
      "reading relevant documents\n",
      "0 of 34 documents read\n",
      "reading history\n",
      "running query 0 of 2\n",
      "user id: emily\n",
      "segmenting documents\n",
      "scoring documents\n",
      "~~~~\n",
      "~~~~\n",
      "available at http://Sunnyvale.ca.gov/PublicComments\n",
      "\n",
      "Planning to provide materials to Council?\n",
      "If you wish to provide the City Council with copies of your presentation materials, \n",
      "please provide 12 copies of the materials to the City Clerk (located to the left of \n",
      "the Council dais). The City Clerk will distribute your items to the Council.\n",
      "\n",
      "Upcoming Meetings\n",
      "Visit https://sunnyvaleca.legistar.com for upcoming Council, board and \n",
      "commission meeting information.\n",
      "\n",
      "City of Sunnyvale\n",
      "\n",
      "Page 7 \n",
      "\n",
      "Printed on 11/26/2019\n",
      "\n",
      "\n",
      "====\n",
      "====\n",
      "~~~~\n",
      "~~~~\n",
      "\n",
      "the filing of an application for Priority Development Area \n",
      "designation in Moffett Park under the Plan Bay Area 2040 \n",
      "Program and Find that the action is exempt from CEQA.\n",
      "\n",
      "1.G\n",
      "\n",
      "19-1024\n",
      "\n",
      "Adopt a Resolution Approving the Application for Senate Bill \n",
      "(SB) 2 Planning Grant Funds from the California Department \n",
      "of Housing and Community Development and Find that the \n",
      "\n",
      "City of Sunnyvale\n",
      "\n",
      "Page 3 \n",
      "\n",
      "Printed on 11/26/2019\n",
      "\n",
      "City Council\n",
      "\n",
      "Notice and Agenda\n",
      "\n",
      "December 3, 2019\n",
      "\n",
      "Action is Exempt from CEQA.\n",
      "\n",
      "Recommendation: Find that the action is exempt from the California \n",
      "\n",
      "Environmental Quality Act (\"CEQA\") pursuant to CEQA \n",
      "Guidelines section 15378(b)(4) and (b)(5) and adopt the \n",
      "Resolution ratifying and approving the City's application for \n",
      "SB2 Planning Grant Funds (Attachment 1 to the report).\n",
      "\n",
      "1.H\n",
      "\n",
      "19-1231\n",
      "\n",
      "Ratify Appointment of Councilmember Nancy Smith to the \n",
      "League of California Cities Policy Committee on Housing, \n",
      "Community and Economic Development for the 2019/20 Term; \n",
      "Ratify Appointment of Vice Mayor Russ Melton to the League \n",
      "of California Cities Policy Committee on Revenue and \n",
      "Taxation for the 2019/20 Term\n",
      "\n",
      "Recommendation: Staff makes no recommendation.\n",
      "====\n",
      "====\n",
      "\n",
      "\n",
      "running query 1 of 2\n",
      "user id: some_other_journalist\n",
      "segmenting documents\n",
      "scoring documents\n",
      "this excpert has already been returned\n",
      "~~~~\n",
      "~~~~\n",
      "decisions in the critical functions the City provides to the community. They are: \n",
      "\n",
      " Strategic Support - The internal functions that enable the CSAs to provide direct \n",
      "   services to the community in an effective and efficient manner. \n",
      "\n",
      " Community & Economic Development - Manage the growth and change of the community in \n",
      "   order to create and preserve healthy neighborhoods and ensure a diverse range of employment and \n",
      "   housing opportunities. \n",
      "\n",
      " Neighborhood Services - Serve, foster, and strengthen community by providing access to lifelong \n",
      "   learning and opportunities to enjoy life. \n",
      "\n",
      "Transportation & Aviation Services - A safe and efficient transportation system that contributes to \n",
      "  the livability and economic health of the City; and provide for the air transportation needs of the \n",
      "  community and the region at levels that is acceptable to the community. \n",
      "\n",
      " Environmental and Utility Services - Manage environmental services and utility systems to ensure \n",
      "  a sustainable environment for the community. \n",
      "\n",
      " Public Safety - Commitment to excellence in public safety by investing in neighborhood \n",
      "  partnerships as well as prevention, enforcement, and emergency preparedness services. \n",
      "\n",
      "You may speak to the City Council about any discussion item that is on the agenda, and you may also speak \n",
      "====\n",
      "====\n",
      "~~~~\n",
      "~~~~\n",
      "      solutions,’ ‘interim housing,’ etc.), but should enable Council and public to understand if   \n",
      "      we're maximizing our scarce dollars within that broad strategy. Where metrics cannot   \n",
      "      adequately capture progress, or where too speculative, simply designate that there is no   \n",
      "      suitable metric.”1   \n",
      "\n",
      "3. Pilot the use of “invitation zones” in those specific heavily-impacted neighborhoods   \n",
      "where temporary housing solutions (e.g., shelters, transitional housing, and safe   \n",
      "parking) with services for the homeless are being deployed, to:   \n",
      "      a. Give first priority to helping the homeless in the immediate geographic vicinity get   \n",
      "      off the street, and   \n",
      "      b. Enforce a “no encampment zone” for the blocks surrounding the facility to ensure   \n",
      "      a visible, measurable improvement in the quality of life of the neighborhood served   \n",
      "      by the City homeless resources.   \n",
      "\n",
      "The City Manager and Housing Director can start complying with this direction by first   \n",
      "identifying what objective(s) we seek to achieve within each category of funding, and   \n",
      "settle upon a suitable means for measuring success. That metric should then be used as a   \n",
      "basis for comparing all programs within that category.   \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      " City of San José \n",
      "\n",
      "Page 11   \n",
      "\n",
      "September 24, 2019 \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "19-826 \n",
      "\n",
      " \n",
      " City Council \n",
      " \n",
      " 4.6 \n",
      " \n",
      "  \n",
      "  \n",
      " \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      " \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      " \n",
      "4.7 \n",
      " \n",
      " \n",
      " \n",
      " Recommendation: \n",
      "  \n",
      "  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "19-824 \n",
      "\n",
      "Minutes Synopsis \n",
      "====\n",
      "====\n",
      "\n",
      "\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# vectors = setup_word_vectors()\n",
    "def run_queries(use_cached_idf = False, query_source=\"actual\", k=3): \n",
    "    print(\"reading queries\")\n",
    "    queries = read_queries(query_source)\n",
    "    print(\"reading metadata\")\n",
    "    metadata = read_metadata()\n",
    "    print(\"setting up reader\")\n",
    "    doc_text_reader = DocTextReader(log_every=100)\n",
    "    if use_cached_idf:\n",
    "        # used cached idf and only read relevant documents\n",
    "        print(\"loading cached idf\")\n",
    "        idf = read_cached_idf()\n",
    "        print(\"finding relevant filenames\")\n",
    "        relevant_filenames = find_relevant_filenames(queries, metadata)\n",
    "        # (not actually *all*, but all the ones we care about for queries)\n",
    "        print(\"reading relevant documents\")\n",
    "        all_docs = doc_text_reader.read_docs(relevant_filenames)\n",
    "    else:\n",
    "        # read all documents and calculate inverse document frequency\n",
    "        all_filenames = metadata[\"local_path_txt\"]\n",
    "        print(\"reading all documents\")\n",
    "        all_docs = doc_text_reader.read_docs(all_filenames)\n",
    "        print(\"calculating idf\")\n",
    "        idf = calculate_idf(all_docs)\n",
    "        cache_idf(idf)\n",
    "    print(\"reading history\")\n",
    "    history = read_history()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for q, query in enumerate(queries):\n",
    "        print(\"running query {} of {}\".format(q, len(queries)))\n",
    "        user_id = query[\"id\"]\n",
    "        print(\"user id: {}\".format(user_id))\n",
    "        user_history = [x for x in history if x['id'] == user_id]\n",
    "        keywords = query[\"Keywords\"]\n",
    "        time_window = query[\"Time Window\"]\n",
    "        municipalities = query[\"Municipalities\"]\n",
    "        relevant_docs = select_relevant_docs(municipalities, time_window, all_docs, metadata)\n",
    "        print(\"segmenting documents\")\n",
    "        doc_sections = segment_docs(relevant_docs)\n",
    "        print(\"scoring documents\")\n",
    "        doc_sections_scores = score_doc_sections(doc_sections, keywords, idf)\n",
    "        top_k_sections = select_top_k(doc_sections, doc_sections_scores, k, user_history)\n",
    "        print(\"~~~~\")\n",
    "        print(\"~~~~\")\n",
    "        print(re.sub(\"(\\n(\\n)+)\", \"\\n\\n\",\n",
    "                     re.sub(\"\\u2022\", \"\", top_k_sections[0][1][\"section_text\"])))\n",
    "        print(\"====\")\n",
    "        print(\"====\")\n",
    "        print(\"~~~~\")\n",
    "        print(\"~~~~\")\n",
    "        print(re.sub(\"(\\n(\\n)+)\", \"\\n\\n\",\n",
    "                     re.sub(\"\\u2022\", \"\", top_k_sections[1][1][\"section_text\"])))\n",
    "        print(\"====\")\n",
    "        print(\"====\")\n",
    "        results = update_with_top_k(results, top_k_sections, query)\n",
    "        history = update_with_top_k(history, top_k_sections, query)\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "#     print(\"sending emails\")\n",
    "#     send_emails(results)\n",
    "#     write_history(history)\n",
    "    print(\"finished\")\n",
    "\n",
    "\n",
    "run_queries(use_cached_idf=True, query_source=\"quick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
