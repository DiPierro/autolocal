{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "from datetime import *\n",
    "import os\n",
    "from autolocal.parsers.nlp import Tokenizer\n",
    "from gensim.parsing.preprocessing import *\n",
    "\n",
    "from autolocal.databases import S3DocumentManager\n",
    "import boto3\n",
    "from decimal import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up word vectors\n",
    "# (this takes a loooong time)\n",
    "def setup_word_vectors():\n",
    "  print(\"loading language model\")\n",
    "  # load language model (this takes a few minutes)\n",
    "  model = gensim.load('word2vec-google-news-300')\n",
    "  print(\"model loaded\")\n",
    "\n",
    "  vectors = model.wv\n",
    "  del model\n",
    "  vectors.init_sims(True) # normalize the vectors (!), so we can use the dot product as similarity measure\n",
    "\n",
    "  print('embeddings loaded ')\n",
    "  print('loading docs ... ')\n",
    "  return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata():\n",
    "    table = boto3.resource('dynamodb', region_name='us-west-1').Table('autolocal-documents')\n",
    "    s3_client = boto3.client('s3')\n",
    "    metadata = pd.DataFrame(table.scan()[\"Items\"])\n",
    "    metadata[\"date\"] = [datetime.strptime(d, '%Y-%m-%d') for d in metadata[\"date\"]]\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: is lowercasing necessary?\n",
    "preprocess_filters = [\n",
    "    lambda x: x.lower(),\n",
    "    strip_punctuation,\n",
    "    strip_numeric,\n",
    "    strip_non_alphanum,\n",
    "    strip_multiple_whitespaces,\n",
    "    strip_numeric,\n",
    "    remove_stopwords,\n",
    "    strip_short\n",
    "        ]\n",
    "\n",
    "class DocTextReader():\n",
    "    def __init__(self, log_every=100):\n",
    "        self.log_every = log_every\n",
    "        s3 = boto3.resource('s3', region_name='us-west-1')\n",
    "        self.bucket = s3.Bucket('autolocal-documents')\n",
    "\n",
    "    def read_document_string(self, s3_path):\n",
    "        return self.bucket.Object(s3_path).get()['Body'].read()\n",
    "\n",
    "    def read_docs(self, s3_paths):\n",
    "        # read all documents that we know about\n",
    "        # tokenize each document\n",
    "        # return list of documents\n",
    "\n",
    "        documents = {}\n",
    "        n_docs_total = len(s3_paths)\n",
    "\n",
    "        i = 0\n",
    "        n_docs_read = 0\n",
    "        for s3_path in s3_paths:\n",
    "            try:\n",
    "                doc_string = self.read_document_string(s3_path)\n",
    "                doc_tokens = preprocess_string(doc_string, filters=preprocess_filters)\n",
    "                documents[s3_path] = {\n",
    "                    \"original_text\": doc_string,\n",
    "                    \"tokens\": doc_tokens\n",
    "                }\n",
    "            except Exception as e:\n",
    "                if i < 10:\n",
    "                    print(\"Key not found: {}\".format(s3_path))\n",
    "                elif i == 10:\n",
    "                    print(\"More than 10 keys not found\")\n",
    "                    print(e)\n",
    "                    break\n",
    "                i+=1\n",
    "            if n_docs_read % self.log_every == 0:\n",
    "                print(\"{} of {} documents read\".format(n_docs_read, n_docs_total))\n",
    "            n_docs_read+=1\n",
    "\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = DocTextReader()\n",
    "# all_docs = r.read_docs(read_metadata()[\"local_path_txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf = calculate_idf(all_docs)\n",
    "# len(idf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries(query_source):\n",
    "    if query_source == \"actual\":\n",
    "        table = boto3.resource('dynamodb', region_name='us-west-2').Table('autoLocalNews')\n",
    "    elif query_source == \"quick\":\n",
    "        table = boto3.resource('dynamodb', region_name='us-west-1').Table('quick_queries')\n",
    "    else:\n",
    "        raise Exception\n",
    "    queries = table.scan()[\"Items\"]\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_history():\n",
    "    try:\n",
    "        table = boto3.resource('dynamodb', region_name='us-west-1').Table('history')\n",
    "        history = table.scan()[\"Items\"]\n",
    "    except:\n",
    "        history = []\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cached_idf():\n",
    "    s3 = boto3.resource('s3', region_name='us-west-1')\n",
    "    bucket = s3.Bucket('autolocal-documents')\n",
    "    idf = json.load(bucket.Object('idf.json').get()['Body'])\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_idf(idf):\n",
    "    s3 = boto3.resource('s3')\n",
    "    object = s3.Object('autolocal-documents', 'idf.json')\n",
    "    object.put(Body=json.dumps(idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf(all_docs): \n",
    "    # for each word, how many unique docs does it show up in?\n",
    "    from collections import Counter\n",
    "    \n",
    "    doc_freq = {}\n",
    "    for document in all_docs: \n",
    "        tokens = all_docs[document][\"tokens\"]\n",
    "        for token in tokens:\n",
    "            if token in doc_freq:\n",
    "                doc_freq[token] += 1\n",
    "            else:\n",
    "                doc_freq[token] = 1\n",
    "    \n",
    "    inverse_doc_freq = {}\n",
    "    for word in doc_freq:\n",
    "        inverse_doc_freq[word] = 1./doc_freq[word]\n",
    "    \n",
    "    return inverse_doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_windows = {\n",
    "    'upcoming_only': datetime.now() + timedelta(days=0.5),\n",
    "    'this_week': datetime.now() - timedelta(weeks=1),\n",
    "    'this_year': datetime.now() - timedelta(days=365),\n",
    "    'past_six_months':datetime.now() - timedelta(days=183),\n",
    "    'all': None\n",
    "}\n",
    "\n",
    "def find_relevant_filenames(queries, metadata): \n",
    "    \n",
    "    # filter metadata to only those files that match the query municipality and time_window\n",
    "    municipalities_by_time_window = {}\n",
    "    for query in queries:\n",
    "        time_window = query['Time Window']\n",
    "        if time_window in municipalities_by_time_window:\n",
    "            municipalities_by_time_window[time_window].update(query['Municipalities'])\n",
    "        else:\n",
    "            municipalities_by_time_window[time_window] = set(query['Municipalities'])\n",
    "            \n",
    "    relevant_filenames = set()\n",
    "    for time_window in municipalities_by_time_window:\n",
    "        starting_date = time_windows[time_window]\n",
    "        potential_documents = metadata\n",
    "        if starting_date:\n",
    "            potential_documents = potential_documents[potential_documents[\"date\"] >= starting_date]\n",
    "        cities = municipalities_by_time_window[time_window]\n",
    "        potential_documents = potential_documents[[(c in cities) for c in potential_documents[\"city\"]]]\n",
    "        relevant_filenames.update(potential_documents['local_path_txt'])\n",
    "    return relevant_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_docs(municipalities, time_window, all_docs, metadata):\n",
    "    # filter metadata to only those files that match the query municipality and time_window\n",
    "    starting_date = time_windows[time_window]\n",
    "    potential_documents = metadata\n",
    "    if starting_date:\n",
    "        potential_documents = potential_documents[potential_documents[\"date\"] >= starting_date]\n",
    "    potential_documents = potential_documents[[(c in municipalities) for c in potential_documents[\"city\"]]]\n",
    "    # filter all docs to only filenames in subset of metadata\n",
    "    return [{**all_docs[f], 'filename':f} for f in potential_documents['local_path_txt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Play with section length\n",
    "# TODO: smart sectioning that's sensitive to multiple line breaks and other section break signals\n",
    "# TODO: extract sections that overlap with each other\n",
    "def segment_docs(relevant_docs):\n",
    "    doc_sections = []\n",
    "    approx_section_length = 100 # tokens\n",
    "    min_section_length = 5\n",
    "    \n",
    "    for doc in relevant_docs:\n",
    "        doc_tokens = doc[\"tokens\"]\n",
    "        original_text = doc[\"original_text\"].decode('utf-8')\n",
    "        filename = doc[\"filename\"]\n",
    "        \n",
    "        doc_section_lines = []\n",
    "        doc_section_tokens = []\n",
    "        starting_page = 0\n",
    "        starting_line = 0\n",
    "        pages = original_text.split('\\f')\n",
    "        for p, page in enumerate(pages):\n",
    "            lines = page.split('\\n')\n",
    "            for lnum, line in enumerate(lines):\n",
    "                line_tokens = preprocess_string(line, filters=preprocess_filters)\n",
    "                doc_section_tokens += line_tokens\n",
    "                doc_section_lines.append(line)\n",
    "                if len(doc_section_tokens) >= approx_section_length:\n",
    "                    doc_sections.append({\n",
    "                        **doc,\n",
    "                        'starting_page': starting_page,\n",
    "                        'starting_line': starting_line,\n",
    "                        'ending_page': p,\n",
    "                        'ending_line': lnum,\n",
    "                        'section_text': '\\n'.join(doc_section_lines),\n",
    "                        'section_tokens': doc_section_tokens\n",
    "                    })\n",
    "                    doc_section_lines = []\n",
    "                    doc_section_tokens = []\n",
    "                    # have we reached the last line of this page?\n",
    "                    if lnum == (len(lines)-1):\n",
    "                        # next section starts at top of next page\n",
    "                        starting_page = p+1\n",
    "                        starting_line = 0\n",
    "                    else:\n",
    "                        # next section starts on next line of this page\n",
    "                        starting_page = p\n",
    "                        starting_line = lnum+1\n",
    "        # end of the document\n",
    "        if len(doc_section_tokens) >= min_section_length:\n",
    "            doc_sections.append({\n",
    "                **doc,\n",
    "                'starting_page': starting_page,\n",
    "                'ending_page': p,\n",
    "                'section_text': '\\n'.join(doc_section_lines),\n",
    "                'section_tokens': doc_section_tokens\n",
    "            })\n",
    "            \n",
    "    return doc_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_doc_sections(doc_sections, keywords, idf):\n",
    "    # vectorize etc.\n",
    "    # only consider keywords that have idf weights\n",
    "    keywords = [keyword for keyword in keywords if (keyword in idf and keyword in vectors)]\n",
    "    keyword_vectors = np.array([vectors[keyword] for keyword in keywords])\n",
    "    keyword_weights = np.array([idf[keyword] for keyword in keywords])\n",
    "    doc_sections_scores = []\n",
    "    for s, section in enumerate(doc_sections):\n",
    "        score = None\n",
    "        section_tokens = section[\"section_tokens\"]\n",
    "        # TODO: Zipf to figure out what the cutoff should be for normal communication\n",
    "        # If the number of unique tokens in the section is too small, it's probably not an interesting section\n",
    "        if len(set(section_tokens))<20:\n",
    "            score = 0\n",
    "        else:\n",
    "            section_vectors = np.array([vectors[t] for t in section_tokens if (t in idf and t in vectors)])\n",
    "            print(section_vectors)\n",
    "            break\n",
    "            if section_vectors.shape[0]>0:\n",
    "    #             section_weights = np.array([inverse_doc_props[t] for t in section_tokens if t in inverse_doc_props])\n",
    "                similarities = cosine_similarity(section_vectors, keyword_vectors)\n",
    "    #             similarities = similarities * section_weights\n",
    "    #             similarities = similarities*(similarities>0.2)\n",
    "                keyword_similarities = np.mean(similarities, axis=0)\n",
    "    #             keyword_similarities = np.average(similarities, axis=0, weights=section_weights)\n",
    "                score = np.sum(keyword_similarities*keyword_weights)\n",
    "        doc_sections_scores.append(score)\n",
    "\n",
    "    return doc_sections_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_k(doc_sections, doc_scores, k, history): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_results = [\n",
    "#   {\n",
    "#     \"user_id\": \"emily\",\n",
    "#    'special',\n",
    "#    'meeting',\n",
    "#    'subject',\n",
    "#    'approve',\n",
    "#    'august',\n",
    "#     \"document_sections\": [\n",
    "#       {\n",
    "#         \"section_id\": \"000\",\n",
    "#         \"doc_url\": \"https://sanjose.legistar.com/View.ashx?M=A&ID=709096&GUID=CF2165D1-28CB-4670-AB54-F35B649DE71D\",\n",
    "#         \"doc_name\": \"Agenda 2019-09-24\",\n",
    "#         \"user_id\": \"emily\",\n",
    "#         \"page_number\": \"11\",\n",
    "#         \"keywords\": [\"High-Rise\", \"Incentive\", \"Affordable\", \"Housing\", \"exemption\", \"tax\", \"reduction\"],\n",
    "#         \"text\": \"4.3 19-821 Downtown High-Rise Incentive Program.\\n\\nRecommendation: Accept the report on the Downtown High-Rise Feasibility Assessment and direct staff to return to Council with the appropriate ordinance and resolution to enact the following:\\n\\n(a) Extending the certificate of occupancy deadline for the Affordable Housing Impact Fee exemption to December 31, 2023.\\n(b) Amending Title 4.46 and align the construction tax reduction with the certificate of occupancy deadline for the Affordable Housing Impact Fee exemption, and removing the planning and build permit requirements.\\nCEQA: Not a Project, File No. PP17-009, Staff Reports, Assessments, Annual Reports, and Informational Memos that involve no approvals of any City action. (Economic Development)\"\n",
    "#       },\n",
    "#       {\n",
    "#         \"section_id\": \"001\",\n",
    "#         \"doc_url\": \"https://agendaonline.net/public/Meeting.aspx?AgencyID=123&MeetingID=20136&AgencyTypeID=1&IsArchived=True\",\n",
    "#         \"doc_name\": \"School Board Agenda, 2019-9-23\",\n",
    "#         \"user_id\": \"emily\",\n",
    "#         \"page_number\": \"NA\",\n",
    "#         \"keywords\": [\"employee\", \"housing\", \"affordable\"],\n",
    "#         \"text\": \"A.3. Master Plan for San Jose Unified Properties - Step 3 (ACTION)\\n\\nRECOMMENDATION: That San Jose Unified secure pre-development services to complete a pre-development analysis on the potential for employee housing opportunities at the following four locations: (1) 855 Lenzen Avenue, San Jose Unified District Offices, (2) 1088 Broadway, River Glen K-8 School, (3) 1325 Bouret Drive, Second Start-Pine Hill Non-Public School, and (4) 760 Hillsdale Avenue and 705-745 Capital Expressway, Metropolitan Education District.\"\n",
    "#       },\n",
    "#       {\n",
    "#         \"section_id\": \"002\",\n",
    "#         \"doc_url\": \"https://agendaonline.net/public/Meeting.aspx?AgencyID=123&MeetingID=20136&AgencyTypeID=1&IsArchived=True\",\n",
    "#         \"doc_name\": \"School Board Agenda, 2019-9-23\",\n",
    "#         \"user_id\": \"emily\",\n",
    "#         \"page_number\": \"NA\",\n",
    "#         \"keywords\": [\"employee\", \"housing\", \"affordable\"],\n",
    "#         \"text\": \"A.3. Master Plan for San Jose Unified Properties - Step 3 (ACTION)\\n\\nRECOMMENDATION: That San Jose Unified secure pre-development services to complete a pre-development analysis on the potential for employee housing opportunities at the following four locations: (1) 855 Lenzen Avenue, San Jose Unified District Offices, (2) 1088 Broadway, River Glen K-8 School, (3) 1325 Bouret Drive, Second Start-Pine Hill Non-Public School, and (4) 760 Hillsdale Avenue and 705-745 Capital Expressway, Metropolitan Education District.\"\n",
    "#       }\n",
    "#     ]\n",
    "#   }\n",
    "# ]\n",
    "# example_top_k_sections = []\n",
    "# example_query = {\n",
    "#     \"original_text\": \"I am a document document\",\n",
    "#     \"tokens\": [\"I\", \"am\"],\n",
    "#     \"filename\": \"filename1\",\n",
    "#     \"section_id\": \"section1\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_emails(results): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_history(history, top_k_sections, query): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_history(history): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_results(results, top_k_sections, query): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading language model\n",
      "model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erindb/miniconda3/envs/ecj/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings loaded \n",
      "loading docs ... \n"
     ]
    }
   ],
   "source": [
    "vectors = setup_word_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading queries\n",
      "reading metadata\n",
      "setting up reader\n",
      "loading cached idf\n",
      "finding relevant filenames\n",
      "reading relevant documents\n",
      "0 of 34 documents read\n",
      "reading history\n",
      "running query 0 of 2\n",
      "segmenting documents\n",
      "scoring documents\n",
      "[[ 0.05176774  0.07251675 -0.00623518 ... -0.08048102  0.04841436\n",
      "   0.07084007]\n",
      " [-0.01350302 -0.10802414  0.01358486 ...  0.06088634  0.10082254\n",
      "   0.06874264]\n",
      " [-0.00111791  0.0292689  -0.00581955 ... -0.0218233  -0.00421489\n",
      "  -0.00382978]\n",
      " ...\n",
      " [-0.06321549 -0.09482325  0.07480501 ... -0.04179247  0.08147775\n",
      "  -0.05127479]\n",
      " [ 0.00783078  0.01052135 -0.04465551 ... -0.02971679 -0.06393126\n",
      "  -0.07581798]\n",
      " [-0.03987353  0.05729975 -0.09865008 ... -0.04784824 -0.00271362\n",
      "   0.06468374]]\n",
      "running query 1 of 2\n",
      "segmenting documents\n",
      "scoring documents\n",
      "[[ 0.00573791  0.0083151   0.0242159  ... -0.00680768  0.0700219\n",
      "  -0.02159008]\n",
      " [ 0.08574822  0.05516469  0.01243349 ... -0.06173872  0.07288598\n",
      "  -0.02944022]\n",
      " [-0.06255211 -0.0595556   0.04738228 ... -0.07453814 -0.02959052\n",
      "  -0.04682044]\n",
      " ...\n",
      " [ 0.00978591  0.13391249 -0.03382149 ... -0.05699865  0.03725514\n",
      "   0.01085893]\n",
      " [-0.08887284  0.07808113 -0.01428313 ... -0.03618394 -0.09141206\n",
      "   0.01325157]\n",
      " [-0.00435863  0.02483096  0.07945908 ... -0.05304316  0.03296706\n",
      "  -0.10143712]]\n",
      "sending emails\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# vectors = setup_word_vectors()\n",
    "def run_queries(use_cached_idf = False, query_source=\"actual\", k=3): \n",
    "    print(\"reading queries\")\n",
    "    queries = read_queries(query_source)\n",
    "    print(\"reading metadata\")\n",
    "    metadata = read_metadata()\n",
    "    print(\"setting up reader\")\n",
    "    doc_text_reader = DocTextReader(log_every=100)\n",
    "    if use_cached_idf:\n",
    "        # used cached idf and only read relevant documents\n",
    "        print(\"loading cached idf\")\n",
    "        idf = read_cached_idf()\n",
    "        print(\"finding relevant filenames\")\n",
    "        relevant_filenames = find_relevant_filenames(queries, metadata)\n",
    "        # (not actually *all*, but all the ones we care about for queries)\n",
    "        print(\"reading relevant documents\")\n",
    "        all_docs = doc_text_reader.read_docs(relevant_filenames)\n",
    "    else:\n",
    "        # read all documents and calculate inverse document frequency\n",
    "        all_filenames = metadata[\"local_path_txt\"]\n",
    "        print(\"reading all documents\")\n",
    "        all_docs = doc_text_reader.read_docs(all_filenames)\n",
    "        print(\"calculating idf\")\n",
    "        idf = calculate_idf(all_docs)\n",
    "        cache_idf(idf)\n",
    "    print(\"reading history\")\n",
    "    history = read_history()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for q, query in enumerate(queries): \n",
    "        print(\"running query {} of {}\".format(q, len(queries)))\n",
    "        user_id = query[\"id\"]\n",
    "        keywords = query[\"Keywords\"]\n",
    "        time_window = query[\"Time Window\"]\n",
    "        municipalities = query[\"Municipalities\"]\n",
    "        relevant_docs = select_relevant_docs(municipalities, time_window, all_docs, metadata)\n",
    "        print(\"segmenting documents\")\n",
    "        doc_sections = segment_docs(relevant_docs)\n",
    "        print(\"scoring documents\")\n",
    "        doc_sections_scores = score_doc_sections(doc_sections, keywords, idf)\n",
    "        top_k_sections = select_top_k(doc_sections, doc_sections_scores, k, history)\n",
    "        results = update_results(results, top_k_sections, query)\n",
    "        history = update_history(history, top_k_sections, query)\n",
    "        \n",
    "    print(\"sending emails\")\n",
    "    send_emails(results)\n",
    "    write_history(history)\n",
    "    print(\"finished\")\n",
    "\n",
    "\n",
    "run_queries(use_cached_idf=True, query_source=\"quick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
