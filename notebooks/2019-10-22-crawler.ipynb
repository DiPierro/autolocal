{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_urls = [\"https://sunnyvaleca.legistar.com/Calendar.aspx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_page(url):\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        page = f.read()\n",
    "    return BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_site_url(url):\n",
    "    return re.match(\"([^/]*//[^/]*/)\", url).groups()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link(a, url):\n",
    "    href = a.get(\"href\")\n",
    "    if not href:\n",
    "        return None\n",
    "    # ingore empty URLs\n",
    "    if href == \"\":\n",
    "        return None\n",
    "    # ignore javascript calls (TODO: is this correct?)\n",
    "    if re.match(\"javascript\", href):\n",
    "        return None\n",
    "        return None\n",
    "    # ignore anchors\n",
    "    if href[0] == \"#\":\n",
    "        return None\n",
    "    site_url = get_site_url(url)\n",
    "    # if it's a full url AND a subpage of this site, return the url\n",
    "    if re.match(site_url, href):\n",
    "        return href\n",
    "    # but if it's a full url from some other site, ignore it\n",
    "    if re.match(\"https?://\", href):\n",
    "        return None\n",
    "    return \"{}{}\".format(site_url, href)\n",
    "    \n",
    "links_from_this_page = set([get_link(a, source_url) for a in page.find_all(\"a\") if get_link(a, source_url)])\n",
    "# links_from_this_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading page: https://sunnyvaleca.legistar.com/Calendar.aspx\n",
      "it's an html page, so we're searching it.\n",
      "65 new links found, 1 total read, 65 still need to be read\n",
      "***\n",
      "reading page: https://sunnyvaleca.legistar.com/Legislation.aspx\n",
      "it's an html page, so we're searching it.\n",
      "0 new links found, 2 total read, 64 still need to be read\n",
      "***\n",
      "reading page: https://sunnyvaleca.legistar.com/View.ashx?M=A&ID=651877&GUID=CBCE9D3D-CE13-40A5-B781-1EB3305F4EBC\n",
      "it's not an html page, so we're just keeping track of the url\n",
      "0 total read, 3 still need to be read\n",
      "***\n",
      "reading page: https://sunnyvaleca.legistar.com/View.ashx?M=IC&ID=661710&GUID=602A22B1-628A-421C-8D76-CFD951F83807\n",
      "it's not an html page, so we're just keeping track of the url\n",
      "0 total read, 4 still need to be read\n",
      "***\n",
      "reading page: https://sunnyvaleca.legistar.com/MeetingDetail.aspx?ID=672131&GUID=61E92095-3E44-4BD6-A9BC-544F021B0DF7&Search=\n",
      "it's an html page, so we're searching it.\n",
      "25 new links found, 5 total read, 86 still need to be read\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "urls_searched = set()\n",
    "urls_to_search = set([source_url])\n",
    "t = 0\n",
    "while len(urls_to_search) > 0 and t < 5:\n",
    "# while len(urls_to_search) > 0:\n",
    "    url = urls_to_search.pop()\n",
    "    urls_searched.add(url)\n",
    "    print(\"reading page: {}\".format(url))\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        contents = f.read()\n",
    "    if re.search(b'DOCTYPE html', contents):\n",
    "        print(\"it's an html page, so we're searching it.\")\n",
    "        page = BeautifulSoup(contents)\n",
    "        links_from_this_page = set([get_link(a, source_url) for a in page.find_all(\"a\") if get_link(a, source_url)])\n",
    "        new_links_to_search = links_from_this_page.difference(urls_searched).difference(urls_to_search)\n",
    "        urls_to_search.update(new_links_to_search)\n",
    "        print(\"{} new links found, {} total read, {} still need to be read\".format(\n",
    "            len(new_links_to_search),\n",
    "            len(urls_searched),\n",
    "            len(urls_to_search)))\n",
    "    else:\n",
    "        print(\"it's not an html page, so we're just keeping track of the url\")\n",
    "        print(\"{} total read, {} still need to be read\".format(\n",
    "            len(new_links_to_search),\n",
    "            len(urls_searched),\n",
    "            len(urls_to_search)))\n",
    "    print(\"***\")\n",
    "    t+=1\n",
    "#     links_to_search = [link for link in links_from_this_page if not link in urls_searched]\n",
    "# urls_to_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
